{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ai2-Project2-Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ιωάννης Καπετανγεώργης**\n",
        "\n",
        "**Αριθμός μητρώου: 1115201800061** "
      ],
      "metadata": {
        "id": "cFVjmojT2wRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Πριν από την εκτέλεση οποιουδήποτε από τα δύο μοντέλα  πρέπει να εκτελεστεί το παρακάτω κελί έτσι ώστε να γίνουν import όλα όσα χρησιμοποιούνται σε αυτά."
      ],
      "metadata": {
        "id": "-NSHqSJG2y-F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbIWw31LcbiI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "!pip install emoji --upgrade\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Στο παρακάτω κελί υπάρχουν τα path των train set και validation set από όπου διαβάζονται τα δεδομένα.\n",
        "\n",
        "Για την αντικατάσταση του validation set από το test set κατά την διόρθωση της εργασίας αρκεί να αλλάξει το path του validation set.\n",
        "Δηλαδή αλλάζουμε την μεταβλητή validation_set_location αντικαθιστόντας το υπάρχον path με το αντίστοιχο path του test set."
      ],
      "metadata": {
        "id": "v0pcYlLP2rmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_location = r'vaccine_train_set.csv' \n",
        "validation_set_location = r'vaccine_validation_set.csv'"
      ],
      "metadata": {
        "id": "SP5qHY_2c1t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Μοντέλο 1 - Word Embeddings / Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "xQB2uOw1eS7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download golve files\n",
        "!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip"
      ],
      "metadata": {
        "id": "koFRbNdQc_Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainSet = pd.read_csv(train_set_location,index_col=0)  # read csv file and store it to a dataframe\n",
        "validationSet = pd.read_csv(validation_set_location,index_col=0)  # read csv file and store it to a dataframe\n",
        "\n",
        "EMBEDDING_SIZE = 200\n",
        "glove_file = \"glove.twitter.27B.\"+str(EMBEDDING_SIZE)+'d.txt' # glove file that will be used\n",
        "\n",
        "CLEAN_ULR_MENTIONS_NUMBERS = True   # remove urls, mentions and numbers from tweets\n",
        "CLEAN_STOPWORDS = True # remove stopwords from tweets\n",
        "CLEAN_EMOJIS = True # convert emojis to the equivelent text\n",
        "CLEAN_PUNCUATIONS = True  # remove punctuation from tweets\n",
        "CONVERT_TO_LOWERCASE = True # convert all text to lowercase\n",
        "LEMMATIZATION = True # apply lemmatization to the tweets\n",
        "STEMMING = False   # apply stemming to the tweets\n",
        "\n",
        "\n",
        "def construct_embedding_dict(glove_file): # create an embedding dictionary using the glove file\n",
        "    # every word is a key in the dictionary and has it's embedding vector as a value\n",
        "    embedding_dict = {} # create empty dict\n",
        "    count = 0\n",
        "    with open(glove_file,'r') as f: #open the file\n",
        "        for line in f:  # for every line of the file\n",
        "          count += 1\n",
        "          values=line.split() # split the line \n",
        "          word = values[0]  # the fist token is the word \n",
        "          vector = np.asarray(values[1:], 'float32')  # all the other tokens are the embedding vector of the word, so store them in an np array\n",
        "          if vector.size == EMBEDDING_SIZE: # if the embedding vector has the wanted size\n",
        "            embedding_dict[word] = vector # add word/embedding vector to the dictionary\n",
        "    return embedding_dict\n",
        "\n",
        "#create the embedding dictionary from the glove file  \n",
        "embedding_dict =  construct_embedding_dict(glove_file)\n",
        "\n",
        "p.set_options(p.OPT.URL,p.OPT.MENTION,p.OPT.NUMBER) # set tweet-preprocessor to remove urls, mentions and numbers\n",
        "\n",
        "def clean_tweets(data,column):  # function that removes urls, mentions and numbers from all the tweets of the given dataframe\n",
        "  for i,v in enumerate(data[column]): # for every tweet\n",
        "      data.loc[i,column] = p.clean(v)\n",
        "  return data\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()   # initialize tweet tokenizer\n",
        "\n",
        "stop_words = stopwords.words('english') # get english stop words\n",
        "\n",
        "punctuations = list(string.punctuation) # get puncutations \n",
        "punctuations = punctuations + ['–','::','“','’','”','‘','`','...','``']  # add some extra puncutations\n",
        "\n",
        "def removeEmojis(text): # function that converts emojis to the equivalent text\n",
        "    text = emoji.demojize(text) # remove emojis\n",
        "    return str(text) \n",
        "\n",
        "def removePuncutation(text):   # remove punctuation from the given text\n",
        "    text = text.replace(\"-\",\"\")\n",
        "    splitted = tweet_tokenizer.tokenize(text) # split the tweet into tokens\n",
        "    new_text = []\n",
        "    for word in splitted:  # for every token\n",
        "      if word not in punctuations:    # keep it only if it is not a punctuation\n",
        "        new_text.append(word)\n",
        "    text = ' '.join(new_text)\n",
        "    return text\n",
        "\n",
        "def removeStopWordsAndPuncs(text):  # remove stop-words and punctuation from the given text\n",
        "    text = text.replace(\"-\",\"\")\n",
        "    text = text.replace(\"::\",\" \")\n",
        "    splitted = word_tokenize(text)  # split the tweet into tokens\n",
        "    new_text = []\n",
        "    for word in splitted:   # for every token\n",
        "      if word not in stop_words and word not in punctuations: # keep it only if it is not a punctuation or stop_word\n",
        "        new_text.append(word)\n",
        "    text = ' '.join(new_text)\n",
        "    return text\n",
        "\n",
        "def clean(text):    # clean the given tweet\n",
        "    if(CONVERT_TO_LOWERCASE): # if it is enabled\n",
        "      text = text.lower()  # convert all letters in to lowercase\n",
        "    if(CLEAN_EMOJIS):\n",
        "      text = removeEmojis(text)  # convert all emojis to the equivelent text\n",
        "    if(CLEAN_STOPWORDS and CLEAN_PUNCUATIONS):\n",
        "      text = removeStopWordsAndPuncs(text)    # remove all punctuation and stopwords\n",
        "    elif(CLEAN_PUNCUATIONS):\n",
        "      text = removePuncutation(text)    # remove all punctuation \n",
        "    return text\n",
        "\n",
        "def cleanText(data,column):  # apply clean function to every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(clean)\n",
        "  return data\n",
        "\n",
        "# stemming\n",
        "ps = SnowballStemmer(\"english\") # intialize stemmer\n",
        "\n",
        "def stemmTweet(text): # apply stemming to the given tweet\n",
        "  return ' '.join([(ps.stem) for w \\\n",
        "                       in w_tokenizer.tokenize((text))])  # split the tweet into tokens and apply stemming to every token\n",
        "\n",
        "def stem_data(data,column): # apply stemming into every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(stemmTweet)\n",
        "  return data\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()  # initialize Lemmatizer\n",
        "w_tokenizer =  TweetTokenizer() # initialize tokenizer\n",
        " \n",
        "def lemmatize_text(text): # apply lemmatization to the given tweet\n",
        "  return ' '.join([(lemmatizer.lemmatize(w)) for w \\\n",
        "                       in w_tokenizer.tokenize((text))])  # split the tweet into tokens and apply lemmatization to every token\n",
        "\n",
        "def lemmatize_data(data,column): # apply lemmatization into every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(lemmatize_text)\n",
        "  return data   \n",
        "\n",
        "if(CLEAN_ULR_MENTIONS_NUMBERS):\n",
        "  trainSet = clean_tweets(trainSet,'tweet') # remove urls, mentions, numbers from all the tweets in the train set\n",
        "  validationSet = clean_tweets(validationSet,'tweet') # remove urls, mentions, numbers from all the tweets in the validation set\n",
        "if(CLEAN_EMOJIS or CLEAN_STOPWORDS or CLEAN_PUNCUATIONS or CONVERT_TO_LOWERCASE):\n",
        "  trainSet = cleanText(trainSet,'tweet')    # clean the tweets of the train set \n",
        "  validationSet = cleanText(validationSet,'tweet')    # clean the tweets of the validation set \n",
        "if(LEMMATIZATION):\n",
        "  trainSet = lemmatize_data(trainSet,'tweet') # apply lemmatization to all the tweets of the train set\n",
        "  validationSet = lemmatize_data(validationSet,'tweet')  # apply lemmatization to all the tweets of the validation set\n",
        "if(STEMMING):\n",
        "  trainSet = stem_data(trainSet,'tweet')   # apply stemming to all the tweets of the train set\n",
        "  validationSet = stem_data(validationSet,'tweet')   # apply stemming to all the tweets of the validation set\n",
        "\n",
        "\n",
        "X_train = list(trainSet['tweet']) # convert the tweets of the train set into a list\n",
        "y_train = list(trainSet['label']) # convert the labels of the train set into a list\n",
        "\n",
        "X_validation = list(validationSet['tweet']) # convert the tweets of the validation set into a list\n",
        "y_validation = list(validationSet['label']) # convert the labels of the validation set into a list\n",
        "\n",
        "X_train_tokens = [sent.lower().split() for sent in X_train] # split every tweet of the train set into tokens\n",
        "X_validation_tokens = [sent.lower().split() for sent in X_validation] # split every tweet of the validation set into tokens\n",
        "\n",
        "def findAverageVector(words, model, num_features): # find the average vector of the given tweet\n",
        "    feature_vec = np.zeros((num_features),dtype=\"float32\")  # initialize\n",
        "    wordCount = 0.\n",
        "\n",
        "    for word in words:  # for every word of the text\n",
        "      try:\n",
        "        word_feature = model[word] # check the embedding dictionary to find the embedding vector of this word\n",
        "      except KeyError:  # if the word is not in the disctionary\n",
        "        continue\n",
        "      else:\n",
        "        if word_feature is not None: # if dictionary  contains that word\n",
        "            wordCount = wordCount + 1.\n",
        "            feature_vec = np.add(feature_vec,word_feature) # add the vector of this word to the total vector of the tweet\n",
        "    if wordCount > 0:\n",
        "      feature_vec = np.divide(feature_vec, wordCount) # find the average of the total vector of the tweet\n",
        "    return feature_vec\n",
        "\n",
        "\n",
        "def findTextsFeatureVectors(texts, model, num_features): # find the average feature vector for every text\n",
        "    counter = 0\n",
        "    textsFeatureVectors = np.zeros((len(texts),num_features), dtype='float32')  # initialize\n",
        "    \n",
        "    for text in texts:  # for every tweet\n",
        "        textsFeatureVectors[counter] = findAverageVector(text, model, num_features) # find average vector of the tweet\n",
        "        counter = counter + 1\n",
        "    return textsFeatureVectors\n",
        "\n",
        "# find feature vectors of train set\n",
        "trainDataVecs = findTextsFeatureVectors(X_train_tokens, embedding_dict, EMBEDDING_SIZE)\n",
        "# find feature vectors of validation set\n",
        "validationDataVecs  = findTextsFeatureVectors(X_validation_tokens, embedding_dict, EMBEDDING_SIZE)\n",
        "\n",
        "# convert train tweets and labels into tensors\n",
        "x_train_tensor = torch.tensor(trainDataVecs, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "# convert validation tweets and labels into tensors\n",
        "x_validation_tensor = torch.tensor(validationDataVecs, dtype=torch.float)\n",
        "y_validation_tensor = torch.tensor(y_validation, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, D_in, H1, H2, H3,D_out):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.model_stack =  nn.Sequential(\n",
        "                      nn.Linear(D_in, H1),        # input layer             \n",
        "                      nn.ReLU(),  # activation function\n",
        "                      nn.Dropout(0.5),  # dropout\n",
        "                      nn.Linear(H1, H2),    # hidden layer 1\n",
        "                      nn.ReLU(),  # activation function\n",
        "                      nn.Dropout(0.25),  # dropout\n",
        "                      nn.Linear(H2, H3),     # hidden layer 2\n",
        "                      nn.ReLU(),  # activation function\n",
        "                      nn.Dropout(0.1),  # dropout\n",
        "                      nn.Linear(H3, D_out),   # output layer\n",
        "                      # softmax is not needed after ouput layer because we are using nn.CrossEntropyLoss as the loss function\n",
        "                      )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x) # flatten the input in order to pass it to the sewuentian model\n",
        "        logits = self.model_stack(x)\n",
        "        return logits\n",
        "\n",
        "#Define layer sizes\n",
        "D_in = x_train_tensor.shape[1]\n",
        "H1 = 64\n",
        "H2 = 32\n",
        "H3 = 16\n",
        "D_out = 3\n",
        "\n",
        "#define epochs\n",
        "TRAINING_EPOCHS = 100\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 0.0025\n",
        "\n",
        "#Initialize model, loss, optimizer\n",
        "nn_model = Net(D_in, H1, H2, H3,D_out)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "nn_optimizer = torch.optim.SGD(nn_model.parameters(), lr=learning_rate,momentum=0.5)\n",
        "\n",
        "#Initialize dataloader for train set\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "#Initialize dataloader for validation set\n",
        "validation_dataset = torch.utils.data.TensorDataset(x_validation_tensor, y_validation_tensor)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "def train_model(model,criterion,train_loader,validation_loader,optimizer,epochs = 200):\n",
        "  train_loss = [] # store the train loss for every epoch\n",
        "  valid_loss = []  # store the validation loss for every epoch\n",
        "  for epoch in range(epochs):\n",
        "    train_batch_losses = [] # store the loss of every batch of the train set\n",
        "    validation_batch_losses = [] # store the loss of every batch of the validation set\n",
        "    model.train() # set the model to training mode\n",
        "    y_total_predict_train = []   # store the predictions of the the train set\n",
        "    y_total_train = []     # store the labels of the the train set\n",
        "    for x_batch, y_batch in train_loader:\n",
        "      #Delete previously stored gradients\n",
        "      optimizer.zero_grad()\n",
        "      z = model(x_batch)\n",
        "      loss = criterion(z, y_batch)  # compute the train loss\n",
        "      train_batch_losses.append(loss.data.item()) # store the train loss of the current batch\n",
        "      #Perform backpropagation starting from the loss calculated in this epoch\n",
        "      loss.backward()\n",
        "      #Update model's weights based on the gradients calculated during backprop\n",
        "      optimizer.step()\n",
        "      _, y_pred_tags = torch.max(z, dim = 1)  # get the prediction based on the maximum posibility\n",
        "      y_total_predict_train = y_total_predict_train+ list(y_pred_tags) # store the predictions of the current batch\n",
        "      y_total_train = y_total_train + (list(y_batch.detach().numpy())) # store the labels of the current batch\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()   # set the model to evaluation mode\n",
        "    y_total_valid = []   # store the predictions of the the validation set\n",
        "    y_total_predict_valid = []      # store the labels of the the validation set\n",
        "    for x_batch, y_batch in validation_loader:\n",
        "      z = model(x_batch)\n",
        "      loss = criterion(z, y_batch)   # compute the validation loss\n",
        "      validation_batch_losses.append(loss.data.item())   # store the validation loss of the current batch\n",
        "      _, label = torch.max(z,1)    # get the label prediction based on the maximum posibility\n",
        "      correct += (label==y_batch).sum().item()    # compute how many corect preditions were made\n",
        "      count += len(y_batch)\n",
        "      y_total_predict_valid = y_total_predict_valid+ list(label) # store the predictions of the current batch\n",
        "      y_total_valid= y_total_valid + list(y_batch.detach().numpy()) # store the labels of the current batch\n",
        "    accuracy = 100*(correct/(count))  # compute validation accuracy based on the correct predictions\n",
        "    current_train_loss = sum(train_batch_losses)/len(train_loader)  #compute the train loss of this epoch\n",
        "    train_loss.append(current_train_loss)   # store the train loss in order to plot loss curves\n",
        "    current_valid_loss = sum(validation_batch_losses)/len(validation_loader)   #compute the validation loss of this epoch\n",
        "    valid_loss.append(current_valid_loss)   # store the validation loss in order to plot loss curves\n",
        "    f1_score_train = f1_score(y_total_train,y_total_predict_train,average='weighted')# compure train f1-score\n",
        "    f1_score_valid = f1_score(y_total_valid,y_total_predict_valid,average='weighted') # compure validation f1-score\n",
        "    print(f\"Epoch {epoch:3}: Train Loss = {current_train_loss:.5f} | Validation Loss = {current_valid_loss:.5f} | Accuracy = {accuracy:.4f} | Train-f1 = {f1_score_train:.4f} | Valid-F1 = {f1_score_valid:.4f}\")\n",
        "  return model,train_loss,valid_loss\n",
        "\n",
        "# train the model\n",
        "trained_model, train_loss,valid_loss = train_model(nn_model,loss_func,train_loader,validation_loader,nn_optimizer,TRAINING_EPOCHS)\n",
        "\n",
        "def plot_loss_curves(t_loss,v_loss,epochs = 100):\n",
        "  x = list(range(1,epochs+1))\n",
        "  plt.plot(x, t_loss, 'r',label='Train Loss')\n",
        "  plt.plot(x, v_loss, 'g',label='Validation Loss')\n",
        "  plt.legend(loc='best')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Loss vs Epoch curves')\n",
        "  plt.show()\n",
        "\n",
        "plot_loss_curves(train_loss,valid_loss,TRAINING_EPOCHS) \n",
        "\n",
        "y_pred = trained_model(x_validation_tensor)  # test the trained model on the validation set\n",
        "y_pred_numpy = y_pred.detach().numpy()  # convert output to nupy array\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3  # number of classes/labels\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_validation_tensor, y_pred_numpy[:,i], pos_label=i)\n",
        "    \n",
        "# plotting the ROC curves\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);  \n",
        "\n",
        "_, y_pred_tags = torch.max(y_pred, dim = 1)# get tha labels of the predictions based on the label with the maximum possibility for each instance \n",
        "accuracy_countVec = accuracy_score(y_validation,y_pred_tags)  # accuracy metric\n",
        "f1_score_countVec = f1_score(y_validation,y_pred_tags,average='weighted') # f1_score metric\n",
        "print(\" Accuracy: %.2f%%\" %(accuracy_countVec*100))\n",
        "print(\" f1 score: %.2f%%\" %(f1_score_countVec*100))\n",
        "print(' Precision: %.2f%%' % (precision_score(y_validation, y_pred_tags,average='weighted')*100)) # precision\n",
        "print(' Recall: %.2f%%' % (recall_score(y_validation, y_pred_tags,average='weighted')*100)) # recall\n",
        "print(classification_report(y_validation, y_pred_tags))"
      ],
      "metadata": {
        "id": "4hcjC7wfdFqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Μοντέλο 2 - TF-IDF / Feed Forward Neural Network"
      ],
      "metadata": {
        "id": "8Tl-DdRGekXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLEAN_ULR_MENTIONS_NUMBERS = True   # remove urls, mentions and numbers from tweets\n",
        "CLEAN_STOPWORDS = True # remove stopwords from tweets\n",
        "CLEAN_EMOJIS = True # convert emojis to the equivelent text\n",
        "CLEAN_PUNCUATIONS = True  # remove punctuation from tweets\n",
        "CONVERT_TO_LOWERCASE = True # convert all text to lowercase\n",
        "LEMMATIZATION = True # apply lemmatization to the tweets\n",
        "STEMMING = False   # apply stemming to the tweets\n",
        "\n",
        "trainSet = pd.read_csv(train_set_location,index_col=0)  # read csv file and store it to a dataframe\n",
        "validationSet = pd.read_csv(validation_set_location,index_col=0)  # read csv file and store it to a dataframe\n",
        "\n",
        "p.set_options(p.OPT.URL,p.OPT.MENTION,p.OPT.NUMBER) # set tweet-preprocessor to remove urls, mentions and numbers\n",
        "\n",
        "def clean_tweets(data,column):  # function that removes urls, mentions and numbers from all the tweets of the given dataframe\n",
        "  for i,v in enumerate(data[column]): # for every tweet\n",
        "      data.loc[i,column] = p.clean(v)\n",
        "  return data\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()   # initialize tweet tokenizer\n",
        "\n",
        "stop_words = stopwords.words('english') # get english stop words\n",
        "\n",
        "punctuations = list(string.punctuation) # get puncutations \n",
        "punctuations = punctuations + ['–','::','“','’','”','‘','`','...','``']  # add some extra puncutations\n",
        "punctuations.remove('#')  # remove from puncutation\n",
        "punctuations.remove('!')  # remove from puncutation\n",
        "punctuations.remove('?')  # remove from puncutation \n",
        "\n",
        "def removeEmojis(text): # function that converts emojis to the equivalent text\n",
        "    text = emoji.demojize(text) # remove emojis\n",
        "    return str(text) \n",
        "\n",
        "def removePuncutation(text):   # remove punctuation from the given text\n",
        "    text = text.replace(\"-\",\"\")\n",
        "    text = text.replace(\"::\",\" \")\n",
        "    splitted = word_tokenize(text) # split the tweet into tokens\n",
        "    new_text = []\n",
        "    for word in splitted:  # for every token\n",
        "      if word not in punctuations:    # keep it only if it is not a punctuation\n",
        "        new_text.append(word)\n",
        "    text = ' '.join(new_text)\n",
        "    return text\n",
        "\n",
        "def removeStopWordsAndPuncs(text):  # remove stop-words and punctuation from the given text\n",
        "    text = text.replace(\"-\",\"\")\n",
        "    text = text.replace(\"::\",\" \")\n",
        "    splitted = word_tokenize(text)  # split the tweet into tokens\n",
        "    new_text = []\n",
        "    for word in splitted:   # for every token\n",
        "      if word not in stop_words and word not in punctuations: # keep it only if it is not a punctuation or stop_word\n",
        "        new_text.append(word)\n",
        "    text = ' '.join(new_text)\n",
        "    return text\n",
        "\n",
        "def clean(text):    # clean the given tweet\n",
        "    if(CONVERT_TO_LOWERCASE): # if it is enabled\n",
        "      text = text.lower()  # convert all letters in to lowercase\n",
        "    if(CLEAN_EMOJIS):\n",
        "      text = removeEmojis(text)  # convert all emojis to the equivelent text\n",
        "    if(CLEAN_STOPWORDS and CLEAN_PUNCUATIONS):\n",
        "      text = removeStopWordsAndPuncs(text)    # remove all punctuation and stopwords\n",
        "    elif(CLEAN_PUNCUATIONS):\n",
        "      text = removePuncutation(text)    # remove all punctuation \n",
        "    return text\n",
        "\n",
        "def cleanText(data,column):  # apply clean function to every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(clean)\n",
        "  return data\n",
        "\n",
        "# stemming\n",
        "ps = SnowballStemmer(\"english\") # intialize stemmer\n",
        "\n",
        "def stemmTweet(text): # apply stemming to the given tweet\n",
        "  return ' '.join([(ps.stem) for w \\\n",
        "                       in w_tokenizer.tokenize((text))])  # split the tweet into tokens and apply stemming to every token\n",
        "\n",
        "def stem_data(data,column): # apply stemming into every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(stemmTweet)\n",
        "  return data\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()  # initialize Lemmatizer\n",
        "w_tokenizer =  TweetTokenizer() # initialize tokenizer\n",
        " \n",
        "def lemmatize_text(text): # apply lemmatization to the given tweet\n",
        "  return ' '.join([(lemmatizer.lemmatize(w)) for w \\\n",
        "                       in w_tokenizer.tokenize((text))])  # split the tweet into tokens and apply lemmatization to every token\n",
        "\n",
        "def lemmatize_data(data,column): # apply lemmatization into every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(lemmatize_text)\n",
        "  return data   \n",
        "\n",
        "if(CLEAN_ULR_MENTIONS_NUMBERS):\n",
        "  trainSet = clean_tweets(trainSet,'tweet') # remove urls, mentions, numbers from all the tweets in the train set\n",
        "  validationSet = clean_tweets(validationSet,'tweet') # remove urls, mentions, numbers from all the tweets in the validation set\n",
        "if(CLEAN_EMOJIS or CLEAN_STOPWORDS or CLEAN_PUNCUATIONS or CONVERT_TO_LOWERCASE):\n",
        "  trainSet = cleanText(trainSet,'tweet')    # clean the tweets of the train set \n",
        "  validationSet = cleanText(validationSet,'tweet')    # clean the tweets of the validation set \n",
        "if(LEMMATIZATION):\n",
        "  trainSet = lemmatize_data(trainSet,'tweet') # apply lemmatization to all the tweets of the train set\n",
        "  validationSet = lemmatize_data(validationSet,'tweet')  # apply lemmatization to all the tweets of the validation set\n",
        "if(STEMMING):\n",
        "  trainSet = stem_data(trainSet,'tweet')   # apply stemming to all the tweets of the train set\n",
        "  validationSet = stem_data(validationSet,'tweet')   # apply stemming to all the tweets of the validation set\n",
        "\n",
        "\n",
        "X_train = list(trainSet['tweet']) # convert the tweets of the train set into a list\n",
        "y_train = list(trainSet['label']) # convert the labels of the train set into a list\n",
        "\n",
        "X_validation = list(validationSet['tweet']) # convert the tweets of the validation set into a list\n",
        "y_validation = list(validationSet['label']) # convert the labels of the validation set into a list\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3),max_features=300) #initialize the TfidfVectorizer\n",
        "trainSet_transformed_tfidf = tfidf_vectorizer.fit_transform(X_train)  # run the vectorizer for all the tweets of the train set\n",
        "validationSet_transformed_tfidf = tfidf_vectorizer.transform(X_validation)  # transform the tweets of the validation set\n",
        "\n",
        "# convert train tweets and labels into tensors\n",
        "x_train_tensor = torch.tensor(trainSet_transformed_tfidf.toarray(), dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "# convert validation tweets and labels into tensors\n",
        "x_validation_tensor = torch.tensor(validationSet_transformed_tfidf.toarray(), dtype=torch.float)\n",
        "y_validation_tensor = torch.tensor(y_validation, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, D_in, H1, H2, H3,D_out):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.model_stack =  nn.Sequential(\n",
        "                      nn.Linear(D_in, H1),    # input layer         \n",
        "                      nn.ReLU(),  # activation function\n",
        "                      nn.Dropout(0.5),  # dropout\n",
        "                      nn.Linear(H1, H2),    # hidden layer 1\n",
        "                      nn.ReLU(),  # activation function\n",
        "                      nn.Dropout(0.25),  # dropout\n",
        "                      nn.Linear(H2, H3),     # hidden layer 2\n",
        "                      nn.ReLU(),  # activation function\n",
        "                      nn.Dropout(0.1),  # dropout\n",
        "                      nn.Linear(H3, D_out),   # output layer\n",
        "                      # softmax is not needed after ouput layer because we are using nn.CrossEntropyLoss as the loss function\n",
        "                      )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x) # flatten the input in order to pass it to the sewuentian model\n",
        "        logits = self.model_stack(x)\n",
        "        return logits\n",
        "\n",
        "#Define layer sizes\n",
        "D_in = x_train_tensor.shape[1]\n",
        "H1 = 20\n",
        "H2 = 10\n",
        "H3 = 10\n",
        "D_out = 3\n",
        "\n",
        "#define epochs\n",
        "TRAINING_EPOCHS = 100\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 0.1\n",
        "\n",
        "#Initialize model, loss, optimizer\n",
        "nn_model = Net(D_in, H1, H2, H3,D_out)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "nn_optimizer = torch.optim.SGD(nn_model.parameters(), lr=learning_rate)\n",
        "\n",
        "#Initialize dataloader for train set\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "#Initialize dataloader for validation set\n",
        "validation_dataset = torch.utils.data.TensorDataset(x_validation_tensor, y_validation_tensor)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "def train_model(model,criterion,train_loader,validation_loader,optimizer,epochs = 200):\n",
        "  train_loss = [] # store the train loss for every epoch\n",
        "  valid_loss = []  # store the validation loss for every epoch\n",
        "  for epoch in range(epochs):\n",
        "    train_batch_losses = [] # store the loss of every batch of the train set\n",
        "    validation_batch_losses = [] # store the loss of every batch of the validation set\n",
        "    model.train() # set the model to training mode\n",
        "    y_total_predict_train = []   # store the predictions of the the train set\n",
        "    y_total_train = []     # store the labels of the the train set\n",
        "    for x_batch, y_batch in train_loader:\n",
        "      #Delete previously stored gradients\n",
        "      optimizer.zero_grad()\n",
        "      z = model(x_batch)\n",
        "      loss = criterion(z, y_batch)  # compute the train loss\n",
        "      train_batch_losses.append(loss.data.item()) # store the train loss of the current batch\n",
        "      #Perform backpropagation starting from the loss calculated in this epoch\n",
        "      loss.backward()\n",
        "      #Update model's weights based on the gradients calculated during backprop\n",
        "      optimizer.step()\n",
        "      _, y_pred_tags = torch.max(z, dim = 1)  # get the prediction based on the maximum posibility\n",
        "      y_total_predict_train = y_total_predict_train+ list(y_pred_tags) # store the predictions of the current batch\n",
        "      y_total_train = y_total_train + (list(y_batch.detach().numpy())) # store the labels of the current batch\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()   # set the model to evaluation mode\n",
        "    y_total_valid = []    # store the predictions of the the validation set\n",
        "    y_total_predict_valid = []      # store the labels of the the validation set\n",
        "    for x_batch, y_batch in validation_loader:\n",
        "      z = model(x_batch)\n",
        "      loss = criterion(z, y_batch)    # compute the validation loss\n",
        "      validation_batch_losses.append(loss.data.item())   # store the validation loss of the current batch\n",
        "      _, label = torch.max(z,1)     # get the label prediction based on the maximum posibility\n",
        "      correct += (label==y_batch).sum().item()  # compute how many corect preditions were made\n",
        "      count += len(y_batch)\n",
        "      y_total_predict_valid = y_total_predict_valid+ list(label)   # store the predictions of the current batch\n",
        "      y_total_valid= y_total_valid + list(y_batch.detach().numpy())  # store the labels of the current batch\n",
        "    accuracy = 100*(correct/(count))  # compute validation accuracy based on the correct predictions\n",
        "    current_train_loss = sum(train_batch_losses)/len(train_loader)  #compute the train loss of this epoch\n",
        "    train_loss.append(current_train_loss)   # store the train loss in order to plot loss curves\n",
        "    current_valid_loss = sum(validation_batch_losses)/len(validation_loader)    #compute the validation loss of this epoch\n",
        "    valid_loss.append(current_valid_loss)   # store the validation loss in order to plot loss curves\n",
        "    f1_score_train = f1_score(y_total_train,y_total_predict_train,average='weighted') # compure train f1-score\n",
        "    f1_score_valid = f1_score(y_total_valid,y_total_predict_valid,average='weighted') # compure validation f1-score\n",
        "    print(f\"Epoch {epoch:3}: Train Loss = {current_train_loss:.5f} | Validation Loss = {current_valid_loss:.5f} | Accuracy = {accuracy:.4f} | Train-f1 = {f1_score_train:.4f} | Valid-F1 = {f1_score_valid:.4f}\")\n",
        "  return model,train_loss,valid_loss\n",
        "\n",
        "# train the model\n",
        "trained_model, train_loss,valid_loss = train_model(nn_model,loss_func,train_loader,validation_loader,nn_optimizer,TRAINING_EPOCHS)\n",
        "\n",
        "def plot_loss_curves(t_loss,v_loss,epochs = 100):\n",
        "  x = list(range(1,epochs+1))\n",
        "  plt.plot(x, t_loss, 'r',label='Train Loss') # plotting t, a separately \n",
        "  plt.plot(x, v_loss, 'g',label='Validation Loss') # plotting t, b separately \n",
        "  plt.legend(loc='best')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Loss vs Epoch curves')\n",
        "  plt.show()\n",
        "\n",
        "plot_loss_curves(train_loss,valid_loss,TRAINING_EPOCHS)\n",
        "\n",
        "y_pred = trained_model(x_validation_tensor) # test the trained model on the validation set\n",
        "y_pred_numpy = y_pred.detach().numpy() # convert output to nupy array\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "n_class = 3 # number of classes/labels\n",
        "\n",
        "# compute roc curve values for every class/label\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_validation_tensor, y_pred_numpy[:,i], pos_label=i)\n",
        "    \n",
        "# plotting  the ROC curves\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);  \n",
        "\n",
        "_, y_pred_tags = torch.max(y_pred, dim = 1) # get tha labels of the predictions based on the label with the maximum possibility for each instance \n",
        "accuracy_countVec = accuracy_score(y_validation,y_pred_tags)  # accuracy metric\n",
        "f1_score_countVec = f1_score(y_validation,y_pred_tags,average='weighted') # f1_score metric\n",
        "print(\" Accuracy: %.2f%%\" %(accuracy_countVec*100))\n",
        "print(\" f1 score: %.2f%%\" %(f1_score_countVec*100))\n",
        "print(' Precision: %.2f%%' % (precision_score(y_validation, y_pred_tags,average='weighted')*100)) # precision\n",
        "print(' Recall: %.2f%%' % (recall_score(y_validation, y_pred_tags,average='weighted')*100)) # recall\n",
        "print(classification_report(y_validation, y_pred_tags))"
      ],
      "metadata": {
        "id": "dDvJndggeqO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}