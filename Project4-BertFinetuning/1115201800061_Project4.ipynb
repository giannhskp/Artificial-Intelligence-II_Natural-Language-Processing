{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SX137-7io7N"
      },
      "source": [
        "# Μέρος Α - Bert sentiment classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGkToZREo3gp"
      },
      "outputs": [],
      "source": [
        "train_set_location = r'vaccine_train_set.csv' \n",
        "validation_set_location = r'vaccine_validation_set.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KK7YQ45upcTa",
        "outputId": "79753ef7-7028-46c5-8f26-8a65322a57af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 31.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 30.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFV4b74Aonlf",
        "outputId": "326e6298-40c8-47f1-dd54-ba9a5b0287e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 9.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=f04a939b58439a963496ffecdf41bf1556bda949073398fd62689a1b5809210f\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.3\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "from transformers import BertForSequenceClassification\n",
        "from torch.optim import Adam,AdamW\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "!pip install emoji --upgrade\n",
        "import emoji\n",
        "import torch   \n",
        "import torchtext\n",
        "from torchtext.legacy import data   \n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNvUcx19WGTS"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed = 1234):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi6SNlzDo6T3"
      },
      "outputs": [],
      "source": [
        "trainSet = pd.read_csv(train_set_location,index_col=0)  # read csv file and store it to a dataframe\n",
        "validationSet = pd.read_csv(validation_set_location,index_col=0)  # read csv file and store it to a dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be0JmlG0i0Kn"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uz8DJNv4qAG"
      },
      "outputs": [],
      "source": [
        "###### DATA PRE-PROCESSING ####################################################################\n",
        "p.set_options(p.OPT.URL,p.OPT.MENTION,p.OPT.NUMBER) # set tweet-preprocessor to remove urls, mentions and numbers\n",
        "\n",
        "def clean_tweets(data,column):  # function that removes urls, mentions and numbers from all the tweets of the given dataframe\n",
        "  for i,v in enumerate(data[column]): # for every tweet\n",
        "      data.loc[i,column] = p.clean(v)\n",
        "  return data\n",
        "\n",
        "tweet_tokenizer = TweetTokenizer()   # initialize tweet tokenizer\n",
        "\n",
        "stop_words = stopwords.words('english') # get english stop words\n",
        "\n",
        "punctuations = list(string.punctuation) # get puncutations \n",
        "punctuations = punctuations + ['–','::','“','’','”','‘','`','...','``']  # add some extra puncutations\n",
        "\n",
        "def removeEmojis(text): # function that converts emojis to the equivalent text\n",
        "    text = emoji.demojize(text) # remove emojis\n",
        "    return str(text) \n",
        "\n",
        "def removePuncutation(text):   # remove punctuation from the given text\n",
        "    text = text.replace(\"-\",\"\")\n",
        "    splitted = tweet_tokenizer.tokenize(text) # split the tweet into tokens\n",
        "    new_text = []\n",
        "    for word in splitted:  # for every token\n",
        "      if word not in punctuations:    # keep it only if it is not a punctuation\n",
        "        new_text.append(word)\n",
        "    text = ' '.join(new_text)\n",
        "    return text\n",
        "\n",
        "def removeStopWordsAndPuncs(text):  # remove stop-words and punctuation from the given text\n",
        "    text = text.replace(\"-\",\"\")\n",
        "    text = text.replace(\"::\",\" \")\n",
        "    splitted = word_tokenize(text)  # split the tweet into tokens\n",
        "    new_text = []\n",
        "    for word in splitted:   # for every token\n",
        "      if word not in stop_words and word not in punctuations: # keep it only if it is not a punctuation or stop_word\n",
        "        new_text.append(word)\n",
        "    text = ' '.join(new_text)\n",
        "    return text\n",
        "\n",
        "def clean(text):    # clean the given tweet\n",
        "    if(CONVERT_TO_LOWERCASE): # if it is enabled\n",
        "      text = text.lower()  # convert all letters in to lowercase\n",
        "    if(CLEAN_EMOJIS):\n",
        "      text = removeEmojis(text)  # convert all emojis to the equivelent text\n",
        "    if(CLEAN_STOPWORDS and CLEAN_PUNCUATIONS):\n",
        "      text = removeStopWordsAndPuncs(text)    # remove all punctuation and stopwords\n",
        "    elif(CLEAN_PUNCUATIONS):\n",
        "      text = removePuncutation(text)    # remove all punctuation \n",
        "    return text\n",
        "\n",
        "def cleanText(data,column):  # apply clean function to every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(clean)\n",
        "  return data\n",
        "\n",
        "# stemming\n",
        "ps = SnowballStemmer(\"english\") # intialize stemmer\n",
        "\n",
        "def stemmTweet(text): # apply stemming to the given tweet\n",
        "  return ' '.join([(ps.stem) for w \\\n",
        "                       in w_tokenizer.tokenize((text))])  # split the tweet into tokens and apply stemming to every token\n",
        "\n",
        "def stem_data(data,column): # apply stemming into every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(stemmTweet)\n",
        "  return data\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()  # initialize Lemmatizer\n",
        "w_tokenizer =  TweetTokenizer() # initialize tokenizer\n",
        " \n",
        "def lemmatize_text(text): # apply lemmatization to the given tweet\n",
        "  return ' '.join([(lemmatizer.lemmatize(w)) for w \\\n",
        "                       in word_tokenize((text))])  # split the tweet into tokens and apply lemmatization to every token\n",
        "\n",
        "def lemmatize_data(data,column): # apply lemmatization into every tweet of the given dataframe\n",
        "  data[column] = data[column].apply(lemmatize_text)\n",
        "  return data   \n",
        "###############################################################################################\n",
        "\n",
        "CLEAN_ULR_MENTIONS_NUMBERS = True   # remove urls, mentions and numbers from tweets\n",
        "CLEAN_STOPWORDS = True # remove stopwords from tweets\n",
        "CLEAN_EMOJIS = True # convert emojis to the equivelent text\n",
        "CLEAN_PUNCUATIONS = True  # remove punctuation from tweets\n",
        "CONVERT_TO_LOWERCASE = True # convert all text to lowercase\n",
        "LEMMATIZATION = True # apply lemmatization to the tweets\n",
        "STEMMING = False   # apply stemming to the tweets\n",
        "   \n",
        "\n",
        "if(CLEAN_ULR_MENTIONS_NUMBERS):\n",
        "  trainSet = clean_tweets(trainSet,'tweet') # remove urls, mentions, numbers from all the tweets in the train set\n",
        "  validationSet = clean_tweets(validationSet,'tweet') # remove urls, mentions, numbers from all the tweets in the validation set\n",
        "if(CLEAN_EMOJIS or CLEAN_STOPWORDS or CLEAN_PUNCUATIONS or CONVERT_TO_LOWERCASE):\n",
        "  trainSet = cleanText(trainSet,'tweet')    # clean the tweets of the train set \n",
        "  validationSet = cleanText(validationSet,'tweet')    # clean the tweets of the validation set \n",
        "if(LEMMATIZATION):\n",
        "  trainSet = lemmatize_data(trainSet,'tweet') # apply lemmatization to all the tweets of the train set\n",
        "  validationSet = lemmatize_data(validationSet,'tweet')  # apply lemmatization to all the tweets of the validation set\n",
        "if(STEMMING):\n",
        "  trainSet = stem_data(trainSet,'tweet')   # apply stemming to all the tweets of the train set\n",
        "  validationSet = stem_data(validationSet,'tweet')   # apply stemming to all the tweets of the validation set\n",
        "\n",
        "X_train = list(trainSet['tweet']) # convert the tweets of the train set into a list\n",
        "y_train = list(trainSet['label']) # convert the labels of the train set into a list\n",
        "\n",
        "X_validation = list(validationSet['tweet']) # convert the tweets of the validation set into a list\n",
        "y_validation = list(validationSet['label']) # convert the labels of the validation set into a list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sJhqB7hi5Q1"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144,
          "referenced_widgets": [
            "9af3821ae04f46daa9c70e13ff9c75a5",
            "d04c5f99f39c4066b05a1747a51cc8fb",
            "0c0cce3926a1498487838b1bb34de041",
            "fcb5a07a6a2740169cede9bc31b971d7",
            "5fa46c6c9da840b9af51fd16dd8de806",
            "a4b001538bd34026901353fcc1f72ae8",
            "8c6e517870784c10b14173cfdbba97f3",
            "f63b6f34f0af486ba1c0bd0fbfa0f2d1",
            "04001322793a4b41add88a6b37fff6c0",
            "46942f5fc4a041aca713284e8e1b0b7e",
            "c07dfae8c514429888a610146010470e",
            "5232b9467e994e5cbdc9d841e0afb4ef",
            "5564f3537f464f67aceb2880f20b57cc",
            "d80cff6a6c3b4392ade5101267ab0204",
            "83a5c9b3a477437397d2004840d1642f",
            "ff4e3b63c7804434b6280023e28c1df5",
            "fd107d81e83b4e4aba851a6c7200b85c",
            "3a355bebb4aa42c6b0e946ea774df392",
            "0bdf4984d0ce4ad29f53398edf654c75",
            "4853784c8e32473e88368ac9b85a81be",
            "8d2c8450d67e40ba949ca13fdf5c5c1e",
            "fad84de815534338a4cda3a3441a2c27",
            "16e712ac3ce0470ba670f95b60741985",
            "5ffa807df9be4c3cbbd0fb270adeb558",
            "6e57c5d2859842d891dcbe8f4c8fdc73",
            "cd88d30c1e844c01913edabc58189c2c",
            "bce192674b034e71b1357df45737759b",
            "c2bfcd8a4764402f8f6bde5476693fc5",
            "79093ce6e0994950bbb17f0ffd083e8c",
            "8dc7de5707114778937d5a7205655c14",
            "32253a65263a4bc8a512ee0b612f20ce",
            "05f737b255a74c3f9ec79319d01c220e",
            "68bdaa1df71c438bb5b527155a50b214",
            "1f84d16cabf24d72974a3a3d24e734da",
            "0adb998b1cdd450ab7422022f8bd6ced",
            "a7ddc78e782d409994d1166afa086e0e",
            "4ff4ad3855c84cf583d8fcab11e6a120",
            "7481b3823350472883d131d2d5fd5e8b",
            "62c15fc4dd1a4d17aad036bda129f769",
            "689e201ea7954340805adaffc67c5b27",
            "3ce7a41c2f3549de94862aaf0c5fe269",
            "7fbc4da55e2f40a098dce2a3d76aa85b",
            "b9fe1b78a49a4b0fa4165668c60c015f",
            "279071b9eb5347499f7aa86b2da83b3d"
          ]
        },
        "id": "fMT-VEfTpKe0",
        "outputId": "207b161c-373f-4a74-c102-739c7c411c76"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9af3821ae04f46daa9c70e13ff9c75a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5232b9467e994e5cbdc9d841e0afb4ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16e712ac3ce0470ba670f95b60741985",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f84d16cabf24d72974a3a3d24e734da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Initialize the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1ANteG8qGkW"
      },
      "outputs": [],
      "source": [
        "X_train = [tokenizer(text,padding='max_length', max_length = 150) for text in X_train] # tokenize all the tweets of the training set\n",
        "X_validation = [tokenizer(text,padding='max_length', max_length = 150) for text in X_validation] # tokenize all the tweets of the validation set\n",
        "\n",
        "X_train_id = [tokenized['input_ids'] for tokenized in X_train]  # store all the input_ids of the training set into a list\n",
        "X_train_mask = [tokenized['attention_mask'] for tokenized in X_train] # store all the attention_masks of the training set into a list\n",
        "X_validation_id = [tokenized['input_ids'] for tokenized in X_validation]  # store all the input_ids of the validation set into a list\n",
        "X_validation_mask = [tokenized['attention_mask'] for tokenized in X_validation] # store all the attention_masks of the validation set into a list\n",
        "\n",
        "# convert all training data into tensors (input_ids, attention_masks and labels)\n",
        "x_train_id_tensor = torch.tensor(X_train_id)\n",
        "x_train_mask_tensor = torch.tensor(X_train_mask)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "# convert all validation data into tensors (input_ids, attention_masks and labels)\n",
        "x_validation_id_tensor = torch.tensor(X_validation_id)\n",
        "x_validation_mask_tensor = torch.tensor(X_validation_mask)\n",
        "y_validation_tensor = torch.tensor(y_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PMpSOYtF-e8"
      },
      "source": [
        "## Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B6Ba75iF9z7",
        "outputId": "8237e769-84b0-4168-dcbe-e7f1a22c64ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "set_seed(1234) # set seeds for data reprodusability\n",
        "#define epochs\n",
        "TRAINING_EPOCHS = 3\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 5e-6\n",
        "batch_size = 32\n",
        "\n",
        "# define the model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=3)\n",
        "# define the loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr= learning_rate)\n",
        "\n",
        "# use GPU\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "if use_cuda:\n",
        "  model = model.cuda()\n",
        "  loss_func = loss_func.cuda()\n",
        "\n",
        "#Initialize dataloader for train set\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train_id_tensor, y_train_tensor,x_train_mask_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#Initialize dataloader for validation set\n",
        "validation_dataset = torch.utils.data.TensorDataset(x_validation_id_tensor, y_validation_tensor,x_validation_mask_tensor)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnOITazlHWOt",
        "outputId": "eb3d3bef-ee0e-4a66-f7ff-c8509351c0b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0: Train Loss = 0.79033 | Validation Loss = 0.67909 | Accuracy = 70.6836 | Train-f1 = 0.6156 | Valid-F1 = 0.6912\n",
            "Epoch   1: Train Loss = 0.65779 | Validation Loss = 0.64380 | Accuracy = 73.0500 | Train-f1 = 0.7125 | Valid-F1 = 0.7257\n",
            "Epoch   2: Train Loss = 0.59623 | Validation Loss = 0.62333 | Accuracy = 73.7949 | Train-f1 = 0.7523 | Valid-F1 = 0.7377\n"
          ]
        }
      ],
      "source": [
        "def train_model(model,criterion,train_loader,validation_loader,optimizer,epochs = 200):\n",
        "  train_loss = [] # store the train loss for every epoch\n",
        "  valid_loss = []  # store the validation loss for every epoch\n",
        "  for epoch in range(epochs):\n",
        "    train_batch_losses = [] # store the loss of every batch of the train set\n",
        "    validation_batch_losses = [] # store the loss of every batch of the validation set\n",
        "    model.train() # set the model to training mode\n",
        "    y_total_predict_train = []   # store the predictions of the the train set\n",
        "    y_total_train = []     # store the labels of the the train set\n",
        "    for x_batch, y_batch, mask in train_loader:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      #Delete previously stored gradients\n",
        "      optimizer.zero_grad()\n",
        "      z = model(x_batch, token_type_ids=None, attention_mask=mask, labels=y_batch)\n",
        "      z = z[1] # get the output logits\n",
        "      loss = criterion(z, y_batch)  # compute the train loss\n",
        "      train_batch_losses.append(loss.data.item()) # store the train loss of the current batch\n",
        "      #Perform backpropagation starting from the loss calculated in this epoch\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(),5.0) # perform gradient clipping\n",
        "      #Update model's weights based on the gradients calculated during backprop\n",
        "      optimizer.step()\n",
        "      _, y_pred_tags = torch.max(z, dim = 1)  # get the prediction based on the maximum posibility\n",
        "      y_total_predict_train = y_total_predict_train+ list(y_pred_tags.cpu()) # store the predictions of the current batch\n",
        "      y_total_train = y_total_train + (list(y_batch.cpu().detach().numpy())) # store the labels of the current batch\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()   # set the model to evaluation mode\n",
        "    y_total_valid = []   # store the predictions of the the validation set\n",
        "    y_total_predict_valid = []      # store the labels of the the validation set\n",
        "    with torch.no_grad():\n",
        "      for x_batch, y_batch, mask in validation_loader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        mask = mask.to(device)\n",
        "        z = model(x_batch, token_type_ids=None, attention_mask=mask, labels=y_batch)\n",
        "        z = z[1]  # get the output logits\n",
        "        loss = criterion(z, y_batch)   # compute the validation loss\n",
        "        validation_batch_losses.append(loss.data.item())   # store the validation loss of the current batch\n",
        "        _, label = torch.max(z,1)    # get the label prediction based on the maximum posibility\n",
        "        correct += (label==y_batch).sum().item()    # compute how many corect preditions were made\n",
        "        count += len(y_batch)\n",
        "        y_total_predict_valid = y_total_predict_valid+ list(label.cpu()) # store the predictions of the current batch\n",
        "        y_total_valid= y_total_valid + list(y_batch.cpu().detach().numpy()) # store the labels of the current batch\n",
        "    accuracy = 100*(correct/(count))  # compute validation accuracy based on the correct predictions\n",
        "    current_train_loss = sum(train_batch_losses)/len(train_loader)  #compute the train loss of this epoch\n",
        "    train_loss.append(current_train_loss)   # store the train loss in order to plot loss curves\n",
        "    current_valid_loss = sum(validation_batch_losses)/len(validation_loader)   #compute the validation loss of this epoch\n",
        "    valid_loss.append(current_valid_loss)   # store the validation loss in order to plot loss curves\n",
        "    f1_score_train = f1_score(y_total_train,y_total_predict_train,average='weighted')# compure train f1-score\n",
        "    f1_score_valid = f1_score(y_total_valid,y_total_predict_valid,average='weighted') # compure validation f1-score\n",
        "    print(f\"Epoch {epoch:3}: Train Loss = {current_train_loss:.5f} | Validation Loss = {current_valid_loss:.5f} | Accuracy = {accuracy:.4f} | Train-f1 = {f1_score_train:.4f} | Valid-F1 = {f1_score_valid:.4f}\")\n",
        "  return model,train_loss,valid_loss\n",
        "\n",
        "# train the model\n",
        "trained_model, train_loss,valid_loss = train_model(model,loss_func,train_loader,validation_loader,optimizer,TRAINING_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oozh41zzjFDN"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "WTp9sy6o7arT",
        "outputId": "3576522d-6191-4155-ae59-46c1683b9c42"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Accuracy: 73.79%\n",
            " f1 score: 73.77%\n",
            " Precision: 73.94%\n",
            " Recall: 73.79%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.78      0.80      1065\n",
            "           1       0.58      0.52      0.55       296\n",
            "           2       0.69      0.76      0.72       921\n",
            "\n",
            "    accuracy                           0.74      2282\n",
            "   macro avg       0.70      0.69      0.69      2282\n",
            "weighted avg       0.74      0.74      0.74      2282\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_loss_curves(t_loss,v_loss,epochs = 100):\n",
        "  # function that plots the loss vs epochs curves\n",
        "  x = list(range(1,epochs+1))\n",
        "  plt.plot(x, t_loss, 'r',label='Train Loss')\n",
        "  plt.plot(x, v_loss, 'g',label='Validation Loss')\n",
        "  plt.legend(loc='best')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Loss vs Epoch curves')\n",
        "  plt.show()\n",
        "\n",
        "# plot the loss vs epochs curves\n",
        "plot_loss_curves(train_loss,valid_loss,TRAINING_EPOCHS) \n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()   # set the model to evaluation mode\n",
        "  y_total_valid = []   # store the predictions of the the validation set\n",
        "  y_total_predict_valid = []      # store the labels of the the validation set\n",
        "  y_output = []\n",
        "  for x_batch, y_batch, mask in validation_loader:\n",
        "    x_batch = x_batch.to(device) # use gpu\n",
        "    y_batch = y_batch.to(device) # use gpu\n",
        "    mask = mask.to(device) # use gpu\n",
        "    z = model(x_batch, token_type_ids=None, attention_mask=mask, labels=y_batch)\n",
        "    z = z[1]\n",
        "    _, label = torch.max(z,1)    # get the label prediction based on the maximum posibility\n",
        "    y_output = y_output + list(z.cpu().detach().numpy())\n",
        "    y_total_predict_valid = y_total_predict_valid+ list(label.cpu()) # store the predictions of the current batch\n",
        "    y_total_valid= y_total_valid + list(y_batch.cpu().detach().numpy()) # store the labels of the current batch\n",
        "\n",
        "\n",
        "# roc curve for classes\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "thresh ={}\n",
        "\n",
        "y_output = np.array(y_output)\n",
        "n_class = 3  # number of classes/labels\n",
        "\n",
        "for i in range(n_class):    \n",
        "    fpr[i], tpr[i], thresh[i] = roc_curve(y_total_valid, y_output[:,i], pos_label=i)\n",
        "    \n",
        "# plotting the ROC curves\n",
        "plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n",
        "plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n",
        "plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n",
        "plt.title('Multiclass ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig('Multiclass ROC',dpi=300);  \n",
        "\n",
        "y_pred_tags = y_total_predict_valid\n",
        "accuracy_countVec = accuracy_score(y_total_valid,y_pred_tags)  # accuracy metric\n",
        "f1_score_countVec = f1_score(y_total_valid,y_pred_tags,average='weighted') # f1_score metric\n",
        "print(\" Accuracy: %.2f%%\" %(accuracy_countVec*100))\n",
        "print(\" f1 score: %.2f%%\" %(f1_score_countVec*100))\n",
        "print(' Precision: %.2f%%' % (precision_score(y_total_valid, y_pred_tags,average='weighted')*100)) # precision\n",
        "print(' Recall: %.2f%%' % (recall_score(y_total_valid, y_pred_tags,average='weighted')*100)) # recall\n",
        "print(classification_report(y_total_valid, y_pred_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yNFkUTcz7hI"
      },
      "source": [
        "# ΜΕΡΟΣ Β & Γ\n",
        "Το μέρος Β (Bert finetuning for SQuAD dataset) αποτελεί υποερώτημα του μέρους Γ.\n",
        "Έτσι παρουσιάζονται μαζί.\n",
        "\n",
        "Αρχικά στην παράγραφο [Datasets Creation](#data_creation) δημιουργούνται τα εξής 5 datasets:\n",
        "  - SQuAD 2.0\n",
        "  - Trivia QA\n",
        "  - News QA\n",
        "  - Natural Questions (NQ)\n",
        "  - Quac\n",
        "\n",
        "Τα datasets (εκτός του SQuAD) μετατρέπονται στο ίδιο format με τα αρχεία του SQuAD.\n",
        "\n",
        "Στην συνέχεια, για κάθε ένα dataset πραγματοποιείται:\n",
        " - Finetuning του μοντέλου Bert χρησιμοποιώντας το training set του συγκεκριμένου dataset.\n",
        " - Evaluation του μοντέλου χρησιμοποιώντας το dev set του ίδιου dataset.\n",
        " - Evaluation του μοντέλου χρησιμοποιώντας τα dev sets των υπόλοιπων 4 datasets.\n",
        "\n",
        "Περισσότερες λεπτομέρειες εξηγούνται στο README.\n",
        "\n",
        "Όσον αφορά το μέρος Β της εργασίας, παρουσιάζεται στις παραγράφους:\n",
        "\n",
        "- [SQUAD FINETUNE](#squad_finetune) (Bert finetuning on SQuAD dataset)\n",
        "- [SQuAD fined-tuned model evaluation](#squad_evaluation) (evaluation of the finetuned model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVELM0Pit6VM"
      },
      "source": [
        "<a name=\"data_creation\"></a>\n",
        "# Datasets Creation\n",
        "Download and convert all datastets to SQuAD format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M77NGpcuuI6G"
      },
      "source": [
        "## SQuAD dataset creation\n",
        "Download of SQuAD dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:22:02.420237Z",
          "iopub.status.busy": "2022-03-04T11:22:02.419962Z",
          "iopub.status.idle": "2022-03-04T11:22:04.819596Z",
          "shell.execute_reply": "2022-03-04T11:22:04.818796Z",
          "shell.execute_reply.started": "2022-03-04T11:22:02.420206Z"
        },
        "id": "o9UQFn86OFFJ",
        "outputId": "94f0484c-5fb5-484a-8d2d-24aac6a4bba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-04 11:22:03--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M   184MB/s    in 0.2s    \n",
            "\n",
            "2022-03-04 11:22:03 (184 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n",
            "--2022-03-04 11:22:04--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.111.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-03-04 11:22:04 (60.0 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRQ5-aU1s50X"
      },
      "source": [
        "## TriviaQA dataset creation\n",
        "Download of TriviaQA dataset and tranformation to SQuAD format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3eIbLasbwA9",
        "outputId": "3383464e-d228-4274-f3da-80b35d11b97b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMMS2lEIbZj5",
        "outputId": "306df834-1443-4de4-8736-9b722e9c7f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "from transformers import DistilBertTokenizerFast, BertTokenizerFast, DistilBertForQuestionAnswering, BertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j76Qrxyzbmuf",
        "outputId": "67eb29f9-1563-4f86-fbbb-b2b1e8919c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-03 10:59:44--  https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\n",
            "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.120, 2607:4000:200:12::78\n",
            "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2665779500 (2.5G) [application/x-gzip]\n",
            "Saving to: ‘triviaqa-rc.tar.gz’\n",
            "\n",
            "triviaqa-rc.tar.gz  100%[===================>]   2.48G  19.6MB/s    in 2m 24s  \n",
            "\n",
            "2022-03-03 11:02:09 (17.7 MB/s) - ‘triviaqa-rc.tar.gz’ saved [2665779500/2665779500]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\n",
        "!tar -xzf triviaqa-rc.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAsk9PvFboVx"
      },
      "outputs": [],
      "source": [
        "# UTILS \n",
        "def add_triple_data(datum, page, domain):\n",
        "    qad = {'Source': domain}\n",
        "    for key in ['QuestionId', 'Question', 'Answer']:\n",
        "        qad[key] = datum[key]\n",
        "    for key in page:\n",
        "        qad[key] = page[key]\n",
        "    return qad\n",
        "\n",
        "\n",
        "def get_qad_triples(data):\n",
        "    qad_triples = []\n",
        "    for datum in data['Data']:\n",
        "        for key in ['EntityPages', 'SearchResults']:\n",
        "            for page in datum.get(key, []):\n",
        "                qad = add_triple_data(datum, page, key)\n",
        "                qad_triples.append(qad)\n",
        "    return qad_triples\n",
        "\n",
        "\n",
        "def get_file_contents(filename, encoding='utf-8'):\n",
        "    with open(filename, encoding=encoding) as f:\n",
        "        content = f.read()\n",
        "    return content\n",
        "\n",
        "def read_json(filename, encoding='utf-8'):\n",
        "    contents = get_file_contents(filename, encoding=encoding)\n",
        "    return json.loads(contents)\n",
        "\n",
        "def read_clean_part(datum):\n",
        "    for key in ['EntityPages', 'SearchResults']:\n",
        "        new_page_list = []\n",
        "        for page in datum.get(key, []):\n",
        "            if page['DocPartOfVerifiedEval']:\n",
        "                new_page_list.append(page)\n",
        "        datum[key] = new_page_list\n",
        "    assert len(datum['EntityPages']) + len(datum['SearchResults']) > 0\n",
        "    return datum\n",
        "\n",
        "\n",
        "def read_triviaqa_data(qajson):\n",
        "    data = read_json(qajson)\n",
        "    # read only documents and questions that are a part of clean data set\n",
        "    if data['VerifiedEval']:\n",
        "        clean_data = []\n",
        "        for datum in data['Data']:\n",
        "            if datum['QuestionPartOfVerifiedEval']:\n",
        "                if data['Domain'] == 'Web':\n",
        "                    datum = read_clean_part(datum)\n",
        "                clean_data.append(datum)\n",
        "        data['Data'] = clean_data\n",
        "    return data\n",
        "\n",
        "def get_question_doc_string(qid, doc_name):\n",
        "    return '{}--{}'.format(qid, doc_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrEdEC63bqyT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import nltk\n",
        "# from utils.convert_to_squad_format import get_qad_triples\n",
        "# from utils.utils import get_file_contents\n",
        "# from utils.dataset_utils import read_triviaqa_data, get_question_doc_string\n",
        "\n",
        "\n",
        "def answer_index_in_document(answer, document):\n",
        "    answer_list = answer['Aliases'] + answer['NormalizedAliases']\n",
        "    for answer_string_in_doc in answer_list:\n",
        "        index = document.find(answer_string_in_doc)\n",
        "        if index != -1:\n",
        "            return answer_string_in_doc, index\n",
        "    return answer['NormalizedValue'], -1\n",
        "\n",
        "\n",
        "def select_relevant_portion(text):\n",
        "    paras = text.split('\\n')\n",
        "    selected = []\n",
        "    done = False\n",
        "    for para in paras:\n",
        "        sents = sent_tokenize.tokenize(para)\n",
        "        for sent in sents:\n",
        "            words = nltk.word_tokenize(sent)\n",
        "            for word in words:\n",
        "                selected.append(word)\n",
        "                if len(selected) >= 800:\n",
        "                    done = True\n",
        "                    break\n",
        "            if done:\n",
        "                break\n",
        "        if done:\n",
        "            break\n",
        "        selected.append('\\n')\n",
        "    st = ' '.join(selected).strip()\n",
        "    return st\n",
        "\n",
        "\n",
        "def triviaqa_to_squad_format(triviaqa_file, data_dir, output_file):\n",
        "    triviaqa_json = read_triviaqa_data(triviaqa_file)\n",
        "    qad_triples = get_qad_triples(triviaqa_json)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for triviaqa_example in qad_triples:\n",
        "        question_text = triviaqa_example['Question']\n",
        "        text = get_file_contents(os.path.join(data_dir, triviaqa_example['Filename']), encoding='utf-8')\n",
        "        context = select_relevant_portion(text)\n",
        "\n",
        "        para = {'context': context, 'qas': [{'question': question_text, 'answers': []}]}\n",
        "        data.append({'paragraphs': [para]})\n",
        "        qa = para['qas'][0]\n",
        "        qa['id'] = get_question_doc_string(triviaqa_example['QuestionId'], triviaqa_example['Filename'])\n",
        "        qa['is_impossible'] = True\n",
        "        ans_string, index = answer_index_in_document(triviaqa_example['Answer'], context)\n",
        "\n",
        "        if index != -1:\n",
        "            qa['answers'].append({'text': ans_string, 'answer_start': index})\n",
        "            qa['is_impossible'] = False\n",
        "\n",
        "    triviaqa_as_squad = {'data': data, 'version': '2.0'}\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(triviaqa_as_squad, outfile, indent=2, sort_keys=True, ensure_ascii=False)\n",
        "\n",
        "sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGmPNZQBbuzO"
      },
      "outputs": [],
      "source": [
        "triviaqa_file_train = 'qa/wikipedia-train.json'\n",
        "data_dir = 'evidence/wikipedia/'\n",
        "output_file_train = 'triviaqa_train_SquadFormat.json'\n",
        "triviaqa_to_squad_format(triviaqa_file_train, data_dir, output_file_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdlbCLqCb5fk"
      },
      "outputs": [],
      "source": [
        "triviaqa_file_dev = 'qa/wikipedia-dev.json'\n",
        "data_dir = 'evidence/wikipedia/'\n",
        "output_file_dev = 'triviaqa_dev_SquadFormat.json'\n",
        "triviaqa_to_squad_format(triviaqa_file_dev, data_dir, output_file_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YQ0M1GNr30V"
      },
      "source": [
        "## NewsQA dataset creation\n",
        "Download of NewsQA dataset and tranformation to SQuAD format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pcfOGw3650w",
        "outputId": "44a6f34f-e379-43db-83c1-cceebcec6050"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'newsqa'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Total 132 (delta 0), reused 0 (delta 0), pack-reused 132\u001b[K\n",
            "Receiving objects: 100% (132/132), 599.31 KiB | 1.37 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Maluuba/newsqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-7DiDw0CYQY",
        "outputId": "193188d1-56e6-4d6f-9901-1ec0df5e8075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-01 19:51:32--  https://nlp.stanford.edu/software/stanford-postagger-2015-12-09.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-postagger-2015-12-09.zip [following]\n",
            "--2022-03-01 19:51:32--  https://downloads.cs.stanford.edu/nlp/software/stanford-postagger-2015-12-09.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25169032 (24M) [application/zip]\n",
            "Saving to: ‘stanford-postagger-2015-12-09.zip’\n",
            "\n",
            "stanford-postagger- 100%[===================>]  24.00M  23.9MB/s    in 1.0s    \n",
            "\n",
            "2022-03-01 19:51:33 (23.9 MB/s) - ‘stanford-postagger-2015-12-09.zip’ saved [25169032/25169032]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/software/stanford-postagger-2015-12-09.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaZjwH5zCc-Q"
      },
      "outputs": [],
      "source": [
        "!unzip /content/stanford-postagger-2015-12-09.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDfzSRxh9zx1"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aPHtWR5_JQi",
        "outputId": "623793a6-439a-4c9b-89fb-93abfde9d3bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.5.4\n",
            "  latest version: 4.11.0\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/newsqa\n",
            "\n",
            "  added / updated specs: \n",
            "    - pandas[version='>=0.19.2']\n",
            "    - python=2.7\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    tk-8.6.11                  |       h1ccaba5_0         3.2 MB\n",
            "    mkl-service-2.3.0          |   py27he904b0f_0         205 KB\n",
            "    pytz-2021.3                |     pyhd3eb1b0_0         224 KB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    python-dateutil-2.8.2      |     pyhd3eb1b0_0         241 KB\n",
            "    six-1.16.0                 |     pyhd3eb1b0_1          19 KB\n",
            "    readline-8.1.2             |       h7f8727e_1         423 KB\n",
            "    intel-openmp-2022.0.1      |    h06a4308_3633         8.5 MB\n",
            "    pandas-0.24.2              |   py27he6710b0_0        10.9 MB\n",
            "    ca-certificates-2022.2.1   |       h06a4308_0         130 KB\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_0         1.3 MB\n",
            "    libstdcxx-ng-9.1.0         |       hdf63c60_0         4.0 MB\n",
            "    mkl-2020.2                 |              256       213.9 MB\n",
            "    setuptools-44.0.0          |           py27_0         647 KB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    ncurses-6.3                |       h7f8727e_2         1.0 MB\n",
            "    libffi-3.3                 |       he6710b0_2          54 KB\n",
            "    sqlite-3.37.2              |       hc218d9a_0         1.5 MB\n",
            "    pip-19.3.1                 |           py27_0         1.9 MB\n",
            "    numpy-base-1.16.6          |   py27hde5b4d6_0         4.3 MB\n",
            "    wheel-0.37.1               |     pyhd3eb1b0_0          31 KB\n",
            "    python-2.7.18              |       ha1903f6_2        12.6 MB\n",
            "    numpy-1.16.6               |   py27hbc911f0_0          49 KB\n",
            "    mkl_random-1.1.0           |   py27hd6b4f25_0         332 KB\n",
            "    blas-1.0                   |              mkl           6 KB\n",
            "    mkl_fft-1.0.15             |   py27ha843d7b_0         164 KB\n",
            "    certifi-2020.6.20          |     pyhd3eb1b0_3         159 KB\n",
            "    zlib-1.2.11                |       h7f8727e_4         125 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       274.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:   0.1-main              \n",
            "    blas:            1.0-mkl               \n",
            "    ca-certificates: 2022.2.1-h06a4308_0   \n",
            "    certifi:         2020.6.20-pyhd3eb1b0_3\n",
            "    intel-openmp:    2022.0.1-h06a4308_3633\n",
            "    libffi:          3.3-he6710b0_2        \n",
            "    libgcc-ng:       9.1.0-hdf63c60_0      \n",
            "    libgfortran-ng:  7.3.0-hdf63c60_0      \n",
            "    libstdcxx-ng:    9.1.0-hdf63c60_0      \n",
            "    mkl:             2020.2-256            \n",
            "    mkl-service:     2.3.0-py27he904b0f_0  \n",
            "    mkl_fft:         1.0.15-py27ha843d7b_0 \n",
            "    mkl_random:      1.1.0-py27hd6b4f25_0  \n",
            "    ncurses:         6.3-h7f8727e_2        \n",
            "    numpy:           1.16.6-py27hbc911f0_0 \n",
            "    numpy-base:      1.16.6-py27hde5b4d6_0 \n",
            "    pandas:          0.24.2-py27he6710b0_0 \n",
            "    pip:             19.3.1-py27_0         \n",
            "    python:          2.7.18-ha1903f6_2     \n",
            "    python-dateutil: 2.8.2-pyhd3eb1b0_0    \n",
            "    pytz:            2021.3-pyhd3eb1b0_0   \n",
            "    readline:        8.1.2-h7f8727e_1      \n",
            "    setuptools:      44.0.0-py27_0         \n",
            "    six:             1.16.0-pyhd3eb1b0_1   \n",
            "    sqlite:          3.37.2-hc218d9a_0     \n",
            "    tk:              8.6.11-h1ccaba5_0     \n",
            "    wheel:           0.37.1-pyhd3eb1b0_0   \n",
            "    zlib:            1.2.11-h7f8727e_4     \n",
            "\n",
            "Proceed ([y]/n)? y\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "tk-8.6.11            |  3.2 MB | : 100% 1.0/1 [00:00<00:00,  1.21it/s]               \n",
            "mkl-service-2.3.0    |  205 KB | : 100% 1.0/1 [00:00<00:00,  9.47it/s]\n",
            "pytz-2021.3          |  224 KB | : 100% 1.0/1 [00:00<00:00,  3.68it/s]               \n",
            "libgcc-ng-9.1.0      |  8.1 MB | : 100% 1.0/1 [00:01<00:00,  1.46s/it]               \n",
            "python-dateutil-2.8. |  241 KB | : 100% 1.0/1 [00:00<00:00, 15.69it/s]\n",
            "six-1.16.0           |   19 KB | : 100% 1.0/1 [00:00<00:00, 28.14it/s]\n",
            "readline-8.1.2       |  423 KB | : 100% 1.0/1 [00:00<00:00,  7.51it/s]\n",
            "intel-openmp-2022.0. |  8.5 MB | : 100% 1.0/1 [00:01<00:00,  1.62s/it]              \n",
            "pandas-0.24.2        | 10.9 MB | : 100% 1.0/1 [00:03<00:00,  3.12s/it]               \n",
            "ca-certificates-2022 |  130 KB | : 100% 1.0/1 [00:00<00:00, 21.85it/s]\n",
            "libgfortran-ng-7.3.0 |  1.3 MB | : 100% 1.0/1 [00:00<00:00,  3.01it/s]               \n",
            "libstdcxx-ng-9.1.0   |  4.0 MB | : 100% 1.0/1 [00:00<00:00,  1.27it/s]               \n",
            "mkl-2020.2           | 213.9 MB | : 100% 1.0/1 [00:50<00:00, 50.10s/it]                \n",
            "setuptools-44.0.0    |  647 KB | : 100% 1.0/1 [00:00<00:00,  3.60it/s]               \n",
            "_libgcc_mutex-0.1    |    3 KB | : 100% 1.0/1 [00:00<00:00, 47.64it/s]\n",
            "ncurses-6.3          |  1.0 MB | : 100% 1.0/1 [00:01<00:00,  1.03s/it]               \n",
            "libffi-3.3           |   54 KB | : 100% 1.0/1 [00:00<00:00, 24.19it/s]\n",
            "sqlite-3.37.2        |  1.5 MB | : 100% 1.0/1 [00:00<00:00,  3.62it/s]               \n",
            "pip-19.3.1           |  1.9 MB | : 100% 1.0/1 [00:00<00:00,  1.33it/s]               \n",
            "numpy-base-1.16.6    |  4.3 MB | : 100% 1.0/1 [00:01<00:00,  1.50s/it]               \n",
            "wheel-0.37.1         |   31 KB | : 100% 1.0/1 [00:00<00:00, 25.26it/s]\n",
            "python-2.7.18        | 12.6 MB | : 100% 1.0/1 [00:03<00:00,  3.27s/it]               \n",
            "numpy-1.16.6         |   49 KB | : 100% 1.0/1 [00:00<00:00, 11.03it/s]\n",
            "mkl_random-1.1.0     |  332 KB | : 100% 1.0/1 [00:00<00:00,  8.10it/s]               \n",
            "blas-1.0             |    6 KB | : 100% 1.0/1 [00:00<00:00, 34.19it/s]\n",
            "mkl_fft-1.0.15       |  164 KB | : 100% 1.0/1 [00:00<00:00, 12.40it/s]\n",
            "certifi-2020.6.20    |  159 KB | : 100% 1.0/1 [00:00<00:00, 13.80it/s]\n",
            "zlib-1.2.11          |  125 KB | : 100% 1.0/1 [00:00<00:00, 13.56it/s]\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "#\n",
            "# To activate this environment, use:\n",
            "# > source activate newsqa\n",
            "#\n",
            "# To deactivate an active environment, use:\n",
            "# > source deactivate\n",
            "#\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!conda create --name newsqa python=2.7 \"pandas>=0.19.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJdytkrRAP7C",
        "outputId": "9fd79a74-4200-4f0b-830a-1b00a400ae86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] 2022-03-01 19:53:21,484 - data_processing.py::__init__\n",
            "Loading dataset from `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...\n",
            "[INFO] 2022-03-01 19:53:21,485 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...\n",
            "[INFO] 2022-03-01 19:53:22,167 - data_processing.py::__init__\n",
            "Loading stories from `/content/newsqa/maluuba/newsqa/cnn_stories.tgz`...\n",
            "Getting story texts: 100% 12.7k/12.7k [00:13<00:00, 974 stories/s]    \n",
            "Setting story texts: 100% 120k/120k [00:03<00:00, 36.4k questions/s] \n",
            "[INFO] 2022-03-01 19:53:38,565 - data_processing.py::__init__\n",
            "Done loading dataset.\n",
            "[INFO] 2022-03-01 19:53:38,675 - data_processing.py::dump\n",
            "Packaging dataset to `combined-newsqa-data-v1.json`.\n",
            "Building json: 100% 120k/120k [00:06<00:00, 18.0k questions/s]\n",
            "[INFO] 2022-03-01 19:53:52,757 - data_processing.py::dump\n",
            "Packaging dataset to `combined-newsqa-data-v1.csv`.\n",
            "[INFO] 2022-03-01 19:54:04,776 - data_processing.py::load_combined\n",
            "Loading data from `combined-newsqa-data-v1.csv`...\n",
            "Adjusting story texts: 100% 120k/120k [00:02<00:00, 59.8k questions/s]\n",
            "[INFO] 2022-03-01 19:54:10,183 - tokenize_dataset.py::tokenize\n",
            "Extracting dependencies from `/content/newsqa/maluuba/newsqa/stanford-postagger-2015-12-09.zip`.\n",
            "[INFO] 2022-03-01 19:54:10,208 - tokenize_dataset.py::tokenize\n",
            "(1/3) - Packing data to `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv.pck`.\n",
            "Packing: 100% 120k/120k [00:27<00:00, 4.42k questions/s]\n",
            "[INFO] 2022-03-01 19:54:37,300 - tokenize_dataset.py::tokenize\n",
            "(2/3) - Tokenizing packed file to `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv.tpck`.\n",
            "[INFO] 2022-03-01 19:54:37,300 - tokenize_dataset.py::tokenize\n",
            "Running `javac -classpath /content/newsqa/maluuba/newsqa:/content/newsqa/maluuba/newsqa/stanford-postagger.jar:/content/newsqa/maluuba/newsqa/slf4j-api.jar /content/newsqa/maluuba/newsqa/TokenizerSplitter.java`\n",
            "[INFO] 2022-03-01 19:54:39,250 - tokenize_dataset.py::tokenize\n",
            "Running `java -classpath /content/newsqa/maluuba/newsqa:/content/newsqa/maluuba/newsqa/stanford-postagger.jar:/content/newsqa/maluuba/newsqa/slf4j-api.jar TokenizerSplitter /content/newsqa/maluuba/newsqa/newsqa-data-v1.csv.pck > /content/newsqa/maluuba/newsqa/newsqa-data-v1.csv.tpck`\n",
            "The warnings below are normal.\n",
            "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
            "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
            "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
            "Mar 01, 2022 7:54:53 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:54:53 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:00 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:00 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:00 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:00 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:37 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:37 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:52 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:52 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:56 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:56 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:57 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:57 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:58 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:55:58 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:56:42 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "Mar 01, 2022 7:56:42 PM edu.stanford.nlp.process.PTBLexer next\n",
            "WARNING: Untokenizable: ‬ (U+202C, decimal: 8236)\n",
            "[INFO] 2022-03-01 19:56:58,460 - tokenize_dataset.py::tokenize\n",
            "(3/3) - Unpacking tokenized file to `/content/newsqa/maluuba/newsqa/newsqa-data-tokenized-v1.csv`\n",
            "Unpacking: 100% 120k/120k [02:52<00:00, 694 questions/s]\n",
            "[INFO] 2022-03-01 19:59:50,917 - tokenize_dataset.py::unpack\n",
            "Writing to `/content/newsqa/maluuba/newsqa/newsqa-data-tokenized-v1.csv`.\n",
            "[INFO] 2022-03-01 20:00:04,998 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/maluuba/newsqa/newsqa-data-tokenized-v1.csv`...\n",
            "Adjusting story texts: 100% 120k/120k [00:02<00:00, 59.4k questions/s]\n",
            "[INFO] 2022-03-01 20:00:11,169 - split_dataset.py::split_data\n",
            "Loading story ID's split.\n",
            "Splitting data: 100% 120k/120k [00:00<00:00, 138k questions/s]\n",
            "[INFO] 2022-03-01 20:00:12,059 - split_dataset.py::split_data\n",
            "Writing split data to split_data\n",
            "[INFO] 2022-03-01 20:00:12,059 - split_dataset.py::_write_to_csv\n",
            "Writing 92549 rows to split_data/train.csv\n",
            "[INFO] 2022-03-01 20:00:21,918 - split_dataset.py::_write_to_csv\n",
            "Writing 5166 rows to split_data/dev.csv\n",
            "[INFO] 2022-03-01 20:00:22,460 - split_dataset.py::_write_to_csv\n",
            "Writing 5126 rows to split_data/test.csv\n"
          ]
        }
      ],
      "source": [
        "!python2.7 newsqa/maluuba/newsqa/data_generator.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO7df-J2ErYw",
        "outputId": "52b89243-364b-48fd-b50f-acde15e4b7f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] 2022-03-01 20:02:13,041 - data_processing.py::__init__\n",
            "Loading dataset from `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...\n",
            "[INFO] 2022-03-01 20:02:13,041 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/maluuba/newsqa/newsqa-data-v1.csv`...\n",
            "[INFO] 2022-03-01 20:02:13,712 - data_processing.py::__init__\n",
            "Loading stories from `/content/newsqa/maluuba/newsqa/cnn_stories.tgz`...\n",
            "Getting story texts: 100% 12.7k/12.7k [00:13<00:00, 974 stories/s]    \n",
            "Setting story texts: 100% 120k/120k [00:03<00:00, 36.0k questions/s] \n",
            "[INFO] 2022-03-01 20:02:30,140 - data_processing.py::__init__\n",
            "Done loading dataset.\n",
            "Checking for possible corruption: 100% 120k/120k [00:01<00:00, 105k questions/s]\n",
            ".[INFO] 2022-03-01 20:02:31,400 - data_processing.py::dump\n",
            "Packaging dataset to `/content/newsqa/combined-newsqa-data-v1.json`.\n",
            "Building json: 100% 120k/120k [00:06<00:00, 19.5k questions/s] \n",
            "Checking for possible corruption: 100% 12.7k/12.7k [00:00<00:00, 15.3k stories/s]\n",
            "Gathering answers: 100% 120k/120k [00:01<00:00, 103k questions/s]\n",
            "..[INFO] 2022-03-01 20:02:52,829 - data_processing.py::dump\n",
            "Packaging dataset to `/content/newsqa/combined-newsqa-data-v1.csv`.\n",
            "[INFO] 2022-03-01 20:03:05,024 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/combined-newsqa-data-v1.csv`...\n",
            "Adjusting story texts: 100% 120k/120k [00:02<00:00, 55.7k questions/s]\n",
            "Comparing stories: 100% 120k/120k [00:29<00:00, 4.01k rows/s]\n",
            ".[INFO] 2022-03-01 20:03:40,745 - data_processing.py::load_combined\n",
            "Loading data from `/content/newsqa/maluuba/newsqa/newsqa-data-tokenized-v1.csv`...\n",
            "Adjusting story texts: 100% 120k/120k [00:01<00:00, 60.8k questions/s]\n",
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 12 tests in 93.857s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python2.7 -m unittest discover ./newsqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CmvzfLv5psz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import uuid\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "def answer_text(story_text, answer_token_ranges):\n",
        "    story_text_list = story_text.split()\n",
        "    if answer_token_ranges:\n",
        "        token_range = list(map(int, re.split(':|,', answer_token_ranges)))[0:2]\n",
        "        answer = ' '.join(story_text_list[slice(token_range[0], token_range[1])])\n",
        "        return answer\n",
        "\n",
        "\n",
        "def answer_start(story_text, answer_text):\n",
        "    if answer_text:\n",
        "        return story_text.find(answer_text)\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def newsqa_to_squad(newsqa_file, output_file):\n",
        "    newsqa = pd.read_csv(newsqa_file)\n",
        "    newsqa['answer_text'] = newsqa[['story_text', 'answer_token_ranges']].apply(lambda x: answer_text(*x), axis=1)\n",
        "    newsqa['answer_start'] = newsqa[['story_text', 'answer_text']].apply(lambda x: answer_start(*x), axis=1)\n",
        "    newsqa['id'] = newsqa['story_id'].apply(lambda x: str(uuid.uuid4().hex))\n",
        "    newsqa = newsqa[['id', 'story_text', 'question', 'answer_text', 'answer_start']]\n",
        "    newsqa_json = newsqa.to_json(orient='records')\n",
        "    newsqa_json = json.loads(newsqa_json)\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for newsqa_example in newsqa_json:\n",
        "        question_text = newsqa_example['question']\n",
        "        context = newsqa_example['story_text']\n",
        "\n",
        "        para = {'context': context, 'qas': [{'question': question_text, 'answers': []}]}\n",
        "        data.append({'paragraphs': [para]})\n",
        "        qa = para['qas'][0]\n",
        "        qa['id'] = newsqa_example['id']\n",
        "        qa['is_impossible'] = True\n",
        "\n",
        "        if newsqa_example['answer_start'] != -1:\n",
        "            ans_string = newsqa_example['answer_text']\n",
        "            index = newsqa_example['answer_start']\n",
        "            qa['answers'].append({'text': ans_string, 'answer_start': index})\n",
        "            qa['is_impossible'] = False\n",
        "\n",
        "    newsqa_as_squad = {'data': data, 'version': '2.0'}\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(newsqa_as_squad, outfile, indent=2, sort_keys=True, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmggrSur5yQw"
      },
      "outputs": [],
      "source": [
        "triviaqa_file_train = 'split_data/train.csv'\n",
        "output_file_train = 'newsqa_train_SquadFormat.json'\n",
        "newsqa_to_squad(triviaqa_file_train, output_file_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8mwK8N259QJ"
      },
      "outputs": [],
      "source": [
        "triviaqa_file_dev = 'split_data/dev.csv'\n",
        "output_file_dev = 'newsqa_dev_SquadFormat.json'\n",
        "newsqa_to_squad(triviaqa_file_dev, output_file_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGfiiHlfsW2k"
      },
      "source": [
        "## Natural Questions (NQ) dataset creation\n",
        "Download of NQ dataset and tranformation to SQuAD format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdvUJRljp3x9",
        "outputId": "548c152d-47f5-4884-9c29-c1e8fe040438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://natural_questions/v1.0/dev/nq-dev-02.jsonl.gz...\n",
            "/ [0/62 files][    0.0 B/ 42.0 GiB]   0% Done                                   \r",
            "Copying gs://natural_questions/v1.0/dev/nq-dev-03.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/dev/nq-dev-00.jsonl.gz...\n",
            "/ [0/62 files][    0.0 B/ 42.0 GiB]   0% Done                                   \r",
            "/ [0/62 files][    0.0 B/ 42.0 GiB]   0% Done                                   \r",
            "Copying gs://natural_questions/v1.0/dev/nq-dev-04.jsonl.gz...\n",
            "/ [0/62 files][    0.0 B/ 42.0 GiB]   0% Done                                   \r",
            "Copying gs://natural_questions/v1.0/sample/nq-train-sample.jsonl.gz...\n",
            "/ [0/62 files][    0.0 B/ 42.0 GiB]   0% Done                                   \r",
            "Copying gs://natural_questions/v1.0/LICENSE.txt...\n",
            "/ [0/62 files][    0.0 B/ 42.0 GiB]   0% Done                                   \r",
            "Copying gs://natural_questions/v1.0/sample/nq-dev-sample.jsonl.gz...\n",
            "/ [0/62 files][    0.0 B/ 42.0 GiB]   0% Done                                   \r",
            "Copying gs://natural_questions/v1.0/README.txt...\n",
            "/ [0/62 files][    0.0 B/ 42.0 GiB]   0% Done                                   \r",
            "Copying gs://natural_questions/v1.0/dev/nq-dev-01.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-00.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-01.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-02.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-03.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-04.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-05.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-06.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-07.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-08.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-09.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-10.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-11.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-12.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-13.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-14.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-15.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-16.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-17.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-18.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-19.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-20.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-21.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-22.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-23.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-24.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-25.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-26.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-27.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-28.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-29.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-30.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-31.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-32.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-33.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-34.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-35.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-36.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-37.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-38.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-39.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-40.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-41.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-42.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-43.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-44.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-45.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-46.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-47.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-48.jsonl.gz...\n",
            "Copying gs://natural_questions/v1.0/train/nq-train-49.jsonl.gz...\n"
          ]
        }
      ],
      "source": [
        "!gsutil -m cp -R gs://natural_questions/v1.0 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C_a0_jHscvU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import argparse\n",
        "import gzip\n",
        "import glob\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def has_long_answer(nq_example):\n",
        "    if len(nq_example['annotations']) == 1:\n",
        "        annotation = nq_example['annotations'][0]\n",
        "        return annotation['long_answer']['start_byte'] >= 0\n",
        "    else:\n",
        "        return sum([annotation['long_answer']['start_byte'] >= 0 for annotation in nq_example['annotations']]) >= 2\n",
        "\n",
        "\n",
        "def has_short_answer(nq_example):\n",
        "    if len(nq_example['annotations']) == 1:\n",
        "        annotation = nq_example['annotations'][0]\n",
        "        return annotation['short_answers'] or annotation['yes_no_answer'] != 'NONE'\n",
        "    else:\n",
        "        return sum([bool(annotation['short_answers']) or annotation['yes_no_answer'] != 'NONE'\n",
        "                    for annotation in nq_example['annotations']]) >= 2\n",
        "\n",
        "\n",
        "def render_answer(nq_example, start_byte, end_byte):\n",
        "    html = nq_example['document_html'].encode('utf-8')\n",
        "    answer_text = BeautifulSoup(html[start_byte:end_byte].decode('utf-8'), features='lxml').get_text()\n",
        "    return answer_text\n",
        "\n",
        "\n",
        "def get_long_answer(nq_example):\n",
        "    if has_long_answer(nq_example):\n",
        "        long_answers = [a['long_answer'] for a in nq_example['annotations'] if a['long_answer']['start_byte'] >= 0]\n",
        "        long_answer_bounds = [(la['start_byte'], la['end_byte']) for la in long_answers]\n",
        "        long_answer_counts = [long_answer_bounds.count(la) for la in long_answer_bounds]\n",
        "        long_answer = long_answers[np.argmax(long_answer_counts)]\n",
        "        html_tag = nq_example['document_tokens'][long_answer['end_token'] - 1]['token']\n",
        "        if html_tag == '</P>':\n",
        "            long_answer_text = render_answer(nq_example, long_answer['start_byte'], long_answer['end_byte'])\n",
        "            return long_answer_text\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_short_answers(nq_example):\n",
        "    if has_short_answer(nq_example):\n",
        "        short_answers = [a['short_answers'] for a in nq_example['annotations'] if a['short_answers']]\n",
        "        short_answers_texts = [\n",
        "            ', '.join([render_answer(nq_example, s['start_byte'], s['end_byte']) for s in short_answer])\n",
        "            for short_answer in short_answers]\n",
        "        short_answers_texts = set(short_answers_texts)\n",
        "        return short_answers_texts\n",
        "    return None\n",
        "\n",
        "\n",
        "def nq_to_squad_format(nq_dir, output_file):\n",
        "    data = []\n",
        "    for filename in glob.glob(nq_dir + '/*.gz'):\n",
        "        with gzip.open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                nq_example = json.loads(line)\n",
        "                long_answer_text = get_long_answer(nq_example)\n",
        "\n",
        "                if long_answer_text:\n",
        "                    question_text = nq_example['question_text']\n",
        "                    context = long_answer_text\n",
        "                    para = {'context': context, 'qas': [{'question': question_text, 'answers': []}]}\n",
        "                    data.append({'paragraphs': [para]})\n",
        "                    qa = para['qas'][0]\n",
        "                    qa['id'] = str(nq_example['example_id'])\n",
        "                    qa['is_impossible'] = True\n",
        "                    short_answer_texts = get_short_answers(nq_example)\n",
        "\n",
        "                    if short_answer_texts:\n",
        "                        for ans_string in short_answer_texts:\n",
        "                            index = context.find(ans_string)\n",
        "                            if index != -1:\n",
        "                                qa['answers'].append({'text': ans_string, 'answer_start': index})\n",
        "                                qa['is_impossible'] = False\n",
        "\n",
        "    nq_as_squad = {'data': data, 'version': '2.0'}\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        outfile.write(json.dumps(nq_as_squad, indent=2, sort_keys=True, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfylk_NPswRp",
        "outputId": "fba814ef-181c-42f3-ad6a-1708e5b4d850"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.example.com/index.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ]
        }
      ],
      "source": [
        "nq_to_squad_format('/content/v1.0/train', 'nq_train.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7QzwVwvRlc4"
      },
      "outputs": [],
      "source": [
        "nq_to_squad_format('/content/v1.0/dev', 'nq_dev.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPrZxSr0spi8"
      },
      "source": [
        "## Quac dataset creation\n",
        "Download of Quac dataset and tranformation to SQuAD format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-03-06T20:36:02.240630Z",
          "iopub.status.busy": "2022-03-06T20:36:02.240086Z",
          "iopub.status.idle": "2022-03-06T20:36:07.238164Z",
          "shell.execute_reply": "2022-03-06T20:36:07.237165Z",
          "shell.execute_reply.started": "2022-03-06T20:36:02.240519Z"
        },
        "id": "M5Bt7lDLsls-",
        "outputId": "8709c6e3-b22b-48d4-9b40-d7281ef5b2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-06 20:36:02--  https://s3.amazonaws.com/my89public/quac/train_v0.2.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.171.80\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.171.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68114819 (65M) [application/json]\n",
            "Saving to: ‘train_v0.2.json’\n",
            "\n",
            "train_v0.2.json     100%[===================>]  64.96M  34.6MB/s    in 1.9s    \n",
            "\n",
            "2022-03-06 20:36:05 (34.6 MB/s) - ‘train_v0.2.json’ saved [68114819/68114819]\n",
            "\n",
            "--2022-03-06 20:36:06--  https://s3.amazonaws.com/my89public/quac/val_v0.2.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.171.80\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.171.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8929167 (8.5M) [application/json]\n",
            "Saving to: ‘val_v0.2.json’\n",
            "\n",
            "val_v0.2.json       100%[===================>]   8.51M  15.0MB/s    in 0.6s    \n",
            "\n",
            "2022-03-06 20:36:07 (15.0 MB/s) - ‘val_v0.2.json’ saved [8929167/8929167]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3.amazonaws.com/my89public/quac/train_v0.2.json\n",
        "!wget https://s3.amazonaws.com/my89public/quac/val_v0.2.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T20:36:13.917547Z",
          "iopub.status.busy": "2022-03-06T20:36:13.917217Z",
          "iopub.status.idle": "2022-03-06T20:36:13.927922Z",
          "shell.execute_reply": "2022-03-06T20:36:13.927143Z",
          "shell.execute_reply.started": "2022-03-06T20:36:13.917514Z"
        },
        "id": "kWxvkJJfsltE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import argparse\n",
        "\n",
        "\n",
        "def quac_to_squad(quac_file, output_file):\n",
        "    quac_json = json.load(open(quac_file))\n",
        "    data = []\n",
        "\n",
        "    for quac_example in quac_json['data']:\n",
        "        context = quac_example['background'] # use background info as context\n",
        "        for p in quac_example['paragraphs']:\n",
        "#             context = p['context']\n",
        "            for q in p['qas']:\n",
        "                question_text = q['question']\n",
        "                para = {'context': context, 'qas': [{'question': question_text, 'answers': []}]}\n",
        "                data.append({'paragraphs': [para]})\n",
        "                qa = para['qas'][0]\n",
        "                qa['id'] = q['id']\n",
        "                qa['is_impossible'] = True\n",
        "\n",
        "                if q['orig_answer']['text'] != 'CANNOTANSWER':\n",
        "                    ans_string = q['orig_answer']['text']\n",
        "                    index = q['orig_answer']['answer_start']\n",
        "                    qa['answers'].append({'text': ans_string, 'answer_start': index})\n",
        "                    qa['is_impossible'] = False\n",
        "\n",
        "    quac_as_squad = {'data': data, 'version': '2.0'}\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(quac_as_squad, outfile, indent=2, sort_keys=True, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T20:36:16.173293Z",
          "iopub.status.busy": "2022-03-06T20:36:16.172786Z",
          "iopub.status.idle": "2022-03-06T20:36:23.404431Z",
          "shell.execute_reply": "2022-03-06T20:36:23.403692Z",
          "shell.execute_reply.started": "2022-03-06T20:36:16.173226Z"
        },
        "id": "K9FPGKdmsltG"
      },
      "outputs": [],
      "source": [
        "quac_file_train = 'train_v0.2.json'\n",
        "output_file_train = 'quac_train_SquadFormat.json'\n",
        "quac_to_squad(quac_file_train, output_file_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T20:36:25.958012Z",
          "iopub.status.busy": "2022-03-06T20:36:25.957613Z",
          "iopub.status.idle": "2022-03-06T20:36:26.641398Z",
          "shell.execute_reply": "2022-03-06T20:36:26.640746Z",
          "shell.execute_reply.started": "2022-03-06T20:36:25.957976Z"
        },
        "id": "k6RQZrepsltH"
      },
      "outputs": [],
      "source": [
        "quac_file_dev = 'val_v0.2.json'\n",
        "output_file_dev = 'quac_dev_SquadFormat.json'\n",
        "quac_to_squad(quac_file_dev, output_file_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmHqOUt8xVKC"
      },
      "source": [
        "# Fine-Tuning and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-28T18:34:15.161934Z",
          "iopub.status.busy": "2022-02-28T18:34:15.161185Z",
          "iopub.status.idle": "2022-02-28T18:34:23.621117Z",
          "shell.execute_reply": "2022-02-28T18:34:23.620326Z",
          "shell.execute_reply.started": "2022-02-28T18:34:15.161843Z"
        },
        "id": "11XXafyovmOA"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:21:31.502782Z",
          "iopub.status.busy": "2022-03-04T11:21:31.501985Z",
          "iopub.status.idle": "2022-03-04T11:21:37.784257Z",
          "shell.execute_reply": "2022-03-04T11:21:37.783515Z",
          "shell.execute_reply.started": "2022-03-04T11:21:31.502641Z"
        },
        "id": "poCfDJb_cpMm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from transformers import DistilBertTokenizerFast, BertTokenizerFast, BertTokenizer, DistilBertForQuestionAnswering, BertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:22:08.280676Z",
          "iopub.status.busy": "2022-03-04T11:22:08.279764Z",
          "iopub.status.idle": "2022-03-04T11:22:08.326380Z",
          "shell.execute_reply": "2022-03-04T11:22:08.325607Z",
          "shell.execute_reply.started": "2022-03-04T11:22:08.280636Z"
        },
        "id": "PWXpkRFZdwpA"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWMUWI5LBVcQ"
      },
      "outputs": [],
      "source": [
        "# create a file that contains the evaluation script provided by SQuAD website, in order to use it for the evaluation\n",
        "import urllib.request\n",
        "url = \"https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\"\n",
        "uf = urllib.request.urlopen(url)  # open the url that contains the script code\n",
        "html = uf.read().decode('utf-8')  # read the code\n",
        "f = open(\"evaluation.py\", \"w\")  # create a new file\n",
        "f.write(html) # write the evalutation script code to the new file\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkZwRLGxBHE9"
      },
      "source": [
        "<a name=\"squad_finetune\"></a>\n",
        "## SQUAD FINETUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:21:52.952648Z",
          "iopub.status.busy": "2022-03-04T11:21:52.952124Z",
          "iopub.status.idle": "2022-03-04T11:21:52.956866Z",
          "shell.execute_reply": "2022-03-04T11:21:52.956036Z",
          "shell.execute_reply.started": "2022-03-04T11:21:52.952608Z"
        },
        "id": "tfvzQyx0dMc9"
      },
      "outputs": [],
      "source": [
        "# the train and dev files were downloaded in paragraph Data Creation, so store the names of the files\n",
        "train_path = r'train-v2.0.json'\n",
        "validation_path = r'dev-v2.0.json'\n",
        "\n",
        "MAX_LENGTH = 512  # max length that will be used during tokenization\n",
        "USE_IMPOSSIBLE_QNAS = True  # include also impossible questions in the training set\n",
        "USE_ALL_QNAS = True # use the entire dataset in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:22:09.899815Z",
          "iopub.status.busy": "2022-03-04T11:22:09.899449Z",
          "iopub.status.idle": "2022-03-04T11:22:12.198381Z",
          "shell.execute_reply": "2022-03-04T11:22:12.197642Z",
          "shell.execute_reply.started": "2022-03-04T11:22:09.899768Z"
        },
        "id": "QWU5Az69cqYG"
      },
      "outputs": [],
      "source": [
        "def read_file(file_name): # read data from .json file\n",
        "  with open(file_name, 'rb') as file: # open the file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain all the context that correspond to every question/answer set\n",
        "  question_list = []  # list that will contain all the quections. Questions that have multiple answers will be inserted multiple times at the list\n",
        "  answer_list = []  # list that will contain all the answers to the questions\n",
        "  impossible_list = []  # list that will contatin if the corresponding question in the question list is impossible or not\n",
        "  squad = squad['data'] # get the data\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this paragraph\n",
        "              question = qa['question'] # store the question text\n",
        "              impossible = qa['is_impossible']  # store if the question is impossible or not\n",
        "              if impossible and USE_IMPOSSIBLE_QNAS:  # if the question is impossible\n",
        "                  context_list.append(context)  # add the context of this question to the list\n",
        "                  question_list.append(question)  # add the question to the list\n",
        "                  answer_list.append({'text':\"\",'answer_start':0,'answer_end':0})   # add the empty string as the answer of the question\n",
        "                  impossible_list.append(impossible)    # store whether the question is impossible or not\n",
        "              for answer in qa['answers']: # if the question is possible, then it has one or more correct answers\n",
        "                  answer['answer_end'] = answer['answer_start'] + len(answer['text']) # find the end index of the answer\n",
        "                  answer_start_index = answer['answer_start'] # store tha start index of the answer\n",
        "                  answer_end_index = answer['answer_end'] # store tha end index of the answer\n",
        "                  answer_text = answer['text']  # get the answer text\n",
        "                  count = 0 # offset of the indexes\n",
        "                  # the answer start/end index may be off by some characters, so correct them\n",
        "                  while True:\n",
        "                    if context[answer_start_index-count:answer_end_index-count] == answer_text: # if we have the correct start/end indexes\n",
        "                      # store them in the answer dictionary\n",
        "                      answer['answer_start'] = answer_start_index - count\n",
        "                      answer['answer_end'] = answer_end_index - count\n",
        "                      break\n",
        "                    if count>4: # never actualy happens, just for safety reasons to avoid infinite loop\n",
        "                        break\n",
        "                    count = count + 1 # if the indexes are not correct, increase the offset in order to \"move\" the indexes\n",
        "                  # if USE_ALL_QNAS is true, then we use every question/answer\n",
        "                  # if USE_ALL_QNAS is false, the we only use the questions/answers that have total words < MAX_LENGTH\n",
        "                  if(len(context.split())+len(question.split())<MAX_LENGTH) or USE_ALL_QNAS:\n",
        "                    context_list.append(context)  # add the context of this question to the list\n",
        "                    question_list.append(question)  # add the question to the list\n",
        "                    answer_list.append(answer)  # add the answer to the list\n",
        "                    impossible_list.append(impossible)  # store whether the question is impossible or not\n",
        "\n",
        "  return context_list, question_list, answer_list, impossible_list\n",
        "\n",
        "train_contexts, train_questions, train_answers, train_impossible = read_file(train_path) # read the training set file\n",
        "valid_contexts, valid_questions, valid_answers, valid_impossible = read_file(validation_path) # read the validation/dev set file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:22:15.679073Z",
          "iopub.status.busy": "2022-03-04T11:22:15.678529Z",
          "iopub.status.idle": "2022-03-04T11:22:15.686938Z",
          "shell.execute_reply": "2022-03-04T11:22:15.685927Z",
          "shell.execute_reply.started": "2022-03-04T11:22:15.679031Z"
        },
        "id": "ww04A0fgflD6",
        "outputId": "4d5c3c28-8b77-4a9c-a423-ea86c6753b4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "130319"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_contexts) # how many questions/answers we have in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6d41c06a22bd4ff7acc804181edc6f16",
            "43c59e9fe15f44629c6f4f4c8ef39427",
            "856252c6debc4b0b914bbf286c7b7cc4",
            "9027efb0ae404244be2c7a841955d342"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-04T11:22:17.312729Z",
          "iopub.status.busy": "2022-03-04T11:22:17.312459Z",
          "iopub.status.idle": "2022-03-04T11:23:47.968713Z",
          "shell.execute_reply": "2022-03-04T11:23:47.967938Z",
          "shell.execute_reply.started": "2022-03-04T11:22:17.312699Z"
        },
        "id": "fLB1Pzj7uqB1",
        "outputId": "4ea5ed74-4c20-490b-9a43-ea62db5a9daf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d41c06a22bd4ff7acc804181edc6f16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43c59e9fe15f44629c6f4f4c8ef39427",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "856252c6debc4b0b914bbf286c7b7cc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9027efb0ae404244be2c7a841955d342",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') # initialize tokenizer\n",
        "# tokenize the training set, giving contexts and questions to the tokenizer\n",
        "train_tokenized = tokenizer(train_contexts, train_questions, truncation=True, padding=True, max_length = MAX_LENGTH)\n",
        "# tokenize the validation set, giving contexts and questions to the tokenizer\n",
        "validation_tokenized = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True, max_length = MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQXUfG_aX5av"
      },
      "outputs": [],
      "source": [
        "def index_answer(encodings, answers, impossible):\n",
        "    # after the tokenization, we need to convert the start/end indexes to the corresponding token indexes\n",
        "    start_positions = []  # list that will contain all the start indexes\n",
        "    end_positions = []  # list that will contain all the end indexes\n",
        "    for i in range(len(answers)): # for every answer\n",
        "        if impossible[i]: # if the question is impossible\n",
        "            # find the index of the first cls token (= 0)\n",
        "            sep_index = encodings['input_ids'][i].index(tokenizer.cls_token_id)\n",
        "            # set the start and the end index as the cls token index, as the correct answer is the empty string\n",
        "            start_positions.append(sep_index)\n",
        "            end_positions.append(sep_index)\n",
        "            continue\n",
        "        # if the question is not impossible\n",
        "        # convert the start index to the corresponding token index\n",
        "        start_positions.append(encodings[i].char_to_token(answers[i]['answer_start']))\n",
        "        # convert the end index to the corresponding token index \n",
        "        end_positions.append(encodings[i].char_to_token(answers[i]['answer_end']))\n",
        "\n",
        "        if start_positions[-1] is None: # if the start index was not found (due to the truncation of max_length)\n",
        "            # the answer is not inside the tokenized context\n",
        "            start_positions[-1] = tokenizer.model_max_length # assign the last token as the start\n",
        "\n",
        "        shift = 1\n",
        "        while end_positions[-1] is None: # if the end index was not found\n",
        "            # decrease the original index until finding a corresponding token index\n",
        "            end_positions[-1] = encodings[i].char_to_token(answers[i]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "    return start_positions, end_positions\n",
        "\n",
        "train_start,train_end = index_answer(train_tokenized, train_answers,train_impossible)\n",
        "validation_start,validation_end = index_answer(validation_tokenized, valid_answers, valid_impossible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:25:44.233879Z",
          "iopub.status.busy": "2022-03-04T11:25:44.232979Z",
          "iopub.status.idle": "2022-03-04T11:25:44.263860Z",
          "shell.execute_reply": "2022-03-04T11:25:44.262930Z",
          "shell.execute_reply.started": "2022-03-04T11:25:44.233834Z"
        },
        "id": "dXT5bfWbflD9"
      },
      "outputs": [],
      "source": [
        "train_id = [tokenized for tokenized in train_tokenized['input_ids']] # store all the input_ids of the training set into a list\n",
        "train_mask = [tokenized for tokenized in train_tokenized['attention_mask']] # store all the attention_masks of the training set into a list\n",
        "validation_id = [tokenized for tokenized in validation_tokenized['input_ids']] # store all the input_ids of the validation set into a list\n",
        "validation_mask = [tokenized for tokenized in validation_tokenized['attention_mask']] # store all the attention_masks of the validation set into a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:26:02.917523Z",
          "iopub.status.busy": "2022-03-04T11:26:02.916958Z",
          "iopub.status.idle": "2022-03-04T11:26:05.599521Z",
          "shell.execute_reply": "2022-03-04T11:26:05.598774Z",
          "shell.execute_reply.started": "2022-03-04T11:26:02.917483Z"
        },
        "id": "xcW6c8g2flD9"
      },
      "outputs": [],
      "source": [
        "del train_tokenized,validation_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:26:12.605871Z",
          "iopub.status.busy": "2022-03-04T11:26:12.605577Z",
          "iopub.status.idle": "2022-03-04T11:26:30.600288Z",
          "shell.execute_reply": "2022-03-04T11:26:30.599508Z",
          "shell.execute_reply.started": "2022-03-04T11:26:12.605838Z"
        },
        "id": "05iHKg3AoGmL"
      },
      "outputs": [],
      "source": [
        "# convert all training data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "train_id_tensor = torch.tensor(train_id)\n",
        "train_mask_tensor = torch.tensor(train_mask)\n",
        "train_starts_tensors = torch.tensor(train_start)\n",
        "train_ends_tensors = torch.tensor(train_end)\n",
        "train_impossible_tensors = torch.tensor(train_impossible, dtype=torch.bool)\n",
        "# convert all validation data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "validation_id_tensor = torch.tensor(validation_id)\n",
        "validation_mask_tensor = torch.tensor(validation_mask)\n",
        "validation_starts_tensors = torch.tensor(validation_start)\n",
        "validation_ends_tensors = torch.tensor(validation_end)\n",
        "validation_impossible_tensors = torch.tensor(valid_impossible, dtype=torch.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:28:08.529635Z",
          "iopub.status.busy": "2022-03-04T11:28:08.529163Z",
          "iopub.status.idle": "2022-03-04T11:28:09.227177Z",
          "shell.execute_reply": "2022-03-04T11:28:09.226410Z",
          "shell.execute_reply.started": "2022-03-04T11:28:08.529597Z"
        },
        "id": "ZZzuK0reflD-"
      },
      "outputs": [],
      "source": [
        "del train_id,train_mask,train_start,train_end,train_impossible\n",
        "del validation_id,validation_mask,validation_start,validation_end,valid_impossible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1685bb7dca9e434788b73f5298cc7f36"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-04T11:28:20.606208Z",
          "iopub.status.busy": "2022-03-04T11:28:20.605171Z",
          "iopub.status.idle": "2022-03-04T11:28:36.211187Z",
          "shell.execute_reply": "2022-03-04T11:28:36.210425Z",
          "shell.execute_reply.started": "2022-03-04T11:28:20.606160Z"
        },
        "id": "-4wewxY7pbx0",
        "outputId": "d1692b8b-3590-4d09-da0e-0db5f4d19560"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1685bb7dca9e434788b73f5298cc7f36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda...\n"
          ]
        }
      ],
      "source": [
        "# define epochs\n",
        "TRAINING_EPOCHS = 2\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 1e-5\n",
        "batch_size = 16\n",
        "# define the model\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate)\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.to(device)\n",
        "  print('Using cuda...')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:29:06.592659Z",
          "iopub.status.busy": "2022-03-04T11:29:06.592046Z",
          "iopub.status.idle": "2022-03-04T11:29:06.598692Z",
          "shell.execute_reply": "2022-03-04T11:29:06.597927Z",
          "shell.execute_reply.started": "2022-03-04T11:29:06.592619Z"
        },
        "id": "ftViCfvQflD_"
      },
      "outputs": [],
      "source": [
        "#Initialize dataloader for train set\n",
        "train_dataset = torch.utils.data.TensorDataset(train_id_tensor, train_mask_tensor,train_starts_tensors,train_ends_tensors,train_impossible_tensors)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "del train_dataset\n",
        "#Initialize dataloader for validation set\n",
        "validation_dataset = torch.utils.data.TensorDataset(validation_id_tensor, validation_mask_tensor,validation_starts_tensors,validation_ends_tensors,validation_impossible_tensors)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "del validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T11:29:14.865372Z",
          "iopub.status.busy": "2022-03-04T11:29:14.865097Z",
          "iopub.status.idle": "2022-03-04T15:41:29.133783Z",
          "shell.execute_reply": "2022-03-04T15:41:29.133108Z",
          "shell.execute_reply.started": "2022-03-04T11:29:14.865339Z"
        },
        "id": "QCNV6C7xjTTI",
        "outputId": "8554fec9-1c44-4cc0-c676-f5ebaef1deec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8145/8145 [1:58:18<00:00,  1.15it/s]\n",
            "100%|██████████| 1641/1641 [07:48<00:00,  3.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0: Train Loss = 1.59697 | Validation Loss = 1.39957 | Train Accuracy = 0.5348 | Validation Accuracy = 0.5698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8145/8145 [1:58:17<00:00,  1.15it/s]\n",
            "100%|██████████| 1641/1641 [07:49<00:00,  3.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1: Train Loss = 0.94147 | Validation Loss = 1.33266 | Train Accuracy = 0.6183 | Validation Accuracy = 0.6011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train_model(model,train_loader,validation_loader,optimizer,epochs = 5):\n",
        "  train_loss = [] # store the train loss for every epoch\n",
        "  valid_loss = []  # store the validation loss for every epoch\n",
        "  train_accuracy = []\n",
        "  valid_accuracy = []\n",
        "  for epoch in range(epochs):\n",
        "    train_batch_losses = [] # store the loss of every batch of the train set\n",
        "    validation_batch_losses = [] # store the loss of every batch of the validation set\n",
        "    model.train() # set the model to training mode\n",
        "    y_total_predict_train = []   # store the predictions of the the train set\n",
        "    y_total_train = []     # store the labels of the the train set\n",
        "    train_loader_tqdm = tqdm(train_loader)\n",
        "    for x_batch, mask, start, end, impossible in train_loader_tqdm:\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      start = start.to(device)\n",
        "      end = end.to(device)\n",
        "      mask = mask.to(device)\n",
        "      #Delete previously stored gradients\n",
        "      optimizer.zero_grad()\n",
        "      z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)\n",
        "      loss = z[0] # get the output loss\n",
        "      train_batch_losses.append(loss.item()) # store the train loss of the current batch\n",
        "      #Perform backpropagation starting from the loss calculated in this epoch\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(),1.0)  # perform gradient clipping\n",
        "      #Update model's weights based on the gradients calculated during backprop\n",
        "      optimizer.step()\n",
        "      start_pred = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "      # calculate accuracy for both and append to accuracy list\n",
        "      train_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "      train_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "      del x_batch, mask, start, end\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()   # set the model to evaluation mode\n",
        "    y_total_valid = []   # store the predictions of the the validation set\n",
        "    y_total_predict_valid = []      # store the labels of the the validation set\n",
        "    validation_loader_tqdm = tqdm(validation_loader)\n",
        "    with torch.no_grad():\n",
        "        for x_batch, mask, start, end, impossible in validation_loader_tqdm:\n",
        "          # transfer data to GPU\n",
        "          x_batch = x_batch.to(device)\n",
        "          start = start.to(device)\n",
        "          end = end.to(device)\n",
        "          mask = mask.to(device)\n",
        "          z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)  \n",
        "          loss = z[0]  # get the output loss\n",
        "          validation_batch_losses.append(loss.item())   # store the validation loss of the current batch\n",
        "          start_pred = torch.argmax(z['start_logits'], dim=1)  # find the predicted start index, based on the bigger logit\n",
        "          end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "          # calculate accuracy for both and append to accuracy list\n",
        "          valid_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "          valid_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "          del x_batch, mask, start, end\n",
        "    t_accuracy = sum(train_accuracy)/len(train_accuracy) # compute train accuracy based on the correct predictions\n",
        "    v_accuracy = sum(valid_accuracy)/len(valid_accuracy) # compute validation accuracy based on the correct predictions\n",
        "    current_train_loss = sum(train_batch_losses)/len(train_loader)  #compute the train loss of this epoch\n",
        "    train_loss.append(current_train_loss)   # store the train loss in order to plot loss curves\n",
        "    current_valid_loss = sum(validation_batch_losses)/len(validation_loader)   #compute the validation loss of this epoch\n",
        "    valid_loss.append(current_valid_loss)   # store the validation loss in order to plot loss curves\n",
        "    print(f\"Epoch {epoch:3}: Train Loss = {current_train_loss:.5f} | Validation Loss = {current_valid_loss:.5f} | Train Accuracy = {t_accuracy:.4f} | Validation Accuracy = {v_accuracy:.4f}\")\n",
        "  return model,train_loss,valid_loss\n",
        "\n",
        "# train the model\n",
        "trained_model, train_loss, valid_loss = train_model(model,train_loader,validation_loader,optimizer,TRAINING_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zlIu3KwB_LN"
      },
      "source": [
        "<a name=\"squad_evaluation\"></a>\n",
        "### SQuAD fined-tuned model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T15:43:31.854131Z",
          "iopub.status.busy": "2022-03-04T15:43:31.853878Z",
          "iopub.status.idle": "2022-03-04T15:47:15.116958Z",
          "shell.execute_reply": "2022-03-04T15:47:15.116121Z",
          "shell.execute_reply.started": "2022-03-04T15:43:31.854102Z"
        },
        "id": "kmjmfIQsngfa",
        "outputId": "a3838bf8-2b26-414b-b919-5cb84e9174b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 64.92040764760381,\n",
            "  \"f1\": 69.37908304492896,\n",
            "  \"total\": 11873,\n",
            "  \"HasAns_exact\": 60.82995951417004,\n",
            "  \"HasAns_f1\": 69.76009665864375,\n",
            "  \"HasAns_total\": 5928,\n",
            "  \"NoAns_exact\": 68.99915895710681,\n",
            "  \"NoAns_f1\": 68.99915895710681,\n",
            "  \"NoAns_total\": 5945\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {} # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    for x_batch, mask, id_index in test_loader: # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0] # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)): # for every item of the batch\n",
        "        i = (int(id_index_i)) # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the CLS token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.cls_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the CLS token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py dev-v2.0.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyOoGBgfA8iP"
      },
      "source": [
        "### SQuAD fined-tuned model test on TRIVIA QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:59:06.432410Z",
          "iopub.status.busy": "2022-03-06T22:59:06.431120Z",
          "iopub.status.idle": "2022-03-06T22:59:06.437686Z",
          "shell.execute_reply": "2022-03-06T22:59:06.436999Z",
          "shell.execute_reply.started": "2022-03-06T22:59:06.432358Z"
        },
        "id": "1tvlRUMPA8iQ"
      },
      "outputs": [],
      "source": [
        "validation_path = 'triviaqa_dev_SquadFormat.json' # path to the validation-dev set of TRIVIA QA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:59:09.278427Z",
          "iopub.status.busy": "2022-03-06T22:59:09.278107Z",
          "iopub.status.idle": "2022-03-06T23:03:59.394817Z",
          "shell.execute_reply": "2022-03-06T23:03:59.393941Z",
          "shell.execute_reply.started": "2022-03-06T22:59:09.278392Z"
        },
        "id": "yWKLQpYpA8iR",
        "outputId": "9600e6f8-2ec9-4ee8-f509-37a13b42011a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [04:05<00:00,  1.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 36.39047016656125,\n",
            "  \"f1\": 40.263927771473604,\n",
            "  \"total\": 14229,\n",
            "  \"HasAns_exact\": 17.98414956309693,\n",
            "  \"HasAns_f1\": 23.584172755567558,\n",
            "  \"HasAns_total\": 9842,\n",
            "  \"NoAns_exact\": 77.68406656029177,\n",
            "  \"NoAns_f1\": 77.68406656029177,\n",
            "  \"NoAns_total\": 4387\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {} # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm: # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0] # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1)  # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)): # for every item of the batch\n",
        "        i = (int(id_index_i)) # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the CLS token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.cls_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the CLS token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py triviaqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sh94PZSA8iR"
      },
      "source": [
        "### SQuAD fined-tuned model test on NEWS QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:04:47.223260Z",
          "iopub.status.busy": "2022-03-06T23:04:47.222965Z",
          "iopub.status.idle": "2022-03-06T23:04:47.226991Z",
          "shell.execute_reply": "2022-03-06T23:04:47.226207Z",
          "shell.execute_reply.started": "2022-03-06T23:04:47.223207Z"
        },
        "id": "kyoSBVG0A8iS"
      },
      "outputs": [],
      "source": [
        "validation_path = 'newsqa_dev_SquadFormat.json'  # path to the validation-dev set of NEWS QA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:04:49.167866Z",
          "iopub.status.busy": "2022-03-06T23:04:49.167137Z",
          "iopub.status.idle": "2022-03-06T23:06:37.924589Z",
          "shell.execute_reply": "2022-03-06T23:06:37.923758Z",
          "shell.execute_reply.started": "2022-03-06T23:04:49.167815Z"
        },
        "id": "tDtA2rdfA8iS",
        "outputId": "eace9a13-9c8e-4a3c-b263-1fc6ac287bd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 81/81 [01:30<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 16.78281068524971,\n",
            "  \"f1\": 25.44817225388683,\n",
            "  \"total\": 5166,\n",
            "  \"HasAns_exact\": 16.78281068524971,\n",
            "  \"HasAns_f1\": 25.44817225388683,\n",
            "  \"HasAns_total\": 5166\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {} # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm: # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0] # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i)) # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the CLS token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.cls_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the CLS token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py newsqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkjICrRJA8iT"
      },
      "source": [
        "### SQuAD fined-tuned model test on Natural Questions (NQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:07:19.660013Z",
          "iopub.status.busy": "2022-03-06T23:07:19.659524Z",
          "iopub.status.idle": "2022-03-06T23:07:19.664412Z",
          "shell.execute_reply": "2022-03-06T23:07:19.663687Z",
          "shell.execute_reply.started": "2022-03-06T23:07:19.659969Z"
        },
        "id": "u5qgUSsEA8iT"
      },
      "outputs": [],
      "source": [
        "validation_path = 'nq_dev.json'  # path to the validation-dev set of NQ dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:07:22.041759Z",
          "iopub.status.busy": "2022-03-06T23:07:22.040844Z",
          "iopub.status.idle": "2022-03-06T23:08:28.106498Z",
          "shell.execute_reply": "2022-03-06T23:08:28.105669Z",
          "shell.execute_reply.started": "2022-03-06T23:07:22.041713Z"
        },
        "id": "83Dr-2taA8iT",
        "outputId": "4952b3bc-75a7-450c-e4b9-048b43a50211"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 53/53 [00:58<00:00,  1.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 37.399821905609976,\n",
            "  \"f1\": 44.70900438891064,\n",
            "  \"total\": 3369,\n",
            "  \"HasAns_exact\": 36.79966044142615,\n",
            "  \"HasAns_f1\": 47.25154320298811,\n",
            "  \"HasAns_total\": 2356,\n",
            "  \"NoAns_exact\": 38.79565646594274,\n",
            "  \"NoAns_f1\": 38.79565646594274,\n",
            "  \"NoAns_total\": 1013\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# intialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {} # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:  # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0] # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)): # for every item of the batch\n",
        "        i = (int(id_index_i)) # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the CLS token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.cls_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the CLS token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py nq_dev.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHTwtiypA8iU"
      },
      "source": [
        "### SQuAD fined-tuned model test on Quac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:08:36.169441Z",
          "iopub.status.busy": "2022-03-06T23:08:36.169148Z",
          "iopub.status.idle": "2022-03-06T23:08:36.173576Z",
          "shell.execute_reply": "2022-03-06T23:08:36.172658Z",
          "shell.execute_reply.started": "2022-03-06T23:08:36.169410Z"
        },
        "id": "e4gxul0SA8iU"
      },
      "outputs": [],
      "source": [
        "validation_path = 'quac_dev_SquadFormat.json'  # path to the validation-dev set of Quac dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:08:38.566411Z",
          "iopub.status.busy": "2022-03-06T23:08:38.565991Z",
          "iopub.status.idle": "2022-03-06T23:09:43.588365Z",
          "shell.execute_reply": "2022-03-06T23:09:43.587539Z",
          "shell.execute_reply.started": "2022-03-06T23:08:38.566373Z"
        },
        "id": "Iku-18Q-A8iU",
        "outputId": "3f5e5053-6720-41ac-81aa-8ede47de1f91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 115/115 [00:56<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 11.53113951590971,\n",
            "  \"f1\": 13.938658490054106,\n",
            "  \"total\": 7354,\n",
            "  \"HasAns_exact\": 0.34083162917518744,\n",
            "  \"HasAns_f1\": 3.358025653690845,\n",
            "  \"HasAns_total\": 5868,\n",
            "  \"NoAns_exact\": 55.720053835800805,\n",
            "  \"NoAns_f1\": 55.720053835800805,\n",
            "  \"NoAns_total\": 1486\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:  # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]  # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1  # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):   # for every item of the batch\n",
        "        i = (int(id_index_i))  # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the CLS token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.cls_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the CLS token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py quac_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4IwDIgVCqdB"
      },
      "source": [
        "## TRIVIA QA FINETUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:39:45.432967Z",
          "iopub.status.busy": "2022-03-04T13:39:45.432138Z",
          "iopub.status.idle": "2022-03-04T13:39:45.438025Z",
          "shell.execute_reply": "2022-03-04T13:39:45.436705Z",
          "shell.execute_reply.started": "2022-03-04T13:39:45.432927Z"
        },
        "id": "kT7GWFerg0VT"
      },
      "outputs": [],
      "source": [
        "# the train and dev files were downloaded and transformed in paragraph Data Creation, so store the names of the files\n",
        "output_file_train = 'triviaqa_train_SquadFormat.json'\n",
        "output_file_dev = 'triviaqa_dev_SquadFormat.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:39:47.389110Z",
          "iopub.status.busy": "2022-03-04T13:39:47.388676Z",
          "iopub.status.idle": "2022-03-04T13:39:47.394236Z",
          "shell.execute_reply": "2022-03-04T13:39:47.393064Z",
          "shell.execute_reply.started": "2022-03-04T13:39:47.389061Z"
        },
        "id": "MMoRkI57w7Qo"
      },
      "outputs": [],
      "source": [
        "train_path = output_file_train\n",
        "validation_path = output_file_dev\n",
        "\n",
        "MAX_LENGTH = 512  # max length that will be used during tokenization\n",
        "USE_IMPOSSIBLE_QNAS = True  # include also impossible questions in the training set\n",
        "KEEP_ALL_LENGTH_QNAS = False # don't use the entire dataset in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:39:49.059616Z",
          "iopub.status.busy": "2022-03-04T13:39:49.058932Z",
          "iopub.status.idle": "2022-03-04T13:39:49.121913Z",
          "shell.execute_reply": "2022-03-04T13:39:49.120483Z",
          "shell.execute_reply.started": "2022-03-04T13:39:49.059578Z"
        },
        "id": "0jartp99w7Qo"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:39:56.435036Z",
          "iopub.status.busy": "2022-03-04T13:39:56.434461Z",
          "iopub.status.idle": "2022-03-04T13:40:12.101054Z",
          "shell.execute_reply": "2022-03-04T13:40:12.100003Z",
          "shell.execute_reply.started": "2022-03-04T13:39:56.434986Z"
        },
        "id": "Lsu7J3CCw7Qp"
      },
      "outputs": [],
      "source": [
        "def read_file(file_name): # read data from .json file\n",
        "  with open(file_name, 'rb') as file: # open the file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain all the context that correspond to every question/answer set\n",
        "  question_list = []  # list that will contain all the quections. Questions that have multiple answers will be inserted multiple times at the list\n",
        "  answer_list = []  # list that will contain all the answers to the questions\n",
        "  impossible_list = []  # list that will contatin if the corresponding question in the question list is impossible or not\n",
        "  squad = squad['data'] # get the data\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this paragraph\n",
        "              question = qa['question'] # store the question text\n",
        "              impossible = qa['is_impossible']  # store if the question is impossible or not\n",
        "              if impossible and USE_IMPOSSIBLE_QNAS:  # if the question is impossible\n",
        "                  context_list.append(context)  # add the context of this question to the list\n",
        "                  question_list.append(question)  # add the question to the list\n",
        "                  # add the empty string as the answer of the question\n",
        "                  answer_list.append({'text':\"\",'answer_start':0,'answer_end':0})\n",
        "                  impossible_list.append(impossible)  # store whether the question is impossible or not\n",
        "              for answer in qa['answers']:  # if the question is possible, then it has one or more correct answers\n",
        "                  answer['answer_end'] = answer['answer_start'] + len(answer['text']) # find the end index of the answer\n",
        "                  answer_start_index = answer['answer_start'] # store the start index of the answer\n",
        "                  answer_end_index = answer['answer_end'] # store the end index of the answer\n",
        "                  answer_text = answer['text']  # get the answer text\n",
        "                  count = 0# offset of the indexes\n",
        "                  # the answer start/end index may be off by some characters, so correct them\n",
        "                  while True:\n",
        "                    if context[answer_start_index-count:answer_end_index-count] == answer_text: # if we have the correct start/end indexes\n",
        "                      # store them in the answer dictionary\n",
        "                      answer['answer_start'] = answer_start_index - count\n",
        "                      answer['answer_end'] = answer_end_index - count\n",
        "                      break\n",
        "                    if count>4: # never actualy happens, just for safety reasons to avoid infinite loop\n",
        "                        break\n",
        "                    count = count + 1 # if the indexes are not correct, increase the offset in order to \"move\" the indexes\n",
        "                  # if USE_ALL_QNAS is true, then we use every question/answer\n",
        "                  # if USE_ALL_QNAS is false, the we only use the questions/answers that have total words < 700\n",
        "                  if(len(context.split())+len(question.split())<700) or KEEP_ALL_LENGTH_QNAS:\n",
        "                    context_list.append(context)  # add the context of this question to the list\n",
        "                    question_list.append(question)  # add the question to the list\n",
        "                    answer_list.append(answer)  # add the answer to the list\n",
        "                    impossible_list.append(impossible)  # store whether the question is impossible or not\n",
        "\n",
        "  return context_list, question_list, answer_list, impossible_list\n",
        "\n",
        "train_contexts, train_questions, train_answers, train_impossible = read_file(train_path)  # read the training set file\n",
        "valid_contexts, valid_questions, valid_answers, valid_impossible = read_file(validation_path) # read the validation/dev set file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:40:23.849472Z",
          "iopub.status.busy": "2022-03-04T13:40:23.849139Z",
          "iopub.status.idle": "2022-03-04T13:40:23.859015Z",
          "shell.execute_reply": "2022-03-04T13:40:23.857872Z",
          "shell.execute_reply.started": "2022-03-04T13:40:23.849428Z"
        },
        "id": "RXEvHYv4g0VW",
        "outputId": "75eaf942-b7c2-4024-9eed-32e5ad70ca87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "42628"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_contexts) # how many questions/answers we have in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:40:27.006673Z",
          "iopub.status.busy": "2022-03-04T13:40:27.006232Z",
          "iopub.status.idle": "2022-03-04T13:42:27.870360Z",
          "shell.execute_reply": "2022-03-04T13:42:27.869207Z",
          "shell.execute_reply.started": "2022-03-04T13:40:27.006625Z"
        },
        "id": "BzCMzCRcw7Qp"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') # initialize tokenizer\n",
        "# tokenize the training set, giving contexts and questions to the tokenizer\n",
        "train_tokenized = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "# tokenize the validation set, giving contexts and questions to the tokenizer\n",
        "validation_tokenized = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:43:20.568829Z",
          "iopub.status.busy": "2022-03-04T13:43:20.565613Z",
          "iopub.status.idle": "2022-03-04T13:43:21.633319Z",
          "shell.execute_reply": "2022-03-04T13:43:21.631098Z",
          "shell.execute_reply.started": "2022-03-04T13:43:20.568750Z"
        },
        "id": "xeEkJ8t1w7Qq",
        "outputId": "adcf9c83-ebe6-4c95-b893-0c934f4ec2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42628\n",
            "400\n",
            "5567\n",
            "55\n"
          ]
        }
      ],
      "source": [
        "def index_answer(encodings, answers, impossible):\n",
        "    # after the tokenization, we need to convert the start/end indexes to the corresponding token indexes\n",
        "    start_positions = []  # list that will contain all the start indexes\n",
        "    end_positions = []  # list that will contain all the end indexes\n",
        "    count = 0 # how many answers were not found inside the tokenized context\n",
        "    for i in range(len(answers)): # for every answer\n",
        "        if impossible[i]:  # if the question is impossible\n",
        "            # find the index of the first SEP token\n",
        "            sep_index = encodings['input_ids'][i].index(tokenizer.sep_token_id)\n",
        "            # set the start and the end index as the sep token index, as the correct answer is the empty string\n",
        "            start_positions.append(sep_index)\n",
        "            end_positions.append(sep_index)\n",
        "            continue\n",
        "        # if the question is not impossible\n",
        "        # convert the start index to the corresponding token index \n",
        "        start_positions.append(encodings[i].char_to_token(answers[i]['answer_start']))\n",
        "        # convert the end index to the corresponding token index\n",
        "        end_positions.append(encodings[i].char_to_token(answers[i]['answer_end']))\n",
        "\n",
        "        if start_positions[-1] is None: # if the start index was not found (due to the truncation of max_length)\n",
        "            # the answer is not inside the tokenized context\n",
        "            count = count + 1\n",
        "            start_positions[-1] = tokenizer.model_max_length  # assign the last token as the start\n",
        "\n",
        "        shift = 1\n",
        "        while end_positions[-1] is None: # if the end index was not found\n",
        "            # decrease the original index until finding a corresponding token index\n",
        "            end_positions[-1] = encodings[i].char_to_token(answers[i]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "    print(len(answers)) # print how many questions/answers we have\n",
        "    print(count)  # print how many answers were not found inside the tokenized context\n",
        "    return start_positions, end_positions\n",
        "\n",
        "train_start,train_end = index_answer(train_tokenized, train_answers,train_impossible)\n",
        "validation_start,validation_end = index_answer(validation_tokenized, valid_answers, valid_impossible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:43:43.277162Z",
          "iopub.status.busy": "2022-03-04T13:43:43.276852Z",
          "iopub.status.idle": "2022-03-04T13:43:43.292943Z",
          "shell.execute_reply": "2022-03-04T13:43:43.290303Z",
          "shell.execute_reply.started": "2022-03-04T13:43:43.277129Z"
        },
        "id": "r9aBhqt6g0VY"
      },
      "outputs": [],
      "source": [
        "train_id = [tokenized for tokenized in train_tokenized['input_ids']] # store all the input_ids of the training set into a list\n",
        "train_mask = [tokenized for tokenized in train_tokenized['attention_mask']] # store all the attention_masks of the training set into a list\n",
        "validation_id = [tokenized for tokenized in validation_tokenized['input_ids']] # store all the input_ids of the validation set into a list\n",
        "validation_mask = [tokenized for tokenized in validation_tokenized['attention_mask']] # store all the attention_masks of the validation set into a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1XCIgckg0VY"
      },
      "outputs": [],
      "source": [
        "del train_tokenized,validation_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:44:08.332302Z",
          "iopub.status.busy": "2022-03-04T13:44:08.331976Z",
          "iopub.status.idle": "2022-03-04T13:44:13.620416Z",
          "shell.execute_reply": "2022-03-04T13:44:13.619337Z",
          "shell.execute_reply.started": "2022-03-04T13:44:08.332266Z"
        },
        "id": "DgFP4jh_w7Qq"
      },
      "outputs": [],
      "source": [
        "# convert all training data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "train_id_tensor = torch.tensor(train_id)\n",
        "train_mask_tensor = torch.tensor(train_mask)\n",
        "train_starts_tensors = torch.tensor(train_start)\n",
        "train_ends_tensors = torch.tensor(train_end)\n",
        "train_impossible_tensors = torch.tensor(train_impossible, dtype=torch.bool)\n",
        "# convert all validation data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "validation_id_tensor = torch.tensor(validation_id)\n",
        "validation_mask_tensor = torch.tensor(validation_mask)\n",
        "validation_starts_tensors = torch.tensor(validation_start)\n",
        "validation_ends_tensors = torch.tensor(validation_end)\n",
        "validation_impossible_tensors = torch.tensor(valid_impossible, dtype=torch.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:44:16.393284Z",
          "iopub.status.busy": "2022-03-04T13:44:16.392905Z",
          "iopub.status.idle": "2022-03-04T13:44:16.453008Z",
          "shell.execute_reply": "2022-03-04T13:44:16.451719Z",
          "shell.execute_reply.started": "2022-03-04T13:44:16.393248Z"
        },
        "id": "NMLaexzlg0VZ"
      },
      "outputs": [],
      "source": [
        "del train_id,train_mask,train_start,train_end,train_impossible\n",
        "del validation_id,validation_mask,validation_start,validation_end,valid_impossible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7b8ce42abd6445c382eb1bf0514d2be9"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-04T13:44:21.528897Z",
          "iopub.status.busy": "2022-03-04T13:44:21.528019Z",
          "iopub.status.idle": "2022-03-04T13:44:49.074559Z",
          "shell.execute_reply": "2022-03-04T13:44:49.073406Z",
          "shell.execute_reply.started": "2022-03-04T13:44:21.528861Z"
        },
        "id": "KIFOvaXsw7Qq",
        "outputId": "d1692b8b-3590-4d09-da0e-0db5f4d19560"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b8ce42abd6445c382eb1bf0514d2be9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda...\n"
          ]
        }
      ],
      "source": [
        "# define epochs\n",
        "TRAINING_EPOCHS = 2\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 3e-5\n",
        "batch_size = 16\n",
        "# define the model\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate)\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.to(device)\n",
        "  print('Using cuda...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:44:53.604426Z",
          "iopub.status.busy": "2022-03-04T13:44:53.603326Z",
          "iopub.status.idle": "2022-03-04T13:44:53.613392Z",
          "shell.execute_reply": "2022-03-04T13:44:53.612334Z",
          "shell.execute_reply.started": "2022-03-04T13:44:53.604389Z"
        },
        "id": "wg43fo7hg0Va"
      },
      "outputs": [],
      "source": [
        "#Initialize dataloader for train set\n",
        "train_dataset = torch.utils.data.TensorDataset(train_id_tensor, train_mask_tensor,train_starts_tensors,train_ends_tensors,train_impossible_tensors)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "del train_dataset\n",
        "#Initialize dataloader for validation set\n",
        "validation_dataset = torch.utils.data.TensorDataset(validation_id_tensor, validation_mask_tensor,validation_starts_tensors,validation_ends_tensors,validation_impossible_tensors)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "del validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:44:56.590522Z",
          "iopub.status.busy": "2022-03-04T13:44:56.590230Z",
          "iopub.status.idle": "2022-03-04T15:05:58.824583Z",
          "shell.execute_reply": "2022-03-04T15:05:58.823545Z",
          "shell.execute_reply.started": "2022-03-04T13:44:56.590491Z"
        },
        "id": "rsLktmdVw7Qr",
        "outputId": "8554fec9-1c44-4cc0-c676-f5ebaef1deec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2665/2665 [38:50<00:00,  1.14it/s]\n",
            "100%|██████████| 348/348 [01:39<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0: Train Loss = 0.81715 | Validation Loss = 0.64485 | Train Accuracy = 0.8293 | Validation Accuracy = 0.8577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2665/2665 [38:52<00:00,  1.14it/s]\n",
            "100%|██████████| 348/348 [01:39<00:00,  3.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1: Train Loss = 0.42908 | Validation Loss = 0.63243 | Train Accuracy = 0.8605 | Validation Accuracy = 0.8602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train_model(model,train_loader,validation_loader,optimizer,epochs = 5):\n",
        "  train_loss = [] # store the train loss for every epoch\n",
        "  valid_loss = []  # store the validation loss for every epoch\n",
        "  train_accuracy = []\n",
        "  valid_accuracy = []\n",
        "  for epoch in range(epochs):\n",
        "    train_batch_losses = [] # store the loss of every batch of the train set\n",
        "    validation_batch_losses = [] # store the loss of every batch of the validation set\n",
        "    model.train() # set the model to training mode\n",
        "    y_total_predict_train = []   # store the predictions of the the train set\n",
        "    y_total_train = []     # store the labels of the the train set\n",
        "    train_loader_tqdm = tqdm(train_loader)\n",
        "    for x_batch, mask, start, end, impossible in train_loader_tqdm:\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      start = start.to(device)\n",
        "      end = end.to(device)\n",
        "      mask = mask.to(device)\n",
        "      #Delete previously stored gradients\n",
        "      optimizer.zero_grad()\n",
        "      z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)\n",
        "      loss = z[0] # get the output loss\n",
        "      train_batch_losses.append(loss.item()) # store the train loss of the current batch\n",
        "      #Perform backpropagation starting from the loss calculated in this epoch\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(),1.0)    # perform gradient clipping\n",
        "      #Update model's weights based on the gradients calculated during backprop\n",
        "      optimizer.step()\n",
        "      start_pred = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "      # calculate accuracy for both and append to accuracy list\n",
        "      train_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "      train_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "      del x_batch, mask, start, end\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()   # set the model to evaluation mode\n",
        "    y_total_valid = []   # store the predictions of the the validation set\n",
        "    y_total_predict_valid = []      # store the labels of the the validation set\n",
        "    validation_loader_tqdm = tqdm(validation_loader)\n",
        "    with torch.no_grad():\n",
        "        for x_batch, mask, start, end, impossible in validation_loader_tqdm:\n",
        "          # transfer data to GPU\n",
        "          x_batch = x_batch.to(device)\n",
        "          start = start.to(device)\n",
        "          end = end.to(device)\n",
        "          mask = mask.to(device)\n",
        "          z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)  \n",
        "          loss = z[0]  # get the output loss\n",
        "          validation_batch_losses.append(loss.item())   # store the validation loss of the current batch\n",
        "          start_pred = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "          end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "          # calculate accuracy for both and append to accuracy list\n",
        "          valid_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "          valid_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "          del x_batch, mask, start, end\n",
        "    t_accuracy = sum(train_accuracy)/len(train_accuracy) # compute train accuracy based on the correct predictions\n",
        "    v_accuracy = sum(valid_accuracy)/len(valid_accuracy) # compute validation accuracy based on the correct predictions\n",
        "    current_train_loss = sum(train_batch_losses)/len(train_loader)  #compute the train loss of this epoch\n",
        "    train_loss.append(current_train_loss)   # store the train loss in order to plot loss curves\n",
        "    current_valid_loss = sum(validation_batch_losses)/len(validation_loader)   #compute the validation loss of this epoch\n",
        "    valid_loss.append(current_valid_loss)   # store the validation loss in order to plot loss curves\n",
        "    print(f\"Epoch {epoch:3}: Train Loss = {current_train_loss:.5f} | Validation Loss = {current_valid_loss:.5f} | Train Accuracy = {t_accuracy:.4f} | Validation Accuracy = {v_accuracy:.4f}\")\n",
        "  return model,train_loss,valid_loss\n",
        "\n",
        "# train the model\n",
        "trained_model, train_loss, valid_loss = train_model(model,train_loader,validation_loader,optimizer,TRAINING_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9q5rSfCCzDZ"
      },
      "source": [
        "### TRIVIA QA fined-tuned model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T15:21:04.706044Z",
          "iopub.status.busy": "2022-03-04T15:21:04.705245Z",
          "iopub.status.idle": "2022-03-04T15:26:00.866394Z",
          "shell.execute_reply": "2022-03-04T15:26:00.865162Z",
          "shell.execute_reply.started": "2022-03-04T15:21:04.706006Z"
        },
        "id": "wndRD_2fg0Vb",
        "outputId": "78347c33-198b-4420-86e3-4e24fe693938"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 890/890 [04:17<00:00,  3.46it/s]\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:   # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]  # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):   # for every item of the batch\n",
        "        i = (int(id_index_i)) # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T15:26:08.154997Z",
          "iopub.status.busy": "2022-03-04T15:26:08.153747Z",
          "iopub.status.idle": "2022-03-04T15:26:11.730360Z",
          "shell.execute_reply": "2022-03-04T15:26:11.729280Z",
          "shell.execute_reply.started": "2022-03-04T15:26:08.154904Z"
        },
        "id": "klr9193Zg0Vc",
        "outputId": "900fb65b-ad36-4ef4-8822-b19c7609217d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 38.40747768641507,\n",
            "  \"f1\": 39.422830131417484,\n",
            "  \"total\": 14229,\n",
            "  \"HasAns_exact\": 12.131680552733185,\n",
            "  \"HasAns_f1\": 13.599618973779668,\n",
            "  \"HasAns_total\": 9842,\n",
            "  \"NoAns_exact\": 97.35582402552997,\n",
            "  \"NoAns_f1\": 97.35582402552997,\n",
            "  \"NoAns_total\": 4387\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py triviaqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqlHZiSeCdJr"
      },
      "source": [
        "### TRIVIA QA fined-tuned model test on SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:41:52.633012Z",
          "iopub.status.busy": "2022-03-06T22:41:52.632705Z",
          "iopub.status.idle": "2022-03-06T22:41:52.637856Z",
          "shell.execute_reply": "2022-03-06T22:41:52.636671Z",
          "shell.execute_reply.started": "2022-03-06T22:41:52.632981Z"
        },
        "id": "Z2PtGrqJCdJt"
      },
      "outputs": [],
      "source": [
        "validation_path = 'dev-v2.0.json' # path to the validation-dev set of SQuAD dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "cf329d8b2db24694980f120b7567ec7d",
            "061a8a33c6e741dda0aa196b0e031dad",
            "018c61b89bd148aab32a106affbda535",
            "9062c04a46614f0cac289457ae32c2e3"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-06T22:42:11.411026Z",
          "iopub.status.busy": "2022-03-06T22:42:11.410736Z",
          "iopub.status.idle": "2022-03-06T22:45:57.392330Z",
          "shell.execute_reply": "2022-03-06T22:45:57.391139Z",
          "shell.execute_reply.started": "2022-03-06T22:42:11.410994Z"
        },
        "id": "xzYuQjQFCdJu",
        "outputId": "1c9048f4-1570-4358-c20b-43a10092db36"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf329d8b2db24694980f120b7567ec7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "061a8a33c6e741dda0aa196b0e031dad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "018c61b89bd148aab32a106affbda535",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9062c04a46614f0cac289457ae32c2e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 186/186 [03:26<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 13.324349364103428,\n",
            "  \"f1\": 16.115175977555154,\n",
            "  \"total\": 11873,\n",
            "  \"HasAns_exact\": 13.090418353576249,\n",
            "  \"HasAns_f1\": 18.680074963142967,\n",
            "  \"HasAns_total\": 5928,\n",
            "  \"NoAns_exact\": 13.557611438183347,\n",
            "  \"NoAns_f1\": 13.557611438183347,\n",
            "  \"NoAns_total\": 5945\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:   # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0] # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1)  # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):   # for every item of the batch\n",
        "        i = (int(id_index_i)) # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py dev-v2.0.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oyeZqwtCdJv"
      },
      "source": [
        "### TRIVIA QA fined-tuned model test on NEWS QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:46:12.041177Z",
          "iopub.status.busy": "2022-03-06T22:46:12.040864Z",
          "iopub.status.idle": "2022-03-06T22:46:12.046357Z",
          "shell.execute_reply": "2022-03-06T22:46:12.045417Z",
          "shell.execute_reply.started": "2022-03-06T22:46:12.041145Z"
        },
        "id": "bkL4Z6wyCdJw"
      },
      "outputs": [],
      "source": [
        "validation_path = 'newsqa_dev_SquadFormat.json' # path to the validation-dev set of NEWS QA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:46:14.409640Z",
          "iopub.status.busy": "2022-03-06T22:46:14.408882Z",
          "iopub.status.idle": "2022-03-06T22:48:02.880127Z",
          "shell.execute_reply": "2022-03-06T22:48:02.879077Z",
          "shell.execute_reply.started": "2022-03-06T22:46:14.409589Z"
        },
        "id": "NB2hTpgxCdJw",
        "outputId": "4562cd07-b75b-4ecd-a793-144a2c93d3ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 81/81 [01:29<00:00,  1.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 5.555555555555555,\n",
            "  \"f1\": 8.057651464334763,\n",
            "  \"total\": 5166,\n",
            "  \"HasAns_exact\": 5.555555555555555,\n",
            "  \"HasAns_f1\": 8.057651464334763,\n",
            "  \"HasAns_total\": 5166\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:  # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0] # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):   # for every item of the batch\n",
        "        i = (int(id_index_i))  # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py newsqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRrM5tZaCdJw"
      },
      "source": [
        "### TRIVIA QA fined-tuned model test on Natural Questions (NQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:48:34.234738Z",
          "iopub.status.busy": "2022-03-06T22:48:34.234307Z",
          "iopub.status.idle": "2022-03-06T22:48:34.242881Z",
          "shell.execute_reply": "2022-03-06T22:48:34.241854Z",
          "shell.execute_reply.started": "2022-03-06T22:48:34.234694Z"
        },
        "id": "DbvvaLYbCdJx"
      },
      "outputs": [],
      "source": [
        "validation_path = 'nq_dev.json'  # path to the validation-dev set of NQ dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:48:36.173166Z",
          "iopub.status.busy": "2022-03-06T22:48:36.172836Z",
          "iopub.status.idle": "2022-03-06T22:49:42.886875Z",
          "shell.execute_reply": "2022-03-06T22:49:42.885783Z",
          "shell.execute_reply.started": "2022-03-06T22:48:36.173121Z"
        },
        "id": "3tJhC7GlCdJx",
        "outputId": "3b44000a-5bcc-4aca-e56e-8ce2963658fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 53/53 [00:58<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 15.256752745621846,\n",
            "  \"f1\": 20.5597419800712,\n",
            "  \"total\": 3369,\n",
            "  \"HasAns_exact\": 16.850594227504246,\n",
            "  \"HasAns_f1\": 24.4336887652207,\n",
            "  \"HasAns_total\": 2356,\n",
            "  \"NoAns_exact\": 11.549851924975322,\n",
            "  \"NoAns_f1\": 11.549851924975322,\n",
            "  \"NoAns_total\": 1013\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:   # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0] # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i))  # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py nq_dev.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58H659JOCdJx"
      },
      "source": [
        "### TRIVIA QA fined-tuned model test on Quac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:51:56.892791Z",
          "iopub.status.busy": "2022-03-06T22:51:56.891921Z",
          "iopub.status.idle": "2022-03-06T22:51:56.897517Z",
          "shell.execute_reply": "2022-03-06T22:51:56.896426Z",
          "shell.execute_reply.started": "2022-03-06T22:51:56.892739Z"
        },
        "id": "QJGJoWOlCdJy"
      },
      "outputs": [],
      "source": [
        "validation_path = 'quac_dev_SquadFormat.json' # path to the validation-dev set of Quac dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T22:52:05.510094Z",
          "iopub.status.busy": "2022-03-06T22:52:05.509072Z",
          "iopub.status.idle": "2022-03-06T22:53:11.610465Z",
          "shell.execute_reply": "2022-03-06T22:53:11.609263Z",
          "shell.execute_reply.started": "2022-03-06T22:52:05.510043Z"
        },
        "id": "ye3OjJJSCdJy",
        "outputId": "8c75a7a9-70aa-446d-fff0-3c256cacff2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 115/115 [00:56<00:00,  2.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 3.24993200979059,\n",
            "  \"f1\": 5.65380821082426,\n",
            "  \"total\": 7354,\n",
            "  \"HasAns_exact\": 0.22154055896387184,\n",
            "  \"HasAns_f1\": 3.2341693221543504,\n",
            "  \"HasAns_total\": 5868,\n",
            "  \"NoAns_exact\": 15.208613728129206,\n",
            "  \"NoAns_f1\": 15.208613728129206,\n",
            "  \"NoAns_total\": 1486\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:  # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0] # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i))  # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py quac_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJZJ_XIXJ-PM"
      },
      "source": [
        "## NEWS QA FINETUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:40:17.752254Z",
          "iopub.status.busy": "2022-03-03T17:40:17.751440Z",
          "iopub.status.idle": "2022-03-03T17:40:17.756149Z",
          "shell.execute_reply": "2022-03-03T17:40:17.755286Z",
          "shell.execute_reply.started": "2022-03-03T17:40:17.752211Z"
        },
        "id": "X42O6UooiD8C"
      },
      "outputs": [],
      "source": [
        "# the train and dev files were downloaded and transformed in paragraph Data Creation, so store the names of the files\n",
        "output_file_train = 'newsqa_train_SquadFormat.json'\n",
        "output_file_dev = 'newsqa_dev_SquadFormat.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:40:20.803094Z",
          "iopub.status.busy": "2022-03-03T17:40:20.802317Z",
          "iopub.status.idle": "2022-03-03T17:40:20.807745Z",
          "shell.execute_reply": "2022-03-03T17:40:20.806593Z",
          "shell.execute_reply.started": "2022-03-03T17:40:20.803052Z"
        },
        "id": "dIbrD5VOw7Qv"
      },
      "outputs": [],
      "source": [
        "train_path = output_file_train\n",
        "validation_path = output_file_dev\n",
        "\n",
        "MAX_LENGTH = 300  # max length that will be used during tokenization\n",
        "USE_IMPOSSIBLE_QNAS = True  # include also impossible questions in the training set\n",
        "KEEP_ALL_LENGTH_QNAS = False  # don't use the entire dataset in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:40:23.583076Z",
          "iopub.status.busy": "2022-03-03T17:40:23.582326Z",
          "iopub.status.idle": "2022-03-03T17:40:23.639931Z",
          "shell.execute_reply": "2022-03-03T17:40:23.638901Z",
          "shell.execute_reply.started": "2022-03-03T17:40:23.583030Z"
        },
        "id": "L0kWnTqlw7Qv"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:40:35.301497Z",
          "iopub.status.busy": "2022-03-03T17:40:35.300987Z",
          "iopub.status.idle": "2022-03-03T17:40:43.552361Z",
          "shell.execute_reply": "2022-03-03T17:40:43.551472Z",
          "shell.execute_reply.started": "2022-03-03T17:40:35.301443Z"
        },
        "id": "qj87mZXlw7Qv"
      },
      "outputs": [],
      "source": [
        "def read_file(file_name):  # read data from .json file\n",
        "  with open(file_name, 'rb') as file:  # open the file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain all the context that correspond to every question/answer set\n",
        "  question_list = []  # list that will contain all the quections. Questions that have multiple answers will be inserted multiple times at the list\n",
        "  answer_list = []  # list that will contain all the answers to the questions\n",
        "  impossible_list = []  # list that will contatin if the corresponding question in the question list is impossible or not\n",
        "  squad = squad['data']  # get the data\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this paragraph\n",
        "              question = qa['question'] # store the question text\n",
        "              impossible = qa['is_impossible']  # store if the question is impossible or not\n",
        "              if impossible and USE_IMPOSSIBLE_QNAS:  # if the question is impossible\n",
        "                  context_list.append(context)  # add the context of this question to the list\n",
        "                  question_list.append(question)  # add the question to the list\n",
        "                  answer_list.append({'text':\"\",'answer_start':0,'answer_end':0})  # add the empty string as the answer of the question\n",
        "                  impossible_list.append(impossible)    # store whether the question is impossible or not\n",
        "              for answer in qa['answers']: # if the question is possible, then it has one or more correct answers\n",
        "                  answer['answer_end'] = answer['answer_start'] + len(answer['text']) # find the end index of the answer\n",
        "                  answer_start_index = answer['answer_start'] # store the start index of the answer\n",
        "                  answer_end_index = answer['answer_end'] # store the end index of the answer\n",
        "                  answer_text = answer['text']  # get the answer text\n",
        "                  count = 0 # offset of the indexes\n",
        "                  # the answer start/end index may be off by some characters, so correct them\n",
        "                  while True:\n",
        "                    if context[answer_start_index-count:answer_end_index-count] == answer_text: # if we have the correct start/end indexes\n",
        "                      # store them in the answer dictionary\n",
        "                      answer['answer_start'] = answer_start_index - count\n",
        "                      answer['answer_end'] = answer_end_index - count\n",
        "                      break\n",
        "                    if count>4: # never actualy happens, just for safety reasons to avoid infinite loop\n",
        "                        break\n",
        "                    count = count + 1 # if the indexes are not correct, increase the offset in order to \"move\" the indexes\n",
        "                  # if USE_ALL_QNAS is true, then we use every question/answer\n",
        "                  # if USE_ALL_QNAS is false, the we only use the questions/answers that have total words < 750\n",
        "                  if(len(context.split())+len(question.split())<750) or KEEP_ALL_LENGTH_QNAS:\n",
        "                    context_list.append(context)  # add the context of this question to the list\n",
        "                    question_list.append(question)   # add the question to the list\n",
        "                    answer_list.append(answer)   # add the answer to the list\n",
        "                    impossible_list.append(impossible)  # store whether the question is impossible or not\n",
        "\n",
        "  return context_list, question_list, answer_list, impossible_list\n",
        "\n",
        "train_contexts, train_questions, train_answers, train_impossible = read_file(train_path)  # read the training set file\n",
        "valid_contexts, valid_questions, valid_answers, valid_impossible = read_file(validation_path) # read the validation/dev set file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:42:04.659390Z",
          "iopub.status.busy": "2022-03-03T17:42:04.659087Z",
          "iopub.status.idle": "2022-03-03T17:42:04.667100Z",
          "shell.execute_reply": "2022-03-03T17:42:04.666361Z",
          "shell.execute_reply.started": "2022-03-03T17:42:04.659357Z"
        },
        "id": "htukGbyIiD8F",
        "outputId": "064c9d7d-d98f-4745-bd7d-00d62756843d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "53572"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_contexts) # how many questions/answers we have in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:42:08.344441Z",
          "iopub.status.busy": "2022-03-03T17:42:08.343841Z",
          "iopub.status.idle": "2022-03-03T17:43:31.950678Z",
          "shell.execute_reply": "2022-03-03T17:43:31.949887Z",
          "shell.execute_reply.started": "2022-03-03T17:42:08.344392Z"
        },
        "id": "Az6ooWoCw7Qw"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') # initialize tokenizer\n",
        "# tokenize the training set, giving contexts and questions to the tokenizer\n",
        "train_tokenized = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "# tokenize the validation set, giving contexts and questions to the tokenizer\n",
        "validation_tokenized = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)\n",
        "\n",
        "train_id = [tokenized for tokenized in train_tokenized['input_ids']] # store all the input_ids of the training set into a list\n",
        "train_mask = [tokenized for tokenized in train_tokenized['attention_mask']] # store all the attention_masks of the training set into a list\n",
        "validation_id = [tokenized for tokenized in validation_tokenized['input_ids']] # store all the input_ids of the validation set into a list\n",
        "validation_mask = [tokenized for tokenized in validation_tokenized['attention_mask']] # store all the attention_masks of the validation set into a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:49:17.163448Z",
          "iopub.status.busy": "2022-03-03T17:49:17.162832Z",
          "iopub.status.idle": "2022-03-03T17:49:19.213920Z",
          "shell.execute_reply": "2022-03-03T17:49:19.213130Z",
          "shell.execute_reply.started": "2022-03-03T17:49:17.163406Z"
        },
        "id": "JDsWhThyw7Qw",
        "outputId": "5cd3782e-8011-4271-b018-8f144781a71d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53572\n",
            "1721\n",
            "3076\n",
            "132\n"
          ]
        }
      ],
      "source": [
        "def index_answer(encodings, answers, impossible):\n",
        "    # after the tokenization, we need to convert the start/end indexes to the corresponding token indexes\n",
        "    start_positions = []  # list that will contain all the start indexes\n",
        "    end_positions = []  # list that will contain all the end indexes\n",
        "    count = 0 # how many answers were not found inside the tokenized context\n",
        "    for i in range(len(answers)):  # for every answer\n",
        "        if impossible[i]:   # if the question is impossible\n",
        "            # find the index of the first SEP token\n",
        "            sep_index = encodings['input_ids'][i].index(tokenizer.sep_token_id)\n",
        "            # set the start and the end index as the sep token index, as the correct answer is the empty string\n",
        "            start_positions.append(sep_index)\n",
        "            end_positions.append(sep_index)\n",
        "            continue\n",
        "        # if the question is not impossible\n",
        "        # convert the start index to the corresponding token index \n",
        "        start_positions.append(encodings[i].char_to_token(answers[i]['answer_start']))\n",
        "        # convert the end index to the corresponding token index\n",
        "        end_positions.append(encodings[i].char_to_token(answers[i]['answer_end']))\n",
        "\n",
        "        if start_positions[-1] is None: # if the start index was not found (due to the truncation of max_length)\n",
        "            # the answer is not inside the tokenized context\n",
        "            count = count + 1\n",
        "            start_positions[-1] = tokenizer.model_max_length  # assign the last token as the start\n",
        "\n",
        "        shift = 1\n",
        "        while end_positions[-1] is None:  # if the end index was not found\n",
        "            # decrease the original index until finding a corresponding token index\n",
        "            end_positions[-1] = encodings[i].char_to_token(answers[i]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "    print(len(answers))  # print how many questions/answers we have\n",
        "    print(count)  # print how many answers were not found inside the tokenized context\n",
        "    return start_positions, end_positions\n",
        "\n",
        "train_start,train_end = index_answer(train_tokenized, train_answers,train_impossible)\n",
        "validation_start,validation_end = index_answer(validation_tokenized, valid_answers, valid_impossible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:49:25.936795Z",
          "iopub.status.busy": "2022-03-03T17:49:25.936154Z",
          "iopub.status.idle": "2022-03-03T17:49:32.488667Z",
          "shell.execute_reply": "2022-03-03T17:49:32.487866Z",
          "shell.execute_reply.started": "2022-03-03T17:49:25.936754Z"
        },
        "id": "AuQ3SoA3w7Qw"
      },
      "outputs": [],
      "source": [
        "# convert all training data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "train_id_tensor = torch.tensor(train_id)\n",
        "train_mask_tensor = torch.tensor(train_mask)\n",
        "train_starts_tensors = torch.tensor(train_start)\n",
        "train_ends_tensors = torch.tensor(train_end)\n",
        "train_impossible_tensors = torch.tensor(train_impossible, dtype=torch.bool)\n",
        "# convert all validation data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "validation_id_tensor = torch.tensor(validation_id)\n",
        "validation_mask_tensor = torch.tensor(validation_mask)\n",
        "validation_starts_tensors = torch.tensor(validation_start)\n",
        "validation_ends_tensors = torch.tensor(validation_end)\n",
        "validation_impossible_tensors = torch.tensor(valid_impossible, dtype=torch.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3f3b303823f3467d8e8daec9c68c8604"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-03T17:49:55.835597Z",
          "iopub.status.busy": "2022-03-03T17:49:55.834878Z",
          "iopub.status.idle": "2022-03-03T17:50:13.396357Z",
          "shell.execute_reply": "2022-03-03T17:50:13.395470Z",
          "shell.execute_reply.started": "2022-03-03T17:49:55.835535Z"
        },
        "id": "nHilQ-dDw7Qw",
        "outputId": "d1692b8b-3590-4d09-da0e-0db5f4d19560"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f3b303823f3467d8e8daec9c68c8604",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda...\n"
          ]
        }
      ],
      "source": [
        "# define epochs\n",
        "TRAINING_EPOCHS = 2\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 3e-5\n",
        "batch_size = 16\n",
        "# define the model\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate)\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.to(device)\n",
        "  print('Using cuda...')\n",
        "\n",
        "#Initialize dataloader for train set\n",
        "train_dataset = torch.utils.data.TensorDataset(train_id_tensor, train_mask_tensor,train_starts_tensors,train_ends_tensors,train_impossible_tensors)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#Initialize dataloader for validation set\n",
        "validation_dataset = torch.utils.data.TensorDataset(validation_id_tensor, validation_mask_tensor,validation_starts_tensors,validation_ends_tensors,validation_impossible_tensors)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T17:50:26.240498Z",
          "iopub.status.busy": "2022-03-03T17:50:26.240194Z",
          "iopub.status.idle": "2022-03-03T19:28:14.778390Z",
          "shell.execute_reply": "2022-03-03T19:28:14.777541Z",
          "shell.execute_reply.started": "2022-03-03T17:50:26.240448Z"
        },
        "id": "1gqdVuiUw7Qw",
        "outputId": "8554fec9-1c44-4cc0-c676-f5ebaef1deec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3349/3349 [47:59<00:00,  1.16it/s]\n",
            "100%|██████████| 193/193 [00:55<00:00,  3.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0: Train Loss = 2.36612 | Validation Loss = 1.94235 | Train Accuracy = 0.4285 | Validation Accuracy = 0.5015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3349/3349 [47:58<00:00,  1.16it/s]\n",
            "100%|██████████| 193/193 [00:55<00:00,  3.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1: Train Loss = 1.52641 | Validation Loss = 1.89016 | Train Accuracy = 0.5087 | Validation Accuracy = 0.5104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train_model(model,train_loader,validation_loader,optimizer,epochs = 5):\n",
        "  train_loss = [] # store the train loss for every epoch\n",
        "  valid_loss = []  # store the validation loss for every epoch\n",
        "  train_accuracy = []\n",
        "  valid_accuracy = []\n",
        "  for epoch in range(epochs):\n",
        "    train_batch_losses = [] # store the loss of every batch of the train set\n",
        "    validation_batch_losses = [] # store the loss of every batch of the validation set\n",
        "    model.train() # set the model to training mode\n",
        "    y_total_predict_train = []   # store the predictions of the the train set\n",
        "    y_total_train = []     # store the labels of the the train set\n",
        "    train_loader_tqdm = tqdm(train_loader)\n",
        "    for x_batch, mask, start, end, impossible in train_loader_tqdm:\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      start = start.to(device)\n",
        "      end = end.to(device)\n",
        "      mask = mask.to(device)\n",
        "      #Delete previously stored gradients\n",
        "      optimizer.zero_grad()\n",
        "      z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)\n",
        "      loss = z[0] # get the output loss\n",
        "      train_batch_losses.append(loss.item()) # store the train loss of the current batch\n",
        "      #Perform backpropagation starting from the loss calculated in this epoch\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(),1.0)    # perform gradient clipping\n",
        "      #Update model's weights based on the gradients calculated during backprop\n",
        "      optimizer.step()\n",
        "      start_pred = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "      # calculate accuracy for both and append to accuracy list\n",
        "      train_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "      train_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "      del x_batch, mask, start, end\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()   # set the model to evaluation mode\n",
        "    y_total_valid = []   # store the predictions of the the validation set\n",
        "    y_total_predict_valid = []      # store the labels of the the validation set\n",
        "    validation_loader_tqdm = tqdm(validation_loader)\n",
        "    with torch.no_grad():\n",
        "        for x_batch, mask, start, end, impossible in validation_loader_tqdm:\n",
        "          # transfer data to GPU\n",
        "          x_batch = x_batch.to(device)\n",
        "          start = start.to(device)\n",
        "          end = end.to(device)\n",
        "          mask = mask.to(device)\n",
        "          z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)  \n",
        "          loss = z[0]  # get the output loss\n",
        "          validation_batch_losses.append(loss.item())   # store the validation loss of the current batch\n",
        "          start_pred = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "          end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "          # calculate accuracy for both and append to accuracy list\n",
        "          valid_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "          valid_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "          del x_batch, mask, start, end\n",
        "    t_accuracy = sum(train_accuracy)/len(train_accuracy) # compute train accuracy based on the correct predictions\n",
        "    v_accuracy = sum(valid_accuracy)/len(valid_accuracy) # compute validation accuracy based on the correct predictions\n",
        "    current_train_loss = sum(train_batch_losses)/len(train_loader)  #compute the train loss of this epoch\n",
        "    train_loss.append(current_train_loss)   # store the train loss in order to plot loss curves\n",
        "    current_valid_loss = sum(validation_batch_losses)/len(validation_loader)   #compute the validation loss of this epoch\n",
        "    valid_loss.append(current_valid_loss)   # store the validation loss in order to plot loss curves\n",
        "    print(f\"Epoch {epoch:3}: Train Loss = {current_train_loss:.5f} | Validation Loss = {current_valid_loss:.5f} | Train Accuracy = {t_accuracy:.4f} | Validation Accuracy = {v_accuracy:.4f}\")\n",
        "  return model,train_loss,valid_loss\n",
        "\n",
        "# train the model\n",
        "trained_model, train_loss, valid_loss = train_model(model,train_loader,validation_loader,optimizer,TRAINING_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyptTsHqJm09"
      },
      "source": [
        "### News QA fined-tuned model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T19:33:20.419098Z",
          "iopub.status.busy": "2022-03-03T19:33:20.418823Z",
          "iopub.status.idle": "2022-03-03T19:35:07.450871Z",
          "shell.execute_reply": "2022-03-03T19:35:07.450120Z",
          "shell.execute_reply.started": "2022-03-03T19:33:20.419067Z"
        },
        "id": "Upet1y4ViD8N",
        "outputId": "f463ebe2-dd16-4b4e-ce53-12e4adfb758f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 323/323 [01:35<00:00,  3.37it/s]\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:   # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]  # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):     # for every item of the batch\n",
        "        i = (int(id_index_i)) # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-03T19:36:07.876442Z",
          "iopub.status.busy": "2022-03-03T19:36:07.875716Z",
          "iopub.status.idle": "2022-03-03T19:36:09.765369Z",
          "shell.execute_reply": "2022-03-03T19:36:09.764506Z",
          "shell.execute_reply.started": "2022-03-03T19:36:07.876395Z"
        },
        "id": "OHTEb93eiD8O",
        "outputId": "e6e3e80b-5aa7-4460-fe88-e7d41727d6b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 33.15911730545877,\n",
            "  \"f1\": 48.22154510226573,\n",
            "  \"total\": 5166,\n",
            "  \"HasAns_exact\": 33.15911730545877,\n",
            "  \"HasAns_f1\": 48.22154510226573,\n",
            "  \"HasAns_total\": 5166\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py newsqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nNij7SHJfkF"
      },
      "source": [
        "### News QA fined-tuned model test on SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:32:29.520290Z",
          "iopub.status.busy": "2022-03-06T23:32:29.519979Z",
          "iopub.status.idle": "2022-03-06T23:32:29.524060Z",
          "shell.execute_reply": "2022-03-06T23:32:29.523435Z",
          "shell.execute_reply.started": "2022-03-06T23:32:29.520251Z"
        },
        "id": "PB1e_vbMJfkG"
      },
      "outputs": [],
      "source": [
        "validation_path = 'dev-v2.0.json'    # path to the validation-dev set of SQuAD dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:32:29.526097Z",
          "iopub.status.busy": "2022-03-06T23:32:29.525462Z",
          "iopub.status.idle": "2022-03-06T23:36:09.043946Z",
          "shell.execute_reply": "2022-03-06T23:36:09.043007Z",
          "shell.execute_reply.started": "2022-03-06T23:32:29.526062Z"
        },
        "id": "OXslf5EpJfkH",
        "outputId": "6955d6ea-a7b1-46c6-ed3d-7f685e7f3464"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 186/186 [03:25<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 29.66394340099385,\n",
            "  \"f1\": 36.76369805667687,\n",
            "  \"total\": 11873,\n",
            "  \"HasAns_exact\": 51.40013495276653,\n",
            "  \"HasAns_f1\": 65.62000455919768,\n",
            "  \"HasAns_total\": 5928,\n",
            "  \"NoAns_exact\": 7.9899074852817495,\n",
            "  \"NoAns_f1\": 7.9899074852817495,\n",
            "  \"NoAns_total\": 5945\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:    # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):     # for every item of the batch\n",
        "        i = (int(id_index_i))  # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer    # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py dev-v2.0.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fe62FH6JfkH"
      },
      "source": [
        "### News QA fined-tuned model test on TRIVIA QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:36:09.046065Z",
          "iopub.status.busy": "2022-03-06T23:36:09.045766Z",
          "iopub.status.idle": "2022-03-06T23:36:09.051214Z",
          "shell.execute_reply": "2022-03-06T23:36:09.050420Z",
          "shell.execute_reply.started": "2022-03-06T23:36:09.046027Z"
        },
        "id": "IjG1MFhcJfkI"
      },
      "outputs": [],
      "source": [
        "validation_path = 'triviaqa_dev_SquadFormat.json' # path to the validation-dev set of TRIVIA QA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:36:09.054203Z",
          "iopub.status.busy": "2022-03-06T23:36:09.053172Z",
          "iopub.status.idle": "2022-03-06T23:41:05.355984Z",
          "shell.execute_reply": "2022-03-06T23:41:05.354495Z",
          "shell.execute_reply.started": "2022-03-06T23:36:09.054166Z"
        },
        "id": "KL7simhJJfkI",
        "outputId": "267c68b5-1210-40af-bbae-3049589b2919"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [04:12<00:00,  1.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 23.325602642490686,\n",
            "  \"f1\": 29.818630894827496,\n",
            "  \"total\": 14229,\n",
            "  \"HasAns_exact\": 27.778906726275146,\n",
            "  \"HasAns_f1\": 37.166155151646095,\n",
            "  \"HasAns_total\": 9842,\n",
            "  \"NoAns_exact\": 13.334852974697972,\n",
            "  \"NoAns_f1\": 13.334852974697972,\n",
            "  \"NoAns_total\": 4387\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:    # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]  # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):   # for every item of the batch\n",
        "        i = (int(id_index_i))  # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py ../input/triviaqa-dataset/content/TriviaQA_dataset/triviaqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAKq2P9cJfkJ"
      },
      "source": [
        "### News QA fined-tuned model test on Natural Questions (NQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:41:05.361896Z",
          "iopub.status.busy": "2022-03-06T23:41:05.361394Z",
          "iopub.status.idle": "2022-03-06T23:41:05.366406Z",
          "shell.execute_reply": "2022-03-06T23:41:05.365720Z",
          "shell.execute_reply.started": "2022-03-06T23:41:05.361860Z"
        },
        "id": "KKZv6kNPJfkJ"
      },
      "outputs": [],
      "source": [
        "validation_path = 'nq_dev.json'  # path to the validation-dev set of NQ dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:41:05.462532Z",
          "iopub.status.busy": "2022-03-06T23:41:05.462138Z",
          "iopub.status.idle": "2022-03-06T23:42:11.671870Z",
          "shell.execute_reply": "2022-03-06T23:42:11.670986Z",
          "shell.execute_reply.started": "2022-03-06T23:41:05.462498Z"
        },
        "id": "YZIG37qjJfkJ",
        "outputId": "dd465ec2-75c4-4547-b016-00ccd74ed1f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 53/53 [00:58<00:00,  1.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 36.182843573760756,\n",
            "  \"f1\": 44.80130253666088,\n",
            "  \"total\": 3369,\n",
            "  \"HasAns_exact\": 47.15619694397284,\n",
            "  \"HasAns_f1\": 59.48030061375658,\n",
            "  \"HasAns_total\": 2356,\n",
            "  \"NoAns_exact\": 10.661401776900297,\n",
            "  \"NoAns_f1\": 10.661401776900297,\n",
            "  \"NoAns_total\": 1013\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:    # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i))   # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py nq_dev.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqoIBeiiJfkK"
      },
      "source": [
        "### News QA fined-tuned model test on Quac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:42:11.673964Z",
          "iopub.status.busy": "2022-03-06T23:42:11.673509Z",
          "iopub.status.idle": "2022-03-06T23:42:11.679485Z",
          "shell.execute_reply": "2022-03-06T23:42:11.678697Z",
          "shell.execute_reply.started": "2022-03-06T23:42:11.673935Z"
        },
        "id": "TkwIeB5dJfkK"
      },
      "outputs": [],
      "source": [
        "validation_path = 'quac_dev_SquadFormat.json'  # path to the validation-dev set of Quac dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:42:11.682549Z",
          "iopub.status.busy": "2022-03-06T23:42:11.682125Z",
          "iopub.status.idle": "2022-03-06T23:43:16.738459Z",
          "shell.execute_reply": "2022-03-06T23:43:16.737689Z",
          "shell.execute_reply.started": "2022-03-06T23:42:11.682510Z"
        },
        "id": "zdmh95xPJfkL",
        "outputId": "4931f7e6-cb3f-4407-8450-b32945b19574"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 115/115 [00:56<00:00,  2.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 3.1003535490889313,\n",
            "  \"f1\": 6.508353102635553,\n",
            "  \"total\": 7354,\n",
            "  \"HasAns_exact\": 0.3067484662576687,\n",
            "  \"HasAns_f1\": 4.577782671571566,\n",
            "  \"HasAns_total\": 5868,\n",
            "  \"NoAns_exact\": 14.131897711978466,\n",
            "  \"NoAns_f1\": 14.131897711978466,\n",
            "  \"NoAns_total\": 1486\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:  # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i))   # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer    # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py quac_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeaLXDdNI_pD"
      },
      "source": [
        "## Natural Questions (NQ) FINETUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:27:04.403044Z",
          "iopub.status.busy": "2022-03-06T12:27:04.402339Z",
          "iopub.status.idle": "2022-03-06T12:27:04.406920Z",
          "shell.execute_reply": "2022-03-06T12:27:04.405813Z",
          "shell.execute_reply.started": "2022-03-06T12:27:04.403010Z"
        },
        "id": "1XPFCuqIhjrE"
      },
      "outputs": [],
      "source": [
        "# the train and dev files were downloaded and transformed in paragraph Data Creation, so store the names of the files\n",
        "output_file_train = 'nq_train.json'\n",
        "output_file_dev = 'nq_dev.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:27:05.742354Z",
          "iopub.status.busy": "2022-03-06T12:27:05.741817Z",
          "iopub.status.idle": "2022-03-06T12:27:05.746533Z",
          "shell.execute_reply": "2022-03-06T12:27:05.745491Z",
          "shell.execute_reply.started": "2022-03-06T12:27:05.742318Z"
        },
        "id": "HMUUTDtbw7Q0"
      },
      "outputs": [],
      "source": [
        "train_path = output_file_train\n",
        "validation_path = output_file_dev\n",
        "\n",
        "MAX_LENGTH = 512    # max length that will be used during tokenization\n",
        "USE_IMPOSSIBLE_QNAS = True   # include also impossible questions in the training set\n",
        "KEEP_ALL_LENGTH_QNAS = True  # use the entire dataset in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:27:07.741012Z",
          "iopub.status.busy": "2022-03-06T12:27:07.740309Z",
          "iopub.status.idle": "2022-03-06T12:27:07.797809Z",
          "shell.execute_reply": "2022-03-06T12:27:07.796967Z",
          "shell.execute_reply.started": "2022-03-06T12:27:07.740976Z"
        },
        "id": "jaZVIKwDw7Q0"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:27:09.966882Z",
          "iopub.status.busy": "2022-03-06T12:27:09.966459Z",
          "iopub.status.idle": "2022-03-06T12:27:15.219742Z",
          "shell.execute_reply": "2022-03-06T12:27:15.219011Z",
          "shell.execute_reply.started": "2022-03-06T12:27:09.966847Z"
        },
        "id": "MU6bXSOXw7Q0"
      },
      "outputs": [],
      "source": [
        "def read_file(file_name):  # read data from .json file\n",
        "  with open(file_name, 'rb') as file:  # open the file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain all the context that correspond to every question/answer set\n",
        "  question_list = []  # list that will contain all the quections. Questions that have multiple answers will be inserted multiple times at the list\n",
        "  answer_list = []  # list that will contain all the answers to the questions\n",
        "  impossible_list = []  # list that will contatin if the corresponding question in the question list is impossible or not\n",
        "  squad = squad['data']  # get the data\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this paragraph\n",
        "              question = qa['question'] # store the question text\n",
        "              impossible = qa['is_impossible']  # store if the question is impossible or not\n",
        "              if impossible and USE_IMPOSSIBLE_QNAS:  # if the question is impossible\n",
        "                  context_list.append(context)  # add the context of this question to the list\n",
        "                  question_list.append(question)  # add the question to the list\n",
        "                  answer_list.append({'text':\"\",'answer_start':0,'answer_end':0})  # add the empty string as the answer of the question\n",
        "                  impossible_list.append(impossible)    # store whether the question is impossible or not\n",
        "              for answer in qa['answers']: # if the question is possible, then it has one or more correct answers\n",
        "                  answer['answer_end'] = answer['answer_start'] + len(answer['text']) # find the end index of the answer\n",
        "                  answer_start_index = answer['answer_start'] # store the start index of the answer\n",
        "                  answer_end_index = answer['answer_end'] # store the end index of the answer\n",
        "                  answer_text = answer['text']  # get the answer text\n",
        "                  count = 0 # offset of the indexes\n",
        "                  # the answer start/end index may be off by some characters, so correct them\n",
        "                  while True:\n",
        "                    if context[answer_start_index-count:answer_end_index-count] == answer_text: # if we have the correct start/end indexes\n",
        "                      # store them in the answer dictionary\n",
        "                      answer['answer_start'] = answer_start_index - count\n",
        "                      answer['answer_end'] = answer_end_index - count\n",
        "                      break\n",
        "                    if count>4: # never actualy happens, just for safety reasons to avoid infinite loop\n",
        "                        break\n",
        "                    count = count + 1 # if the indexes are not correct, increase the offset in order to \"move\" the indexes\n",
        "                  # if USE_ALL_QNAS is true, then we use every question/answer\n",
        "                  # if USE_ALL_QNAS is false, the we only use the questions/answers that have total words < MAX_LENGTH\n",
        "                  if(len(context.split())+len(question.split())<MAX_LENGTH) or KEEP_ALL_LENGTH_QNAS:\n",
        "                    context_list.append(context)  # add the context of this question to the list\n",
        "                    question_list.append(question)   # add the question to the list\n",
        "                    answer_list.append(answer)   # add the answer to the list\n",
        "                    impossible_list.append(impossible)  # store whether the question is impossible or not\n",
        "\n",
        "  return context_list, question_list, answer_list, impossible_list\n",
        "\n",
        "train_contexts, train_questions, train_answers, train_impossible = read_file(train_path)  # read the training set file\n",
        "valid_contexts, valid_questions, valid_answers, valid_impossible = read_file(validation_path) # read the validation/dev set file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:27:18.595068Z",
          "iopub.status.busy": "2022-03-06T12:27:18.594816Z",
          "iopub.status.idle": "2022-03-06T12:27:18.602616Z",
          "shell.execute_reply": "2022-03-06T12:27:18.601069Z",
          "shell.execute_reply.started": "2022-03-06T12:27:18.595040Z"
        },
        "id": "l8sA_p3VhjrH",
        "outputId": "1c38ad4d-4316-4518-a2a7-67a98d2dfd77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "110865"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_contexts) # how many questions/answers we have in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a6ece92794be4354854fef1ce1ccc264",
            "2591cd0defb141aabb8b103f50ca1c0d",
            "20b1a6e476ab4b07bea9b1b1cae5987a",
            "f3ce993549f242d181f21530571c2063"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-06T12:27:19.922548Z",
          "iopub.status.busy": "2022-03-06T12:27:19.921850Z",
          "iopub.status.idle": "2022-03-06T12:28:19.858034Z",
          "shell.execute_reply": "2022-03-06T12:28:19.857278Z",
          "shell.execute_reply.started": "2022-03-06T12:27:19.922507Z"
        },
        "id": "NmDyZxyXw7Q0",
        "outputId": "4ea5ed74-4c20-490b-9a43-ea62db5a9daf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6ece92794be4354854fef1ce1ccc264",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2591cd0defb141aabb8b103f50ca1c0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20b1a6e476ab4b07bea9b1b1cae5987a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3ce993549f242d181f21530571c2063",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') # initialize tokenizer\n",
        "# tokenize the training set, giving contexts and questions to the tokenizer\n",
        "train_tokenized = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "# tokenize the validation set, giving contexts and questions to the tokenizer\n",
        "validation_tokenized = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:28:50.247083Z",
          "iopub.status.busy": "2022-03-06T12:28:50.246825Z",
          "iopub.status.idle": "2022-03-06T12:28:50.804775Z",
          "shell.execute_reply": "2022-03-06T12:28:50.803301Z",
          "shell.execute_reply.started": "2022-03-06T12:28:50.247055Z"
        },
        "id": "1qrg2MHpw7Q1",
        "outputId": "e1c040b7-5318-4f88-d9d1-667076263a1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "110865\n",
            "14\n",
            "4669\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "def index_answer(encodings, answers, impossible):\n",
        "    # after the tokenization, we need to convert the start/end indexes to the corresponding token indexes\n",
        "    start_positions = []  # list that will contain all the start indexes\n",
        "    end_positions = []  # list that will contain all the end indexes\n",
        "    count = 0 # how many answers were not found inside the tokenized context\n",
        "    for i in range(len(answers)):  # for every answer\n",
        "        if impossible[i]:   # if the question is impossible\n",
        "            # find the index of the first SEP token\n",
        "            sep_index = encodings['input_ids'][i].index(tokenizer.sep_token_id)\n",
        "            # set the start and the end index as the sep token index, as the correct answer is the empty string\n",
        "            start_positions.append(sep_index)\n",
        "            end_positions.append(sep_index)\n",
        "            continue\n",
        "        # if the question is not impossible\n",
        "        # convert the start index to the corresponding token index \n",
        "        start_positions.append(encodings[i].char_to_token(answers[i]['answer_start']))\n",
        "        # convert the end index to the corresponding token index\n",
        "        end_positions.append(encodings[i].char_to_token(answers[i]['answer_end']))\n",
        "\n",
        "        if start_positions[-1] is None: # if the start index was not found (due to the truncation of max_length)\n",
        "            # the answer is not inside the tokenized context\n",
        "            count = count + 1\n",
        "            start_positions[-1] = tokenizer.model_max_length  # assign the last token as the start\n",
        "\n",
        "        shift = 1\n",
        "        while end_positions[-1] is None:  # if the end index was not found\n",
        "            # decrease the original index until finding a corresponding token index\n",
        "            end_positions[-1] = encodings[i].char_to_token(answers[i]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "    print(len(answers))  # print how many questions/answers we have\n",
        "    print(count)  # print how many answers were not found inside the tokenized context\n",
        "    return start_positions, end_positions\n",
        "\n",
        "train_start,train_end = index_answer(train_tokenized, train_answers,train_impossible)\n",
        "validation_start,validation_end = index_answer(validation_tokenized, valid_answers, valid_impossible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:28:53.949939Z",
          "iopub.status.busy": "2022-03-06T12:28:53.949406Z",
          "iopub.status.idle": "2022-03-06T12:28:53.984924Z",
          "shell.execute_reply": "2022-03-06T12:28:53.983673Z",
          "shell.execute_reply.started": "2022-03-06T12:28:53.949903Z"
        },
        "id": "NyPMrYjRhjrL"
      },
      "outputs": [],
      "source": [
        "train_id = [tokenized for tokenized in train_tokenized['input_ids']] # store all the input_ids of the training set into a list\n",
        "train_mask = [tokenized for tokenized in train_tokenized['attention_mask']] # store all the attention_masks of the training set into a list\n",
        "validation_id = [tokenized for tokenized in validation_tokenized['input_ids']] # store all the input_ids of the validation set into a list\n",
        "validation_mask = [tokenized for tokenized in validation_tokenized['attention_mask']] # store all the attention_masks of the validation set into a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:28:56.476194Z",
          "iopub.status.busy": "2022-03-06T12:28:56.475461Z",
          "iopub.status.idle": "2022-03-06T12:28:58.695008Z",
          "shell.execute_reply": "2022-03-06T12:28:58.693895Z",
          "shell.execute_reply.started": "2022-03-06T12:28:56.476161Z"
        },
        "id": "dURYl1_ShjrL"
      },
      "outputs": [],
      "source": [
        "del train_tokenized,validation_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:29:14.696341Z",
          "iopub.status.busy": "2022-03-06T12:29:14.696066Z",
          "iopub.status.idle": "2022-03-06T12:29:27.924305Z",
          "shell.execute_reply": "2022-03-06T12:29:27.923562Z",
          "shell.execute_reply.started": "2022-03-06T12:29:14.696302Z"
        },
        "id": "SemNC8Gyw7Q1"
      },
      "outputs": [],
      "source": [
        "# convert all training data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "train_id_tensor = torch.tensor(train_id)\n",
        "train_mask_tensor = torch.tensor(train_mask)\n",
        "train_starts_tensors = torch.tensor(train_start)\n",
        "train_ends_tensors = torch.tensor(train_end)\n",
        "train_impossible_tensors = torch.tensor(train_impossible, dtype=torch.bool)\n",
        "# convert all validation data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "validation_id_tensor = torch.tensor(validation_id)\n",
        "validation_mask_tensor = torch.tensor(validation_mask)\n",
        "validation_starts_tensors = torch.tensor(validation_start)\n",
        "validation_ends_tensors = torch.tensor(validation_end)\n",
        "validation_impossible_tensors = torch.tensor(valid_impossible, dtype=torch.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:29:27.926045Z",
          "iopub.status.busy": "2022-03-06T12:29:27.925806Z",
          "iopub.status.idle": "2022-03-06T12:29:28.395075Z",
          "shell.execute_reply": "2022-03-06T12:29:28.394335Z",
          "shell.execute_reply.started": "2022-03-06T12:29:27.926011Z"
        },
        "id": "KSlb2PjnhjrM"
      },
      "outputs": [],
      "source": [
        "del train_id,train_mask,train_start,train_end,train_impossible\n",
        "del validation_id,validation_mask,validation_start,validation_end,valid_impossible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9419aa6268d54ec99e72a912f10251c0"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-06T12:29:30.572244Z",
          "iopub.status.busy": "2022-03-06T12:29:30.571996Z",
          "iopub.status.idle": "2022-03-06T12:29:46.320893Z",
          "shell.execute_reply": "2022-03-06T12:29:46.320073Z",
          "shell.execute_reply.started": "2022-03-06T12:29:30.572217Z"
        },
        "id": "o8Su0dDZw7Q1",
        "outputId": "d1692b8b-3590-4d09-da0e-0db5f4d19560"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9419aa6268d54ec99e72a912f10251c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda...\n"
          ]
        }
      ],
      "source": [
        "# define epochs\n",
        "TRAINING_EPOCHS = 2\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 3e-5\n",
        "batch_size = 16\n",
        "# define the model\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate)\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.to(device)\n",
        "  print('Using cuda...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:29:52.264537Z",
          "iopub.status.busy": "2022-03-06T12:29:52.263692Z",
          "iopub.status.idle": "2022-03-06T12:29:52.271035Z",
          "shell.execute_reply": "2022-03-06T12:29:52.270221Z",
          "shell.execute_reply.started": "2022-03-06T12:29:52.264483Z"
        },
        "id": "mbwE2FS3hjrN"
      },
      "outputs": [],
      "source": [
        "#Initialize dataloader for train set\n",
        "train_dataset = torch.utils.data.TensorDataset(train_id_tensor, train_mask_tensor,train_starts_tensors,train_ends_tensors,train_impossible_tensors)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "del train_dataset\n",
        "#Initialize dataloader for validation set\n",
        "validation_dataset = torch.utils.data.TensorDataset(validation_id_tensor, validation_mask_tensor,validation_starts_tensors,validation_ends_tensors,validation_impossible_tensors)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "del validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T12:29:57.486923Z",
          "iopub.status.busy": "2022-03-06T12:29:57.486511Z",
          "iopub.status.idle": "2022-03-06T15:52:23.722410Z",
          "shell.execute_reply": "2022-03-06T15:52:23.721695Z",
          "shell.execute_reply.started": "2022-03-06T12:29:57.486887Z"
        },
        "id": "WctAGzRNw7Q2",
        "outputId": "8554fec9-1c44-4cc0-c676-f5ebaef1deec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6930/6930 [1:39:46<00:00,  1.16it/s]\n",
            "100%|██████████| 292/292 [01:23<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0: Train Loss = 1.52618 | Validation Loss = 1.49361 | Train Accuracy = 0.5548 | Validation Accuracy = 0.5560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6930/6930 [1:39:52<00:00,  1.16it/s]\n",
            "100%|██████████| 292/292 [01:23<00:00,  3.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1: Train Loss = 1.02521 | Validation Loss = 1.45779 | Train Accuracy = 0.6175 | Validation Accuracy = 0.5630\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train_model(model,train_loader,validation_loader,optimizer,epochs = 5):\n",
        "  train_loss = [] # store the train loss for every epoch\n",
        "  valid_loss = []  # store the validation loss for every epoch\n",
        "  train_accuracy = []\n",
        "  valid_accuracy = []\n",
        "  for epoch in range(epochs):\n",
        "    train_batch_losses = [] # store the loss of every batch of the train set\n",
        "    validation_batch_losses = [] # store the loss of every batch of the validation set\n",
        "    model.train() # set the model to training mode\n",
        "    y_total_predict_train = []   # store the predictions of the the train set\n",
        "    y_total_train = []     # store the labels of the the train set\n",
        "    train_loader_tqdm = tqdm(train_loader)\n",
        "    for x_batch, mask, start, end, impossible in train_loader_tqdm:\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      start = start.to(device)\n",
        "      end = end.to(device)\n",
        "      mask = mask.to(device)\n",
        "      #Delete previously stored gradients\n",
        "      optimizer.zero_grad()\n",
        "      z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)\n",
        "      loss = z[0] # get the output loss\n",
        "      train_batch_losses.append(loss.item()) # store the train loss of the current batch\n",
        "      #Perform backpropagation starting from the loss calculated in this epoch\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(),1.0)    # perform gradient clipping\n",
        "      #Update model's weights based on the gradients calculated during backprop\n",
        "      optimizer.step()\n",
        "      start_pred = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "      # calculate accuracy for both and append to accuracy list\n",
        "      train_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "      train_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "      del x_batch, mask, start, end\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()   # set the model to evaluation mode\n",
        "    y_total_valid = []   # store the predictions of the the validation set\n",
        "    y_total_predict_valid = []      # store the labels of the the validation set\n",
        "    validation_loader_tqdm = tqdm(validation_loader)\n",
        "    with torch.no_grad():\n",
        "        for x_batch, mask, start, end, impossible in validation_loader_tqdm:\n",
        "          # transfer data to GPU\n",
        "          x_batch = x_batch.to(device)\n",
        "          start = start.to(device)\n",
        "          end = end.to(device)\n",
        "          mask = mask.to(device)\n",
        "          z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)  \n",
        "          loss = z[0]  # get the output loss\n",
        "          validation_batch_losses.append(loss.item())   # store the validation loss of the current batch\n",
        "          start_pred = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "          end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "          # calculate accuracy for both and append to accuracy list\n",
        "          valid_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "          valid_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "          del x_batch, mask, start, end\n",
        "    t_accuracy = sum(train_accuracy)/len(train_accuracy) # compute train accuracy based on the correct predictions\n",
        "    v_accuracy = sum(valid_accuracy)/len(valid_accuracy) # compute validation accuracy based on the correct predictions\n",
        "    current_train_loss = sum(train_batch_losses)/len(train_loader)  #compute the train loss of this epoch\n",
        "    train_loss.append(current_train_loss)   # store the train loss in order to plot loss curves\n",
        "    current_valid_loss = sum(validation_batch_losses)/len(validation_loader)   #compute the validation loss of this epoch\n",
        "    valid_loss.append(current_valid_loss)   # store the validation loss in order to plot loss curves\n",
        "    print(f\"Epoch {epoch:3}: Train Loss = {current_train_loss:.5f} | Validation Loss = {current_valid_loss:.5f} | Train Accuracy = {t_accuracy:.4f} | Validation Accuracy = {v_accuracy:.4f}\")\n",
        "  return model,train_loss,valid_loss\n",
        "\n",
        "# train the model\n",
        "trained_model, train_loss, valid_loss = train_model(model,train_loader,validation_loader,optimizer,TRAINING_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOm0_EWUIk7C"
      },
      "source": [
        "### Natural Questions (NQ) fined-tuned model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T15:53:39.205731Z",
          "iopub.status.busy": "2022-03-06T15:53:39.205130Z",
          "iopub.status.idle": "2022-03-06T15:54:41.691765Z",
          "shell.execute_reply": "2022-03-06T15:54:41.691043Z",
          "shell.execute_reply.started": "2022-03-06T15:53:39.205686Z"
        },
        "id": "2aLHy4ZzhjrP",
        "outputId": "3619885f-529d-4fbf-dd62-c807d474aeeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 211/211 [01:00<00:00,  3.48it/s]\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:  # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]  # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i))  # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer    # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T16:05:47.564423Z",
          "iopub.status.busy": "2022-03-06T16:05:47.564134Z",
          "iopub.status.idle": "2022-03-06T16:05:49.232313Z",
          "shell.execute_reply": "2022-03-06T16:05:49.231390Z",
          "shell.execute_reply.started": "2022-03-06T16:05:47.564390Z"
        },
        "id": "iX4r5wRchjrQ",
        "outputId": "6e62a960-da64-4f96-ef61-7a4018c292b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 64.49985158800831,\n",
            "  \"f1\": 68.7583873447665,\n",
            "  \"total\": 3369,\n",
            "  \"HasAns_exact\": 57.088285229202036,\n",
            "  \"HasAns_f1\": 63.177846759133345,\n",
            "  \"HasAns_total\": 2356,\n",
            "  \"NoAns_exact\": 81.73741362290227,\n",
            "  \"NoAns_f1\": 81.73741362290227,\n",
            "  \"NoAns_total\": 1013\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py nq_dev.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXs43yxIH5sy"
      },
      "source": [
        "### Natural Questions (NQ) fined-tuned model test on SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:48:54.885758Z",
          "iopub.status.busy": "2022-03-06T23:48:54.885480Z",
          "iopub.status.idle": "2022-03-06T23:48:54.889953Z",
          "shell.execute_reply": "2022-03-06T23:48:54.888806Z",
          "shell.execute_reply.started": "2022-03-06T23:48:54.885720Z"
        },
        "id": "alOfv_muH5s0"
      },
      "outputs": [],
      "source": [
        "validation_path = 'dev-v2.0.json'    # path to the validation-dev set of SQuAD dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:48:54.892268Z",
          "iopub.status.busy": "2022-03-06T23:48:54.891963Z",
          "iopub.status.idle": "2022-03-06T23:52:34.959557Z",
          "shell.execute_reply": "2022-03-06T23:52:34.958641Z",
          "shell.execute_reply.started": "2022-03-06T23:48:54.892231Z"
        },
        "id": "8LkArYQQH5s0",
        "outputId": "c6bf49c4-eaee-4d4c-cefc-525e058f6e41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 186/186 [03:25<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 44.175861197675395,\n",
            "  \"f1\": 47.00064431383382,\n",
            "  \"total\": 11873,\n",
            "  \"HasAns_exact\": 33.8225371120108,\n",
            "  \"HasAns_f1\": 39.480204105625575,\n",
            "  \"HasAns_total\": 5928,\n",
            "  \"NoAns_exact\": 54.49957947855341,\n",
            "  \"NoAns_f1\": 54.49957947855341,\n",
            "  \"NoAns_total\": 5945\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:      # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i))# get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer  # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py dev-v2.0.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4SoC1JlH5s1"
      },
      "source": [
        "### Natural Questions (NQ) fined-tuned model test on TRIVIA QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:52:34.961593Z",
          "iopub.status.busy": "2022-03-06T23:52:34.961334Z",
          "iopub.status.idle": "2022-03-06T23:52:34.968222Z",
          "shell.execute_reply": "2022-03-06T23:52:34.967397Z",
          "shell.execute_reply.started": "2022-03-06T23:52:34.961567Z"
        },
        "id": "jX1oCBhHH5s1"
      },
      "outputs": [],
      "source": [
        "validation_path = 'triviaqa_dev_SquadFormat.json'    # path to the validation-dev set of TriviaQA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:52:34.971004Z",
          "iopub.status.busy": "2022-03-06T23:52:34.970432Z",
          "iopub.status.idle": "2022-03-06T23:57:28.239033Z",
          "shell.execute_reply": "2022-03-06T23:57:28.238200Z",
          "shell.execute_reply.started": "2022-03-06T23:52:34.970969Z"
        },
        "id": "YFwb2SceH5s1",
        "outputId": "1996e17d-1724-416c-a7fc-b843266b8b6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [04:08<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 31.899641577060933,\n",
            "  \"f1\": 36.44250312583861,\n",
            "  \"total\": 14229,\n",
            "  \"HasAns_exact\": 21.011989433042064,\n",
            "  \"HasAns_f1\": 27.579798514281354,\n",
            "  \"HasAns_total\": 9842,\n",
            "  \"NoAns_exact\": 56.32550718030545,\n",
            "  \"NoAns_f1\": 56.32550718030545,\n",
            "  \"NoAns_total\": 4387\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:     # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i))   # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer     # assign it as the answer in the dictionary\n",
        "      del x_batch, mask \n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py triviaqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrZOtCUfH5s2"
      },
      "source": [
        "### Natural Questions (NQ) fined-tuned model test on NewsQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:57:28.243506Z",
          "iopub.status.busy": "2022-03-06T23:57:28.242987Z",
          "iopub.status.idle": "2022-03-06T23:57:28.248627Z",
          "shell.execute_reply": "2022-03-06T23:57:28.247139Z",
          "shell.execute_reply.started": "2022-03-06T23:57:28.243473Z"
        },
        "id": "9lpEIQdOH5s2"
      },
      "outputs": [],
      "source": [
        "validation_path = 'newsqa_dev_SquadFormat.json'    # path to the validation-dev set of NewsQA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:57:28.251180Z",
          "iopub.status.busy": "2022-03-06T23:57:28.250577Z",
          "iopub.status.idle": "2022-03-06T23:59:16.603628Z",
          "shell.execute_reply": "2022-03-06T23:59:16.602783Z",
          "shell.execute_reply.started": "2022-03-06T23:57:28.251131Z"
        },
        "id": "LaMSj-A_H5s3",
        "outputId": "b0866a82-02f1-4f41-d1d3-53ba2fc65759"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 81/81 [01:29<00:00,  1.11s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 9.930313588850174,\n",
            "  \"f1\": 13.07485522398131,\n",
            "  \"total\": 5166,\n",
            "  \"HasAns_exact\": 9.930313588850174,\n",
            "  \"HasAns_f1\": 13.07485522398131,\n",
            "  \"HasAns_total\": 5166\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:     # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):       # for every item of the batch\n",
        "        i = (int(id_index_i))   # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer     # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py newsqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUV3fk03H5s3"
      },
      "source": [
        "### Natural Questions (NQ) fined-tuned model test on Quac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:59:16.605665Z",
          "iopub.status.busy": "2022-03-06T23:59:16.605385Z",
          "iopub.status.idle": "2022-03-06T23:59:16.609932Z",
          "shell.execute_reply": "2022-03-06T23:59:16.609255Z",
          "shell.execute_reply.started": "2022-03-06T23:59:16.605629Z"
        },
        "id": "gQCIF5UsH5s3"
      },
      "outputs": [],
      "source": [
        "validation_path = 'quac_dev_SquadFormat.json'  # path to the validation-dev set of TRIVIA QA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-06T23:59:16.612151Z",
          "iopub.status.busy": "2022-03-06T23:59:16.611214Z",
          "iopub.status.idle": "2022-03-07T00:00:20.323505Z",
          "shell.execute_reply": "2022-03-07T00:00:20.322683Z",
          "shell.execute_reply.started": "2022-03-06T23:59:16.612113Z"
        },
        "id": "GZYONVyXH5s4",
        "outputId": "a0c741e1-9531-4170-9096-fe1154d2f478"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 115/115 [00:55<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 17.555072069621975,\n",
            "  \"f1\": 18.321486880235494,\n",
            "  \"total\": 7354,\n",
            "  \"HasAns_exact\": 0.2556237218813906,\n",
            "  \"HasAns_f1\": 1.2161238100292884,\n",
            "  \"HasAns_total\": 5868,\n",
            "  \"NoAns_exact\": 85.86810228802153,\n",
            "  \"NoAns_f1\": 85.86810228802153,\n",
            "  \"NoAns_total\": 1486\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:      # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):     # for every item of the batch\n",
        "        i = (int(id_index_i))   # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer   # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py quac_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJwdoUmUGjO9"
      },
      "source": [
        "## QUAC FINETUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T13:39:45.432967Z",
          "iopub.status.busy": "2022-03-04T13:39:45.432138Z",
          "iopub.status.idle": "2022-03-04T13:39:45.438025Z",
          "shell.execute_reply": "2022-03-04T13:39:45.436705Z",
          "shell.execute_reply.started": "2022-03-04T13:39:45.432927Z"
        },
        "id": "SxQ8Xu-1f5Vx"
      },
      "outputs": [],
      "source": [
        "# the train and dev files were downloaded and transformed in paragraph Data Creation, so store the names of the files\n",
        "output_file_train = 'quac_train_SquadFormat.json'\n",
        "output_file_dev = 'quac_dev_SquadFormat.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:15:50.389106Z",
          "iopub.status.busy": "2022-03-04T16:15:50.388499Z",
          "iopub.status.idle": "2022-03-04T16:15:50.395254Z",
          "shell.execute_reply": "2022-03-04T16:15:50.394344Z",
          "shell.execute_reply.started": "2022-03-04T16:15:50.389068Z"
        },
        "id": "wCJMAWc3w7Q6"
      },
      "outputs": [],
      "source": [
        "train_path = output_file_train\n",
        "validation_path = output_file_dev\n",
        "\n",
        "MAX_LENGTH = 512    # max length that will be used during tokenization\n",
        "USE_IMPOSSIBLE_QNAS = True   # include also impossible questions in the training set\n",
        "KEEP_ALL_LENGTH_QNAS = True  # use the entire dataset in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:19:31.893160Z",
          "iopub.status.busy": "2022-03-04T16:19:31.892463Z",
          "iopub.status.idle": "2022-03-04T16:19:31.946374Z",
          "shell.execute_reply": "2022-03-04T16:19:31.945597Z",
          "shell.execute_reply.started": "2022-03-04T16:19:31.893123Z"
        },
        "id": "IVbn4K7Xw7Q6"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:15:56.520221Z",
          "iopub.status.busy": "2022-03-04T16:15:56.519967Z",
          "iopub.status.idle": "2022-03-04T16:15:58.243131Z",
          "shell.execute_reply": "2022-03-04T16:15:58.242340Z",
          "shell.execute_reply.started": "2022-03-04T16:15:56.520194Z"
        },
        "id": "WXgGqQxpw7Q6"
      },
      "outputs": [],
      "source": [
        "def read_file(file_name):  # read data from .json file\n",
        "  with open(file_name, 'rb') as file:  # open the file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain all the context that correspond to every question/answer set\n",
        "  question_list = []  # list that will contain all the quections. Questions that have multiple answers will be inserted multiple times at the list\n",
        "  answer_list = []  # list that will contain all the answers to the questions\n",
        "  impossible_list = []  # list that will contatin if the corresponding question in the question list is impossible or not\n",
        "  squad = squad['data']  # get the data\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this paragraph\n",
        "              question = qa['question'] # store the question text\n",
        "              impossible = qa['is_impossible']  # store if the question is impossible or not\n",
        "              if impossible and USE_IMPOSSIBLE_QNAS:  # if the question is impossible\n",
        "                  context_list.append(context)  # add the context of this question to the list\n",
        "                  question_list.append(question)  # add the question to the list\n",
        "                  answer_list.append({'text':\"\",'answer_start':0,'answer_end':0})  # add the empty string as the answer of the question\n",
        "                  impossible_list.append(impossible)    # store whether the question is impossible or not\n",
        "              for answer in qa['answers']: # if the question is possible, then it has one or more correct answers\n",
        "                  answer['answer_end'] = answer['answer_start'] + len(answer['text']) # find the end index of the answer\n",
        "                  answer_start_index = answer['answer_start'] # store the start index of the answer\n",
        "                  answer_end_index = answer['answer_end'] # store the end index of the answer\n",
        "                  answer_text = answer['text']  # get the answer text\n",
        "                  count = 0 # offset of the indexes\n",
        "                  # the answer start/end index may be off by some characters, so correct them\n",
        "                  while True:\n",
        "                    if context[answer_start_index-count:answer_end_index-count] == answer_text: # if we have the correct start/end indexes\n",
        "                      # store them in the answer dictionary\n",
        "                      answer['answer_start'] = answer_start_index - count\n",
        "                      answer['answer_end'] = answer_end_index - count\n",
        "                      break\n",
        "                    if count>4: # never actualy happens, just for safety reasons to avoid infinite loop\n",
        "                        break\n",
        "                    count = count + 1 # if the indexes are not correct, increase the offset in order to \"move\" the indexes\n",
        "                  # if USE_ALL_QNAS is true, then we use every question/answer\n",
        "                  # if USE_ALL_QNAS is false, the we only use the questions/answers that have total words < MAX_LENGTH\n",
        "                  if(len(context.split())+len(question.split())<MAX_LENGTH) or KEEP_ALL_LENGTH_QNAS:\n",
        "                    context_list.append(context)  # add the context of this question to the list\n",
        "                    question_list.append(question)   # add the question to the list\n",
        "                    answer_list.append(answer)   # add the answer to the list\n",
        "                    impossible_list.append(impossible)  # store whether the question is impossible or not\n",
        "\n",
        "  return context_list, question_list, answer_list, impossible_list\n",
        "\n",
        "train_contexts, train_questions, train_answers, train_impossible = read_file(train_path)  # read the training set file\n",
        "valid_contexts, valid_questions, valid_answers, valid_impossible = read_file(validation_path) # read the validation/dev set file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:16:02.306607Z",
          "iopub.status.busy": "2022-03-04T16:16:02.305071Z",
          "iopub.status.idle": "2022-03-04T16:16:02.323333Z",
          "shell.execute_reply": "2022-03-04T16:16:02.322436Z",
          "shell.execute_reply.started": "2022-03-04T16:16:02.306561Z"
        },
        "id": "VoE1lDuef5V0",
        "outputId": "b92c6d89-6c0e-40e2-c4c2-d488ab85b878"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83568"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_contexts) # how many questions/answers we have in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7cdef0593e104019b7d1c17bb85357fb",
            "f3b7b474cc6b4373a7fc9f82d81cbfc9",
            "10817c005a16421d82bfed17d3da5a66",
            "52fac4211e6f403cb80b4a199aa60d32"
          ]
        },
        "execution": {
          "iopub.execute_input": "2022-03-04T16:16:05.110375Z",
          "iopub.status.busy": "2022-03-04T16:16:05.110090Z",
          "iopub.status.idle": "2022-03-04T16:16:49.426279Z",
          "shell.execute_reply": "2022-03-04T16:16:49.425504Z",
          "shell.execute_reply.started": "2022-03-04T16:16:05.110341Z"
        },
        "id": "0qQdj-Tfw7Q6",
        "outputId": "4ea5ed74-4c20-490b-9a43-ea62db5a9daf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cdef0593e104019b7d1c17bb85357fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3b7b474cc6b4373a7fc9f82d81cbfc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10817c005a16421d82bfed17d3da5a66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52fac4211e6f403cb80b4a199aa60d32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') # initialize tokenizer\n",
        "# tokenize the training set, giving contexts and questions to the tokenizer\n",
        "train_tokenized = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "# tokenize the validation set, giving contexts and questions to the tokenizer\n",
        "validation_tokenized = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:16:55.481717Z",
          "iopub.status.busy": "2022-03-04T16:16:55.481437Z",
          "iopub.status.idle": "2022-03-04T16:18:07.052650Z",
          "shell.execute_reply": "2022-03-04T16:18:07.051116Z",
          "shell.execute_reply.started": "2022-03-04T16:16:55.481688Z"
        },
        "id": "tO54mJN4w7Q6",
        "outputId": "1110df67-6dd8-42e7-b0b5-f8da1b9cbc4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83568\n",
            "45242\n",
            "7354\n",
            "3918\n"
          ]
        }
      ],
      "source": [
        "def index_answer(encodings, answers, impossible):\n",
        "    # after the tokenization, we need to convert the start/end indexes to the corresponding token indexes\n",
        "    start_positions = []  # list that will contain all the start indexes\n",
        "    end_positions = []  # list that will contain all the end indexes\n",
        "    count = 0 # how many answers were not found inside the tokenized context\n",
        "    for i in range(len(answers)):  # for every answer\n",
        "        if impossible[i]:   # if the question is impossible\n",
        "            # find the index of the first SEP token\n",
        "            sep_index = encodings['input_ids'][i].index(tokenizer.sep_token_id)\n",
        "            # set the start and the end index as the sep token index, as the correct answer is the empty string\n",
        "            start_positions.append(sep_index)\n",
        "            end_positions.append(sep_index)\n",
        "            continue\n",
        "        # if the question is not impossible\n",
        "        # convert the start index to the corresponding token index \n",
        "        start_positions.append(encodings[i].char_to_token(answers[i]['answer_start']))\n",
        "        # convert the end index to the corresponding token index\n",
        "        end_positions.append(encodings[i].char_to_token(answers[i]['answer_end']))\n",
        "\n",
        "        if start_positions[-1] is None: # if the start index was not found (due to the truncation of max_length)\n",
        "            # the answer is not inside the tokenized context\n",
        "            count = count + 1\n",
        "            start_positions[-1] = tokenizer.model_max_length  # assign the last token as the start\n",
        "\n",
        "        shift = 1\n",
        "        while end_positions[-1] is None:  # if the end index was not found\n",
        "            # decrease the original index until finding a corresponding token index\n",
        "            end_positions[-1] = encodings[i].char_to_token(answers[i]['answer_end'] - shift)\n",
        "            shift += 1\n",
        "    print(len(answers))  # print how many questions/answers we have\n",
        "    print(count)  # print how many answers were not found inside the tokenized context\n",
        "    return start_positions, end_positions\n",
        "\n",
        "train_start,train_end = index_answer(train_tokenized, train_answers,train_impossible)\n",
        "validation_start,validation_end = index_answer(validation_tokenized, valid_answers, valid_impossible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:18:33.170016Z",
          "iopub.status.busy": "2022-03-04T16:18:33.169745Z",
          "iopub.status.idle": "2022-03-04T16:18:33.193710Z",
          "shell.execute_reply": "2022-03-04T16:18:33.192822Z",
          "shell.execute_reply.started": "2022-03-04T16:18:33.169979Z"
        },
        "id": "ssRzpeIxf5V2"
      },
      "outputs": [],
      "source": [
        "train_id = [tokenized for tokenized in train_tokenized['input_ids']] # store all the input_ids of the training set into a list\n",
        "train_mask = [tokenized for tokenized in train_tokenized['attention_mask']] # store all the attention_masks of the training set into a list\n",
        "validation_id = [tokenized for tokenized in validation_tokenized['input_ids']] # store all the input_ids of the validation set into a list\n",
        "validation_mask = [tokenized for tokenized in validation_tokenized['attention_mask']] # store all the attention_masks of the validation set into a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:18:35.688110Z",
          "iopub.status.busy": "2022-03-04T16:18:35.687645Z",
          "iopub.status.idle": "2022-03-04T16:18:36.887595Z",
          "shell.execute_reply": "2022-03-04T16:18:36.886643Z",
          "shell.execute_reply.started": "2022-03-04T16:18:35.688072Z"
        },
        "id": "woSz0k2Tf5V2"
      },
      "outputs": [],
      "source": [
        "del train_tokenized,validation_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:18:39.931249Z",
          "iopub.status.busy": "2022-03-04T16:18:39.930763Z",
          "iopub.status.idle": "2022-03-04T16:18:47.287574Z",
          "shell.execute_reply": "2022-03-04T16:18:47.286331Z",
          "shell.execute_reply.started": "2022-03-04T16:18:39.931204Z"
        },
        "id": "aXKnSg45w7Q7"
      },
      "outputs": [],
      "source": [
        "# convert all training data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "train_id_tensor = torch.tensor(train_id)\n",
        "train_mask_tensor = torch.tensor(train_mask)\n",
        "train_starts_tensors = torch.tensor(train_start)\n",
        "train_ends_tensors = torch.tensor(train_end)\n",
        "train_impossible_tensors = torch.tensor(train_impossible, dtype=torch.bool)\n",
        "# convert all validation data into tensors (input_ids, attention_masks, start indexes, end indexes, impossible)\n",
        "validation_id_tensor = torch.tensor(validation_id)\n",
        "validation_mask_tensor = torch.tensor(validation_mask)\n",
        "validation_starts_tensors = torch.tensor(validation_start)\n",
        "validation_ends_tensors = torch.tensor(validation_end)\n",
        "validation_impossible_tensors = torch.tensor(valid_impossible, dtype=torch.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:18:50.325225Z",
          "iopub.status.busy": "2022-03-04T16:18:50.324462Z",
          "iopub.status.idle": "2022-03-04T16:18:50.621898Z",
          "shell.execute_reply": "2022-03-04T16:18:50.620859Z",
          "shell.execute_reply.started": "2022-03-04T16:18:50.325180Z"
        },
        "id": "V6q034Nlf5V3"
      },
      "outputs": [],
      "source": [
        "del train_id,train_mask,train_start,train_end,train_impossible\n",
        "del validation_id,validation_mask,validation_start,validation_end,valid_impossible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:19:36.250950Z",
          "iopub.status.busy": "2022-03-04T16:19:36.250224Z",
          "iopub.status.idle": "2022-03-04T16:19:43.025718Z",
          "shell.execute_reply": "2022-03-04T16:19:43.024961Z",
          "shell.execute_reply.started": "2022-03-04T16:19:36.250913Z"
        },
        "id": "0O4sUk1rw7Q7",
        "outputId": "d1692b8b-3590-4d09-da0e-0db5f4d19560"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda...\n"
          ]
        }
      ],
      "source": [
        "# define epochs\n",
        "TRAINING_EPOCHS = 2\n",
        "\n",
        "#Define Hyperparameters\n",
        "learning_rate = 3e-5\n",
        "batch_size = 16\n",
        "# define the model\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate)\n",
        "\n",
        "if use_cuda:\n",
        "  model = model.to(device)\n",
        "  print('Using cuda...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:19:46.068973Z",
          "iopub.status.busy": "2022-03-04T16:19:46.068350Z",
          "iopub.status.idle": "2022-03-04T16:19:46.077008Z",
          "shell.execute_reply": "2022-03-04T16:19:46.076246Z",
          "shell.execute_reply.started": "2022-03-04T16:19:46.068933Z"
        },
        "id": "OThkHLhHf5V4"
      },
      "outputs": [],
      "source": [
        "#Initialize dataloader for train set\n",
        "train_dataset = torch.utils.data.TensorDataset(train_id_tensor, train_mask_tensor,train_starts_tensors,train_ends_tensors,train_impossible_tensors)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "del train_dataset\n",
        "#Initialize dataloader for validation set\n",
        "validation_dataset = torch.utils.data.TensorDataset(validation_id_tensor, validation_mask_tensor,validation_starts_tensors,validation_ends_tensors,validation_impossible_tensors)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "del validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T16:19:48.753756Z",
          "iopub.status.busy": "2022-03-04T16:19:48.753429Z",
          "iopub.status.idle": "2022-03-04T18:12:08.809257Z",
          "shell.execute_reply": "2022-03-04T18:12:08.808551Z",
          "shell.execute_reply.started": "2022-03-04T16:19:48.753718Z"
        },
        "id": "H5Ly2T-Sw7Q8",
        "outputId": "8554fec9-1c44-4cc0-c676-f5ebaef1deec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5223/5223 [55:10<00:00,  1.58it/s]\n",
            "100%|██████████| 460/460 [01:00<00:00,  7.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0: Train Loss = 2.73668 | Validation Loss = 2.43826 | Train Accuracy = 0.3481 | Validation Accuracy = 0.3838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5223/5223 [55:09<00:00,  1.58it/s]\n",
            "100%|██████████| 460/460 [01:00<00:00,  7.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1: Train Loss = 2.47316 | Validation Loss = 2.49030 | Train Accuracy = 0.3595 | Validation Accuracy = 0.3855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train_model(model,train_loader,validation_loader,optimizer,epochs = 5):\n",
        "  train_loss = [] # store the train loss for every epoch\n",
        "  valid_loss = []  # store the validation loss for every epoch\n",
        "  train_accuracy = []\n",
        "  valid_accuracy = []\n",
        "  for epoch in range(epochs):\n",
        "    train_batch_losses = [] # store the loss of every batch of the train set\n",
        "    validation_batch_losses = [] # store the loss of every batch of the validation set\n",
        "    model.train() # set the model to training mode\n",
        "    y_total_predict_train = []   # store the predictions of the the train set\n",
        "    y_total_train = []     # store the labels of the the train set\n",
        "    train_loader_tqdm = tqdm(train_loader)\n",
        "    for x_batch, mask, start, end, impossible in train_loader_tqdm:\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      start = start.to(device)\n",
        "      end = end.to(device)\n",
        "      mask = mask.to(device)\n",
        "      #Delete previously stored gradients\n",
        "      optimizer.zero_grad()\n",
        "      z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)\n",
        "      loss = z[0]  # get the output loss\n",
        "      train_batch_losses.append(loss.item()) # store the train loss of the current batch\n",
        "      #Perform backpropagation starting from the loss calculated in this epoch\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(),1.0) # perform gradient clipping\n",
        "      #Update model's weights based on the gradients calculated during backprop\n",
        "      optimizer.step()\n",
        "      start_pred = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      end_pred = torch.argmax(z['end_logits'], dim=1) # find the predicted end index, based on the bigger logit\n",
        "      # calculate accuracy for both and append to accuracy list\n",
        "      train_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "      train_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "      del x_batch, mask, start, end\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    model.eval()   # set the model to evaluation mode\n",
        "    y_total_valid = []   # store the predictions of the the validation set\n",
        "    y_total_predict_valid = []      # store the labels of the the validation set\n",
        "    validation_loader_tqdm = tqdm(validation_loader)\n",
        "    with torch.no_grad():\n",
        "        for x_batch, mask, start, end, impossible in validation_loader_tqdm:\n",
        "          # transfer data to GPU\n",
        "          x_batch = x_batch.to(device)\n",
        "          start = start.to(device)\n",
        "          end = end.to(device)\n",
        "          mask = mask.to(device)\n",
        "          z = model(x_batch, attention_mask=mask, start_positions=start, end_positions=end)  \n",
        "          loss = z[0]   # get the output loss\n",
        "          validation_batch_losses.append(loss.item())   # store the validation loss of the current batch\n",
        "          start_pred = torch.argmax(z['start_logits'], dim=1)\n",
        "          end_pred = torch.argmax(z['end_logits'], dim=1)\n",
        "          # calculate accuracy for both and append to accuracy list\n",
        "          valid_accuracy.append(((start_pred == start).sum()/len(start_pred)).item())\n",
        "          valid_accuracy.append(((end_pred == end).sum()/len(end_pred)).item())\n",
        "          del x_batch, mask, start, end\n",
        "    t_accuracy = sum(train_accuracy)/len(train_accuracy) # compute train accuracy based on the correct predictions\n",
        "    v_accuracy = sum(valid_accuracy)/len(valid_accuracy) # compute validation accuracy based on the correct predictions\n",
        "    current_train_loss = sum(train_batch_losses)/len(train_loader)  #compute the train loss of this epoch\n",
        "    train_loss.append(current_train_loss)   # store the train loss in order to plot loss curves\n",
        "    current_valid_loss = sum(validation_batch_losses)/len(validation_loader)   #compute the validation loss of this epoch\n",
        "    valid_loss.append(current_valid_loss)   # store the validation loss in order to plot loss curves\n",
        "    print(f\"Epoch {epoch:3}: Train Loss = {current_train_loss:.5f} | Validation Loss = {current_valid_loss:.5f} | Train Accuracy = {t_accuracy:.4f} | Validation Accuracy = {v_accuracy:.4f}\")\n",
        "  return model,train_loss,valid_loss\n",
        "\n",
        "# train the model\n",
        "trained_model, train_loss, valid_loss = train_model(model,train_loader,validation_loader,optimizer,TRAINING_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeDKWeLRHEbu"
      },
      "source": [
        "### Quac fined-tuned model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T18:14:38.840256Z",
          "iopub.status.busy": "2022-03-04T18:14:38.839824Z",
          "iopub.status.idle": "2022-03-04T18:15:46.023569Z",
          "shell.execute_reply": "2022-03-04T18:15:46.022843Z",
          "shell.execute_reply.started": "2022-03-04T18:14:38.840221Z"
        },
        "id": "sY55dVPyf5V6",
        "outputId": "9fe82efd-985e-4e73-b796-b7eb2bb3d59c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 460/460 [01:04<00:00,  7.19it/s]\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:  # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):  # for every item of the batch\n",
        "        i = (int(id_index_i)) # get the list index of the item\n",
        "        # get the predicted answer tokens from the tokenized context \n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer   # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-04T18:15:49.462172Z",
          "iopub.status.busy": "2022-03-04T18:15:49.461914Z",
          "iopub.status.idle": "2022-03-04T18:15:51.557664Z",
          "shell.execute_reply": "2022-03-04T18:15:51.556836Z",
          "shell.execute_reply.started": "2022-03-04T18:15:49.462143Z"
        },
        "id": "FL_PsA1pf5V6",
        "outputId": "0ae5a6c0-088a-441f-997c-13f63fbc7a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 18.30296437313027,\n",
            "  \"f1\": 20.626521960802588,\n",
            "  \"total\": 7354,\n",
            "  \"HasAns_exact\": 0.0,\n",
            "  \"HasAns_f1\": 2.9119704328123097,\n",
            "  \"HasAns_total\": 5868,\n",
            "  \"NoAns_exact\": 90.57873485868102,\n",
            "  \"NoAns_f1\": 90.57873485868102,\n",
            "  \"NoAns_total\": 1486\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py quac_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjVbHJg_F-xA"
      },
      "source": [
        "### Quac fined-tuned model test on SQuAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-07T00:08:48.701387Z",
          "iopub.status.busy": "2022-03-07T00:08:48.701124Z",
          "iopub.status.idle": "2022-03-07T00:08:48.705557Z",
          "shell.execute_reply": "2022-03-07T00:08:48.704917Z",
          "shell.execute_reply.started": "2022-03-07T00:08:48.701349Z"
        },
        "id": "kld_qtu_F-xB"
      },
      "outputs": [],
      "source": [
        "validation_path = 'dev-v2.0.json'     # path to the validation-dev set of SQuAD dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-07T00:08:48.707656Z",
          "iopub.status.busy": "2022-03-07T00:08:48.706891Z",
          "iopub.status.idle": "2022-03-07T00:12:41.836771Z",
          "shell.execute_reply": "2022-03-07T00:12:41.835968Z",
          "shell.execute_reply.started": "2022-03-07T00:08:48.707621Z"
        },
        "id": "QhznjAGSF-xB",
        "outputId": "c9ef9ed2-93f4-45b0-9e10-cf23677b4ee6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 186/186 [03:35<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 31.525309525814873,\n",
            "  \"f1\": 32.90385135917714,\n",
            "  \"total\": 11873,\n",
            "  \"HasAns_exact\": 0.0,\n",
            "  \"HasAns_f1\": 2.7610369749511596,\n",
            "  \"HasAns_total\": 5928,\n",
            "  \"NoAns_exact\": 62.96047098402018,\n",
            "  \"NoAns_f1\": 62.96047098402018,\n",
            "  \"NoAns_total\": 5945\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:   # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]    # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):   # for every item of the batch\n",
        "        i = (int(id_index_i))   # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer     # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py dev-v2.0.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7eFJHP-F-xC"
      },
      "source": [
        "### Quac fined-tuned model test on TRIVIA QA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-07T00:12:41.840206Z",
          "iopub.status.busy": "2022-03-07T00:12:41.839965Z",
          "iopub.status.idle": "2022-03-07T00:12:41.843873Z",
          "shell.execute_reply": "2022-03-07T00:12:41.843212Z",
          "shell.execute_reply.started": "2022-03-07T00:12:41.840178Z"
        },
        "id": "dc50brZBF-xD"
      },
      "outputs": [],
      "source": [
        "validation_path = 'triviaqa_dev_SquadFormat.json'  # path to the validation-dev set of TRIVIA QA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-07T00:12:41.845775Z",
          "iopub.status.busy": "2022-03-07T00:12:41.845529Z",
          "iopub.status.idle": "2022-03-07T00:17:47.831627Z",
          "shell.execute_reply": "2022-03-07T00:17:47.830814Z",
          "shell.execute_reply.started": "2022-03-07T00:12:41.845743Z"
        },
        "id": "0XEhbwwFF-xD",
        "outputId": "7a1df5b7-0953-4ca0-bf0f-419837aae03f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 223/223 [04:19<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 25.23719165085389,\n",
            "  \"f1\": 25.376298053152556,\n",
            "  \"total\": 14229,\n",
            "  \"HasAns_exact\": 0.0,\n",
            "  \"HasAns_f1\": 0.20111207054551883,\n",
            "  \"HasAns_total\": 9842,\n",
            "  \"NoAns_exact\": 81.85548210622294,\n",
            "  \"NoAns_f1\": 81.85548210622294,\n",
            "  \"NoAns_total\": 4387\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}   # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:     # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1)  # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1  # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):     # for every item of the batch\n",
        "        i = (int(id_index_i))  # get the list index of the item \n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py triviaqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KF45ArDF-xD"
      },
      "source": [
        "### Quac fined-tuned model test on NewsQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-07T00:17:47.836198Z",
          "iopub.status.busy": "2022-03-07T00:17:47.835957Z",
          "iopub.status.idle": "2022-03-07T00:17:47.840261Z",
          "shell.execute_reply": "2022-03-07T00:17:47.839403Z",
          "shell.execute_reply.started": "2022-03-07T00:17:47.836171Z"
        },
        "id": "q-X4MuJkF-xD"
      },
      "outputs": [],
      "source": [
        "validation_path = 'newsqa_dev_SquadFormat.json'   # path to the validation-dev set of NewsQA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-07T00:17:47.842474Z",
          "iopub.status.busy": "2022-03-07T00:17:47.841842Z",
          "iopub.status.idle": "2022-03-07T00:19:41.216629Z",
          "shell.execute_reply": "2022-03-07T00:19:41.215777Z",
          "shell.execute_reply.started": "2022-03-07T00:17:47.842438Z"
        },
        "id": "Q5hS2S0YF-xE",
        "outputId": "37d134ba-5d39-4fd4-c2b1-d27afc4b31da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 81/81 [01:33<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 0.07742934572202866,\n",
            "  \"f1\": 0.5363297972466945,\n",
            "  \"total\": 5166,\n",
            "  \"HasAns_exact\": 0.07742934572202866,\n",
            "  \"HasAns_f1\": 0.5363297972466945,\n",
            "  \"HasAns_total\": 5166\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:     # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]   # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):    # for every item of the batch\n",
        "        i = (int(id_index_i))    # get the list index of the item\n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the CLS token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer    # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py newsqa_dev_SquadFormat.json results.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6Dt4864F-xE"
      },
      "source": [
        "### Quac fined-tuned model test on Natural Questions (NQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-07T00:19:41.218988Z",
          "iopub.status.busy": "2022-03-07T00:19:41.218658Z",
          "iopub.status.idle": "2022-03-07T00:19:41.225716Z",
          "shell.execute_reply": "2022-03-07T00:19:41.224953Z",
          "shell.execute_reply.started": "2022-03-07T00:19:41.218949Z"
        },
        "id": "ku8vHqygF-xE"
      },
      "outputs": [],
      "source": [
        "validation_path = 'nq_dev.json'   # path to the validation-dev set of NQ dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-07T00:19:41.228698Z",
          "iopub.status.busy": "2022-03-07T00:19:41.228201Z",
          "iopub.status.idle": "2022-03-07T00:20:50.783510Z",
          "shell.execute_reply": "2022-03-07T00:20:50.782714Z",
          "shell.execute_reply.started": "2022-03-07T00:19:41.228633Z"
        },
        "id": "eHI6canGF-xE",
        "outputId": "bd00703c-0a36-4a90-9ba2-4f2b1dd1d6b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 53/53 [01:01<00:00,  1.16s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{\n",
            "  \"exact\": 13.119620065301277,\n",
            "  \"f1\": 18.6813919599444,\n",
            "  \"total\": 3369,\n",
            "  \"HasAns_exact\": 0.0,\n",
            "  \"HasAns_f1\": 7.953144954606389,\n",
            "  \"HasAns_total\": 2356,\n",
            "  \"NoAns_exact\": 43.63277393879566,\n",
            "  \"NoAns_f1\": 43.63277393879566,\n",
            "  \"NoAns_total\": 1013\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def get_validation_predictions(file_name):\n",
        "  # create the validation set in order to evalutate the model\n",
        "  with open(file_name, 'rb') as file: # open the given dev file\n",
        "      squad = json.load(file)\n",
        "\n",
        "  context_list = [] # list that will contain the context corresponding to every question of question_list\n",
        "  question_list = [] # list that will contain all the questions \n",
        "  id_list = [] # list that will contain the id's of the questions\n",
        "  squad = squad['data'] # get the data of the file\n",
        "  for data in squad:\n",
        "      for paragraph in data['paragraphs']: # for every paragraph\n",
        "          context = paragraph['context']  # store the context of the paragraph\n",
        "          for qa in paragraph['qas']: # for every question of this context\n",
        "              question = qa['question'] # get the question text\n",
        "              id = qa['id']  # get the id of the question\n",
        "              question_list.append(question) # add the question to the list\n",
        "              id_list.append(id) # add the id to the list\n",
        "              context_list.append(context) # add the context to the list\n",
        "\n",
        "  return context_list, question_list, id_list\n",
        "\n",
        "# get the data of the dev file\n",
        "test_contexts, test_questions, test_id = get_validation_predictions(validation_path)\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# tokenize the data (without defining a max_length)\n",
        "test_tokenized = tokenizer(test_contexts, test_questions, truncation=True, padding=True)\n",
        "# convert data to tensors\n",
        "test_id_tensor = torch.tensor(test_tokenized['input_ids'] )\n",
        "test_mask_tensor = torch.tensor(test_tokenized['attention_mask'])\n",
        "test_qid_tensor = torch.tensor(list(range(len(test_tokenized['input_ids']))))\n",
        "# create dataloaders, so as to evalutate in batches in order to speedup the process\n",
        "batch_size = 64\n",
        "test_dataset = torch.utils.data.TensorDataset(test_id_tensor, test_mask_tensor,test_qid_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "result_dict = {}  # create a dictionary that will contain the questions id's as keys and the predicted answer as values\n",
        "with torch.no_grad():\n",
        "    test_loader_tqdm = tqdm(test_loader)\n",
        "    for x_batch, mask, id_index in test_loader_tqdm:    # for every batch of dev data\n",
        "      # transfer data to GPU\n",
        "      x_batch = x_batch.to(device)\n",
        "      mask = mask.to(device)\n",
        "      z = model(x_batch, attention_mask=mask)\n",
        "      loss = z[0]    # store the loss (not actually used)\n",
        "      answer_start = torch.argmax(z['start_logits'], dim=1) # find the predicted start index, based on the bigger logit\n",
        "      answer_end = torch.argmax(z['end_logits'], dim=1)+1 # find the predicted end index, based on the bigger logit\n",
        "      # also add 1 to the end index in order to also fetch the last token\n",
        "      for count,id_index_i in enumerate(list(id_index)):    # for every item of the batch\n",
        "        i = (int(id_index_i))    # get the list index of the item\n",
        "        # get the predicted answer tokens from the tokenized context\n",
        "        answer_tokens = x_batch[count][answer_start[count]:answer_end[count]]\n",
        "        # if the predicted answer is only the SEP token\n",
        "        if len(answer_tokens) == 1 and int(answer_tokens[0]) == tokenizer.sep_token_id:\n",
        "            # then assing the empty string as the answer in the dictionary\n",
        "            # test_id[i] : the id of the question\n",
        "            result_dict[test_id[i]] = \"\"\n",
        "        else: # if the predicted answer is NOT the SEP token\n",
        "            # convert the tokens into text\n",
        "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
        "            result_dict[test_id[i]]=answer   # assign it as the answer in the dictionary\n",
        "      del x_batch, mask\n",
        "# write the dictionary into a file\n",
        "with open('results.txt', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(result_dict))\n",
        "# call the evalutation script giving as arguments the dev-file and the printed dictionary file with the predicted answers\n",
        "!python evaluation.py nq_dev.json results.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oVELM0Pit6VM"
      ],
      "name": "project4_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04001322793a4b41add88a6b37fff6c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05f737b255a74c3f9ec79319d01c220e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0adb998b1cdd450ab7422022f8bd6ced": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bdf4984d0ce4ad29f53398edf654c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c0cce3926a1498487838b1bb34de041": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c6e517870784c10b14173cfdbba97f3",
            "placeholder": "​",
            "style": "IPY_MODEL_a4b001538bd34026901353fcc1f72ae8",
            "value": "Downloading: 100%"
          }
        },
        "16e712ac3ce0470ba670f95b60741985": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e57c5d2859842d891dcbe8f4c8fdc73",
              "IPY_MODEL_cd88d30c1e844c01913edabc58189c2c",
              "IPY_MODEL_bce192674b034e71b1357df45737759b"
            ],
            "layout": "IPY_MODEL_5ffa807df9be4c3cbbd0fb270adeb558"
          }
        },
        "1f84d16cabf24d72974a3a3d24e734da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7ddc78e782d409994d1166afa086e0e",
              "IPY_MODEL_4ff4ad3855c84cf583d8fcab11e6a120",
              "IPY_MODEL_7481b3823350472883d131d2d5fd5e8b"
            ],
            "layout": "IPY_MODEL_0adb998b1cdd450ab7422022f8bd6ced"
          }
        },
        "279071b9eb5347499f7aa86b2da83b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32253a65263a4bc8a512ee0b612f20ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a355bebb4aa42c6b0e946ea774df392": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce7a41c2f3549de94862aaf0c5fe269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46942f5fc4a041aca713284e8e1b0b7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4853784c8e32473e88368ac9b85a81be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ff4ad3855c84cf583d8fcab11e6a120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fbc4da55e2f40a098dce2a3d76aa85b",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ce7a41c2f3549de94862aaf0c5fe269",
            "value": 570
          }
        },
        "5232b9467e994e5cbdc9d841e0afb4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d80cff6a6c3b4392ade5101267ab0204",
              "IPY_MODEL_83a5c9b3a477437397d2004840d1642f",
              "IPY_MODEL_ff4e3b63c7804434b6280023e28c1df5"
            ],
            "layout": "IPY_MODEL_5564f3537f464f67aceb2880f20b57cc"
          }
        },
        "5564f3537f464f67aceb2880f20b57cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fa46c6c9da840b9af51fd16dd8de806": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c07dfae8c514429888a610146010470e",
            "placeholder": "​",
            "style": "IPY_MODEL_46942f5fc4a041aca713284e8e1b0b7e",
            "value": " 28.0/28.0 [00:00&lt;00:00, 615B/s]"
          }
        },
        "5ffa807df9be4c3cbbd0fb270adeb558": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62c15fc4dd1a4d17aad036bda129f769": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "689e201ea7954340805adaffc67c5b27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68bdaa1df71c438bb5b527155a50b214": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e57c5d2859842d891dcbe8f4c8fdc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79093ce6e0994950bbb17f0ffd083e8c",
            "placeholder": "​",
            "style": "IPY_MODEL_c2bfcd8a4764402f8f6bde5476693fc5",
            "value": "Downloading: 100%"
          }
        },
        "7481b3823350472883d131d2d5fd5e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_279071b9eb5347499f7aa86b2da83b3d",
            "placeholder": "​",
            "style": "IPY_MODEL_b9fe1b78a49a4b0fa4165668c60c015f",
            "value": " 570/570 [00:00&lt;00:00, 3.03kB/s]"
          }
        },
        "79093ce6e0994950bbb17f0ffd083e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fbc4da55e2f40a098dce2a3d76aa85b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83a5c9b3a477437397d2004840d1642f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4853784c8e32473e88368ac9b85a81be",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bdf4984d0ce4ad29f53398edf654c75",
            "value": 231508
          }
        },
        "8c6e517870784c10b14173cfdbba97f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d2c8450d67e40ba949ca13fdf5c5c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dc7de5707114778937d5a7205655c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9af3821ae04f46daa9c70e13ff9c75a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c0cce3926a1498487838b1bb34de041",
              "IPY_MODEL_fcb5a07a6a2740169cede9bc31b971d7",
              "IPY_MODEL_5fa46c6c9da840b9af51fd16dd8de806"
            ],
            "layout": "IPY_MODEL_d04c5f99f39c4066b05a1747a51cc8fb"
          }
        },
        "a4b001538bd34026901353fcc1f72ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7ddc78e782d409994d1166afa086e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_689e201ea7954340805adaffc67c5b27",
            "placeholder": "​",
            "style": "IPY_MODEL_62c15fc4dd1a4d17aad036bda129f769",
            "value": "Downloading: 100%"
          }
        },
        "b9fe1b78a49a4b0fa4165668c60c015f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bce192674b034e71b1357df45737759b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68bdaa1df71c438bb5b527155a50b214",
            "placeholder": "​",
            "style": "IPY_MODEL_05f737b255a74c3f9ec79319d01c220e",
            "value": " 455k/455k [00:00&lt;00:00, 947kB/s]"
          }
        },
        "c07dfae8c514429888a610146010470e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2bfcd8a4764402f8f6bde5476693fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd88d30c1e844c01913edabc58189c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32253a65263a4bc8a512ee0b612f20ce",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dc7de5707114778937d5a7205655c14",
            "value": 466062
          }
        },
        "d04c5f99f39c4066b05a1747a51cc8fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80cff6a6c3b4392ade5101267ab0204": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a355bebb4aa42c6b0e946ea774df392",
            "placeholder": "​",
            "style": "IPY_MODEL_fd107d81e83b4e4aba851a6c7200b85c",
            "value": "Downloading: 100%"
          }
        },
        "f63b6f34f0af486ba1c0bd0fbfa0f2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fad84de815534338a4cda3a3441a2c27": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcb5a07a6a2740169cede9bc31b971d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04001322793a4b41add88a6b37fff6c0",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f63b6f34f0af486ba1c0bd0fbfa0f2d1",
            "value": 28
          }
        },
        "fd107d81e83b4e4aba851a6c7200b85c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff4e3b63c7804434b6280023e28c1df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fad84de815534338a4cda3a3441a2c27",
            "placeholder": "​",
            "style": "IPY_MODEL_8d2c8450d67e40ba949ca13fdf5c5c1e",
            "value": " 226k/226k [00:00&lt;00:00, 375kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}